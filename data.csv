materia,texto
fisica,"En física clásica, la fuerza (abreviatura F) es un fenómeno que modifica el movimiento de un cuerpo (lo acelera, frena, cambia el sentido, etc.) o bien lo deforma. Las fuerzas pueden representarse mediante vectores, ya que poseen magnitud y dirección.  No debe confundirse el concepto de fuerza con el esfuerzo o la energía. En el Sistema Internacional de Unidades, la unidad de medida de la fuerza es el newton que se representa con el símbolo N, en reconocimiento a Isaac Newton por su aporte a la física, especialmente a la mecánica clásica. El newton es una unidad derivada del Sistema Internacional de Unidades que se define como la fuerza necesaria para proporcionar una aceleración de 1m/s² a un objeto de 1kg de masa.Los conceptos relacionados con la fuerza incluyen: empuje, que aumenta la velocidad de un objeto; arrastrar, que disminuye la velocidad de un objeto; y par motor, que produce cambios en la velocidad de rotación de un objeto. En un cuerpo extendido, cada parte suele aplicar fuerzas sobre las partes adyacentes; la distribución de dichas fuerzas a través del cuerpo es la tensión mecánica interna. Tales tensiones mecánicas internas no causan ninguna aceleración de ese cuerpo, ya que las fuerzas se equilibran entre sí. La presión, la distribución de muchas fuerzas pequeñas aplicadas sobre un área de un cuerpo, es un tipo de tensión simple que, si se desequilibra, puede hacer que el cuerpo se acelere. El estrés suele provocar la deformación de los materiales sólidos, o el flujo en los fluidos.IntroducciónLa fuerza es un modelo matemático de intensidad de las interacciones, junto con la energía. Así, por ejemplo, la fuerza gravitacional es la atracción entre los cuerpos que tienen masa, el peso es la atracción que la Tierra ejerce sobre los objetos en las cercanías de su superficie, la fuerza elástica es la que ejerce un resorte deformado (comprimido o estirado). En física, hay dos tipos de ecuaciones de fuerza: las «de causas», en las cuales se especifica el origen de la atracción o repulsión, como, por ejemplo, la ley de la gravitación universal de Newton o la ley de Coulomb; y las «de efectos», la cual es, fundamentalmente, la segunda ley de Newton.La fuerza es una magnitud física de carácter vectorial capaz de deformar un cuerpo (efecto estático), modificar su velocidad o vencer su inercia y ponerlos en movimiento si estaban inmóviles (efecto dinámico). En este sentido, la fuerza puede definirse como toda acción o influencia capaz de modificar el estado de movimiento o de reposo de un cuerpo (imprimiéndole una aceleración que modifica el módulo o la dirección de su velocidad).Comúnmente nos referimos a la fuerza aplicada sobre un objeto sin tener en cuenta al otro objeto u objetos con los que está interactuando y que experimentarán, a su vez, otras fuerzas. Actualmente, cabe definir la fuerza como un ente físico matemático, de carácter vectorial, asociado con la interacción del cuerpo con otros cuerpos que constituyen su entorno. Este concepto tiene relación directa con la tercera ley de Newton.Desarrollo del conceptoLos filósofos de la antigüedad utilizaron el concepto de fuerza en el estudio de los objetos estacionarios y en movimiento y de las máquinas simples, pero pensadores como Aristóteles y Arquímedes mantuvieron errores fundamentales en la comprensión de la fuerza. En parte, esto se debía a una comprensión incompleta de la fuerza, a veces no evidente, del rozamiento, y a una visión consecuentemente inadecuada de la naturaleza del movimiento natural.[1] Un error fundamental fue la creencia de que se requiere una fuerza para mantener el movimiento, incluso a una velocidad constante. La mayoría de los malentendidos anteriores sobre el movimiento y la fuerza fueron finalmente corregidos por Galileo Galilei e Isaac Newton. Con su perspicacia matemática, Isaac Newton formuló las leyes del movimiento que no fueron mejoradas durante casi trescientos años.[2] A principios del siglo XX, Einstein desarrolló una teoría de la relatividad que predecía correctamente la acción de las fuerzas sobre los objetos con momentos crecientes cercanos a la velocidad de la luz, y también proporcionó una visión de las fuerzas producidas por la gravitación y la inercia.Con los conocimientos modernos de la mecánica cuántica y la tecnología que puede acelerar las partículas cerca de la velocidad de la luz, la física de partículas ha ideado un Modelo Estándar para describir las fuerzas entre partículas más pequeñas que los átomos. El Modelo Estándar predice que las partículas intercambiadas, llamadas bosones de gauge, son el medio fundamental por el que se emiten y absorben las fuerzas. Solo se conocen cuatro interacciones principales: en orden de fuerza decreciente, son: fuerte, electromagnética, débil, y gravitatoria.[3]: 2–10 [4]: 79  Física de partículas de alta energía Las observaciones realizadas durante las décadas de 1970 y 1980 confirmaron que las fuerzas débil y electromagnética son expresiones de una interacción electrodébil más fundamental.[5]: 199–128 HistoriaEl concepto de fuerza fue descrito originalmente por Arquímedes, si bien únicamente en términos estáticos. Arquímedes y otros creyeron que el «estado natural» de los objetos materiales en la esfera terrestre era el reposo y que los cuerpos tendían, por sí mismos, hacia ese estado si no se actuaba sobre ellos en modo alguno. De acuerdo con Aristóteles la perseverancia del movimiento requería siempre una causa eficiente (algo que parece concordar con la experiencia cotidiana, donde las fuerzas de fricción pueden pasar desapercibidas).Galileo Galilei (1564-1642) sería el primero en dar una definición dinámica de fuerza, opuesta a la de Arquímedes, estableciendo claramente la ley de la inercia, afirmando que un cuerpo sobre el que no actúa ninguna fuerza permanece en movimiento inalterado. Esta ley, que refuta la tesis de Arquímedes, aún hoy día no resulta obvia para la mayoría de las personas sin formación científica.Se considera que fue Isaac Newton el primero que formuló matemáticamente la moderna definición de fuerza, aunque también usó el término latino vis impressa ('fuerza impresa') y vis motrix para otros conceptos diferentes. Además, Isaac Newton postuló que las fuerzas gravitatorias variaban según la ley de la inversa del cuadrado de la distancia.Charles Coulomb fue el primero que comprobó que la interacción entre cargas eléctricas o electrónicas puntuales también varía según la ley de la inversa del cuadrado de la distancia (1784).En 1798, Henry Cavendish logró medir experimentalmente la fuerza de atracción gravitatoria entre dos masas pequeñas utilizando una balanza de torsión. Gracias a lo cual pudo determinar el valor de la constante de la gravitación universal y, por tanto, pudo calcular la masa de la Tierra.Con el desarrollo de la electrodinámica cuántica, a mediados del siglo XX, se constató que la «fuerza» era una magnitud puramente macroscópica surgida de la conservación del momento lineal o cantidad de movimiento para partículas elementales. Por esa razón las llamadas fuerzas fundamentales suelen denominarse «interacciones fundamentales».Conceptos prenewtonianosDesde la antigüedad el concepto de fuerza ha sido reconocido como parte integral del funcionamiento de cada una de las máquinas simples. La ventaja mecánica dada por una máquina simple permitía utilizar menos fuerza a cambio de que esa fuerza actuara sobre una mayor distancia para la misma cantidad de trabajo. El análisis de las características de las fuerzas culminó en última instancia con el trabajo de Arquímedes que fue especialmente famoso por formular un tratamiento de las fuerzas de flotación inherentes a los fluidos.[1]Aristóteles proporcionó una discusión filosófica del concepto de fuerza como parte integral de la Cosmología aristotélica. En opinión de Aristóteles, la esfera terrestre contenía cuatro elementos que llegan a descansar en diferentes «lugares naturales» de la misma. Aristóteles creía que los objetos inmóviles de la Tierra, los compuestos mayoritariamente por los elementos tierra y agua, estaban en su lugar natural en el suelo y que permanecerían así si se les dejaba tranquilos. Distinguía entre la tendencia innata de los objetos a encontrar su «lugar natural» (por ejemplo para que los cuerpos pesados caigan), lo que conducía al «movimiento natural», y el movimiento no natural o forzado, que requería la aplicación continua de una fuerza.[6] Esta teoría, basada en la experiencia cotidiana de cómo se mueven los objetos, como la aplicación constante de una fuerza necesaria para mantener un carro en movimiento, tenía problemas conceptuales para explicar el comportamiento de los proyectiles, como el vuelo de las flechas. El lugar en el que el arquero mueve el proyectil estaba en el inicio del vuelo, y mientras el proyectil navegaba por el aire, ninguna causa eficiente discernible actuaba sobre él. Aristóteles era consciente de este problema y propuso que el aire desplazado a través de la trayectoria del proyectil lo lleva hasta su objetivo. Esta explicación exige un continuo como el aire para el cambio de lugar en general.[7]La física aristotélica comenzó a enfrentarse a las críticas en la ciencia medieval, primero por parte de Juan Filopón en el siglo VI.Las deficiencias de la física aristotélica no se corregirían del todo hasta el trabajo del siglo XVII de Galileo Galilei, que se vio influenciado por la idea tardomedieval de que los objetos en movimiento forzado llevaban una fuerza innata de ímpetus. Galileo construyó un experimento en el que se hicieron rodar piedras y balas de cañón por una pendiente para refutar la teoría aristotélica del movimiento. Demostró que los cuerpos eran acelerados por la gravedad en una medida que era independiente de su masa y argumentó que los objetos conservan su velocidad a menos que se actúe sobre ellos con una fuerza, por ejemplo la fricción.[8]A principios del siglo XVII, antes de los Principios de Newton, el término «fuerza» (en latín: vis) se aplicaba a muchos fenómenos físicos y no físicos, por ejemplo, para la aceleración de un punto. El producto de una masa puntual por el cuadrado de su velocidad fue denominado vis viva (fuerza viva) por Leibniz. El concepto moderno de fuerza corresponde a la vis motrix de Newton. (fuerza de aceleración).[9]Leyes de NewtonPrimera leyLa primera ley del movimiento de Newton establece que los objetos continúan moviéndose en un estado de velocidad constante a menos que se actúe sobre ellos con una fuerza neta externa. (fuerza resultante).[10] Esta ley es una extensión de la idea de Galileo de que la velocidad constante estaba asociada a la falta de fuerza neta (véase una descripción más detallada de esto más adelante). Newton propuso que todo objeto con masa tiene una inercia innata que funciona como el «estado natural» de equilibrio fundamental en lugar de la idea aristotélica del «estado natural de reposo». Es decir, la primera ley empírica de Newton contradice la creencia intuitiva aristotélica de que se requiere una fuerza neta para mantener un objeto en movimiento con velocidad constante. Al hacer que el «reposo» sea físicamente indistinguible de la «velocidad constante no nula», la primera ley de Newton conecta directamente la inercia con el concepto de velocidades relativas. En concreto, en los sistemas en los que los objetos se mueven con diferentes velocidades, es imposible determinar qué objeto está «en movimiento» y qué objeto está «en reposo». Las leyes de la física son las mismas en todos los marcos de referencia inerciales, es decir, en todos los marcos relacionados por una transformación galileana.Por ejemplo, mientras se viaja en un vehículo en movimiento a una velocidad constante, las leyes de la física no cambian como resultado de su movimiento. Si una persona que viaja dentro del vehículo lanza una pelota hacia arriba, esa persona observará que se eleva verticalmente y cae verticalmente y no tendrá que aplicar una fuerza en la dirección en que se mueve el vehículo. Otra persona, observando el paso del vehículo en movimiento, observaría que la pelota sigue una trayectoria curva parabólica en la misma dirección que el movimiento del vehículo. Es la inercia de la pelota, asociada a su velocidad constante en la dirección del movimiento del vehículo, la que hace que la pelota siga avanzando incluso cuando es lanzada hacia arriba y vuelve a caer. Desde la perspectiva de la persona que va en el coche, el vehículo."
fisica,"El tiempo (del latín tempus) es una magnitud física con la que se mide la duración o separación de acontecimientos.  Para cada observador, el tiempo medido por dicho observador le permite ordenar los sucesos en tres conjuntos: un pasado, un futuro y un tercer conjunto de eventos ni pasados ni futuros respecto a otro. En mecánica clásica a esta tercera clase se llama presente y está formada por eventos simultáneos a uno en particular. Además en mecánica clásica, la división en pasado, presente y futuro es la misma para todos los observadores, hecho asociado a que en mecánica clásica el tiempo y el espacio son absolutos.  En mecánica relativista el concepto de tiempo es más complejo: los hechos simultáneos (presente) son relativos al observador, salvo que se produzcan en el mismo lugar del espacio; por ejemplo, un choque entre dos partículas. Como consecuencia de lo anterior el tiempo y el espacio en mecánica relativista son relativos.  La unidad básica para medir el tiempo en el Sistema Internacional es el segundo, cuyo símbolo es s (debido a que es un símbolo y no una abreviatura, no se debe escribir con mayúscula, ni se escribe como seg, sg o sec, ni agregando un punto posterior).  El tiempo ha sido durante mucho tiempo un importante tema de estudio en la religión, la filosofía y la ciencia, pero definirlo de manera aplicable a todos los campos sin circularidad ha eludido sistemáticamente a los estudiosos.[1] No obstante, campos tan diversos como los negocios, la industria, los deportes, las ciencias y las artes escénicas incorporan alguna noción de tiempo en sus respectivos sistemas de medición.[2][3][4]  El tiempo en física se define operativamente como ""lo que lee un reloj"".[5][6][7]   La naturaleza física del tiempo es abordada por la relatividad general con respecto a los eventos en el espacio-tiempo. Ejemplos de eventos son la colisión de dos partículas, la explosión de una supernova o la llegada de un cohete. A cada suceso se le pueden asignar cuatro números que representan su tiempo y posición (las coordenadas del suceso). Sin embargo, los valores numéricos son diferentes para los distintos observadores. En la relatividad general, la pregunta de qué hora es ahora solo tiene sentido en relación con un observador concreto. La distancia y el tiempo están íntimamente relacionados y el tiempo necesario para que la luz recorra una distancia específica es el mismo para todos los observadores, como demostró públicamente por primera vez el experimento de Michelson y Morley. La relatividad general no aborda la naturaleza del tiempo para intervalos extremadamente pequeños en los que la mecánica cuántica es válida. En este momento, no existe una teoría generalmente aceptada de la relatividad general cuántica. [8]  El tiempo es una de las siete cantidades físicas fundamentales tanto en el Sistema Internacional de Unidades (SI) como en el Sistema Internacional de Cantidades. La unidad de tiempo base del SI es el segundo. El tiempo se utiliza para definir otras cantidades —como la velocidad— por lo que definir el tiempo en términos de dichas cantidades daría lugar a una circularidad de definición.[9] Una definición operativa del tiempo, en la que se dice que la observación de un cierto número de repeticiones de uno u otro evento cíclico estándar (como el paso de un péndulo de movimiento libre) constituye una unidad estándar como el segundo, es muy útil tanto en la realización de experimentos avanzados como en los asuntos cotidianos de la vida. Para describir las observaciones de un acontecimiento, se suele anotar una ubicación (posición en el espacio) y un tiempo.  La definición operativa del tiempo no aborda cuál es su naturaleza fundamental. No aborda por qué los acontecimientos pueden ocurrir hacia adelante y hacia atrás en el espacio, mientras que los acontecimientos solo ocurren en el avance del tiempo. Las investigaciones sobre la relación entre el espacio y el tiempo llevaron a los físicos a definir el continuo espaciotiempo. La relatividad general es el marco principal para entender cómo funciona el espaciotiempo.[10] A través de los avances en las investigaciones tanto teóricas como experimentales del espacio-tiempo, se ha demostrado que el tiempo puede distorsionarse y dilatarse, particularmente en los bordes de los agujeros negros.  La medición del tiempo ha ocupado a los científicos y a los tecnólogos de la ingeniería, y fue una motivación primordial en la navegación y la astronomía. Los eventos periódicos y el movimiento periódico han servido durante mucho tiempo como estándares para las unidades de tiempo. Algunos ejemplos son el movimiento aparente del sol en el cielo, las fases de la luna, el movimiento de un péndulo y el latido del corazón. Actualmente, la unidad de tiempo internacional, el segundo, se define a partir de la medición de la frecuencia de transición electrónica de los átomos de cesio. El tiempo también tiene una importancia social significativa, ya que tiene un valor económico (""el tiempo es dinero""), así como un valor personal, debido a la conciencia del tiempo limitado en cada día y en la la duración de la vida humana.  Hay muchos sistemas para determinar qué hora es, entre ellos el Sistema de Posicionamiento Global, otros sistemas de satélites, el Tiempo Universal Coordinado y el tiempo solar medio. En general, los números obtenidos a partir de los distintos sistemas de tiempo difieren entre sí.  El concepto físico del tiempo Dados dos eventos puntuales E1 y E2, que ocurren respectivamente en instantes de tiempo t1 y t2, y en puntos del espacio diferentes P1 y P2, todas las teorías físicas admiten que estos pueden cumplir una y solo una de las siguientes tres condiciones:[11]  Dado un evento cualquiera, el conjunto de eventos puede dividirse según esas tres categorías anteriores. Es decir, todas las teorías físicas permiten, fijado un evento, clasificar a los eventos en: (1) pasado, (2) futuro y (3) resto de eventos (ni pasados ni futuros). La clasificación de un tiempo presente es debatible por la poca durabilidad de este intervalo que no se puede medir como un estado actual sino como un dato que se obtiene en una continua sucesión de eventos. En mecánica clásica esta última categoría está formada por los sucesos llamados simultáneos, y en mecánica relativista, por los eventos no relacionados causalmente con el primer evento. Sin embargo, la mecánica clásica y la mecánica relativista difieren en el modo concreto en que puede hacerse esa división entre pasado, futuro y otros eventos y en el hecho de que dicho carácter pueda ser absoluto o relativo respecto al contenido de los conjuntos.  El tiempo en mecánica clásica En mecánica clásica, el tiempo se concibe como una magnitud absoluta, es decir, es un escalar cuya medida es idéntica para todos los observadores (una magnitud relativa es aquella cuyo valor depende del observador concreto). Esta concepción del tiempo recibe el nombre de tiempo absoluto. Esa concepción está de acuerdo con la concepción filosófica de Kant, que establece el espacio y el tiempo como necesarios para cualquier experiencia humana. Kant asimismo concluyó que el espacio y el tiempo eran conceptos subjetivos. Mas, no por ello, Kant establecerá que tiempo y espacio sean dimensiones absolutas, ni en sí mismas, sí apoyadas, en cambio, por Newton y Leibniz respectivamente. Para Kant no son dimensiones sino formas puras de la intuición suministrada por la experiencia, de manera que, al no tratarse de magnitudes, no hay posible choque entre ellas. Fijado un evento, cada observador clasificará el resto de eventos según una división tripartita clasificándolos en: (1) eventos pasados, (2) eventos futuros y (3) eventos ni pasados y ni futuros. La mecánica clásica y la física prerrelativista asumen:  Aunque dentro de la teoría especial de la relatividad y dentro de la teoría general de la relatividad, la división tripartita de eventos sigue siendo válida, no se verifican las últimas dos propiedades:  El tiempo en mecánica relativista En mecánica relativista la medida del transcurso del tiempo depende del sistema de referencia donde esté situado el observador y de su estado de movimiento, es decir, diferentes observadores miden diferentes tiempos transcurridos entre dos eventos causalmente conectados. Por tanto, la duración de un proceso depende del sistema de referencia donde se encuentre el observador.  De acuerdo con la teoría de la relatividad, fijados dos observadores situados en diferentes marcos de referencia, dos sucesos A y B dentro de la categoría (3) (eventos ni pasados ni futuros), pueden ser percibidos por los dos observadores como simultáneos, o puede que A ocurra ""antes"" que B para el primer observador mientras que B ocurre ""antes"" de A para el segundo observador. En esas circunstancias no existe, por tanto, ninguna posibilidad de establecer una noción absoluta de simultaneidad independiente del observador. Según la relatividad general el conjunto de los sucesos dentro de la categoría (3) es un subconjunto tetradimensional topológicamente abierto del espacio-tiempo. Cabe aclarar que esta teoría solo parece funcionar con la rígida condición de dos marcos de referencia solamente. Cuando se agrega un marco de referencia adicional, la teoría de la Relatividad queda invalidada: el observador A en la Tierra percibirá que el observador B viaja a mayor velocidad dentro de una nave espacial girando alrededor de la Tierra a 7000 kilómetros por segundo. El observador B notará que el dato de tiempo al reloj se ha desacelerado y concluye que el tiempo se ha dilatado por causa de la velocidad de la nave. Un observador C localizado fuera del sistema solar, notará que tanto el hombre en tierra como el astronauta girando alrededor de la Tierra, están viajando simultáneamente —la nave espacial y el planeta Tierra— a 28 kilómetros por segundo alrededor del Sol. La más certera conclusión acerca del comportamiento del reloj en la nave espacial, es que ese reloj está funcionando mal, porque no fue calibrado ni probado para esos nuevos cambios en su ambiente. Esta conclusión está respaldada por el hecho de que no existe prueba alguna que muestre que el tiempo es objetivo.  Solo si dos sucesos están atados causalmente todos los observadores ven el suceso «causal» antes que el suceso «efecto», es decir, las categorías (1) de eventos pasados y (2) de eventos futuros causalmente ligados sí son absolutos. Fijado un evento E el conjunto de eventos de la categoría (3) que no son eventos ni futuros ni pasados respecto a E puede dividirse en tres subconjuntos:  Las curiosas relaciones causales de la teoría de la relatividad, conllevan a que no existe un tiempo único y absoluto para los observadores, de hecho cualquier observador percibe el espacio-tiempo o espacio tetradimensional según su estado de movimiento, la dirección paralela a su cuadrivelocidad coincidirá con la dirección temporal, y los eventos que acontecen en las hipersuperficies espaciales perpendiculares en cada punto a la dirección temporal, forman el conjunto de acontecimientos simultáneos para ese observador.  Lamentablemente, dichos conjuntos de acontecimientos percibidos como simultáneos difieren de un observador a otro.  Si el tiempo propio es la duración de un suceso medido en reposo respecto a ese sistema, la duración de ese suceso medida desde un sistema de referencia que se mueve con velocidad constante con respecto al suceso viene dada por:  El tiempo en mecánica cuántica En mecánica cuántica debe distinguirse entre la mecánica cuántica convencional, en la que puede trabajarse bajo el supuesto clásico de un tiempo absoluto, y la mecánica cuántica relativista, dentro de la cual, al igual que sucede en la teoría de la relatividad, el supuesto de un tiempo absoluto es inaceptable e inapropiado.  La flecha del tiempo y la entropía Se ha señalado que la dirección del tiempo está relacionada con el aumento de entropía, aunque eso parece deberse a las peculiares condiciones que se dieron durante el Big Bang. Aunque algunos científicos como Penrose han argumentado que dichas condiciones no serían tan peculiares si consideramos que existe un principio o teoría física más completa que explique por qué nuestro universo, y tal vez otros, nacen con condiciones iniciales aparentemente improbables, que se reflejan en una bajísima entropía inicial.   "
fisica,"La velocidad es el cambio de posición de un objeto con respecto al tiempo. En física se representa con:      v     {\displaystyle \mathbf {v} \,}  o          v →       {\displaystyle {\vec {v}}\,}  . En análisis dimensional sus dimensiones son: [L]/[t].[1][2] Su unidad en el Sistema Internacional de Unidades es el metro por segundo (símbolo, m/s).  En matemática vectorial se puede entender por velocidad que esta incluye a la dirección del movimiento, de modo que dos objetos moviéndose en direcciones opuestas pero igual velocidad pueden tener un vector de velocidad distinto. A veces, y en estos contextos, para distinguir esta ambigüedad se proponen los términos rapidez o celeridad para referirse a la magnitud, o valor absoluto del vector velocidad.[3] Por ejemplo, ""5 metros por segundo"" es una velocidad, mientras que ""5 metros por segundo al oeste"" también es una velocidad, vectorial. Si al pasar el tiempo la velocidad se mide como ""5 metros por segundo al norte"", entonces el objeto tiene una velocidad cambiante, pero una rapidez constante, y se considera que está sufriendo una aceleración.  Historia Aristóteles estudió los fenómenos físicos sin llegar a conceptualizar una noción de velocidad. En efecto, sus explicaciones (que posteriormente se demostrarían incorrectas) solo describían los fenómenos inherentes al movimiento sin usar las matemáticas como herramienta.  Fue Galileo Galilei quien, estudiando el movimiento de los cuerpos en un plano inclinado, formuló el concepto de velocidad.  Para ello, fijó un patrón de unidad de tiempo, como por ejemplo 1 segundo, y midió la distancia recorrida por un cuerpo en cada unidad de tiempo. De esta manera, Galileo desarrolló el concepto de la velocidad como la distancia recorrida por unidad de tiempo. A pesar del gran avance que representó la introducción de esta nueva noción, sus alcances se limitaban a los alcances mismos de las matemáticas. Por ejemplo, era relativamente sencillo calcular la velocidad de un móvil que se desplazase a velocidad constante, puesto que en cada unidad de tiempo recorre distancias iguales. También lo era calcular la velocidad de un móvil con aceleración constante, como es el caso un cuerpo en caída libre. Sin embargo, cuando la velocidad del objeto variaba de forma más complicada, Galileo no disponía de herramientas matemáticas que le permitiesen determinar  la velocidad instantánea de un cuerpo.   Fue recién en el siglo XVI, con el desarrollo del cálculo por parte de Isaac Newton y Gottfried Leibniz, cuando se pudo solucionar la cuestión de obtener la velocidad instantánea de un cuerpo. Esta está determinada por la derivada del vector de posición del objeto respecto del tiempo.  Las aplicaciones de la velocidad, con el uso de Cálculo, es una herramienta fundamental en Física e Ingeniería, extendiéndose en prácticamente todo fenómeno que implique cambios de posición respecto del tiempo, esto es, que implique movimiento.  Un término relacionado con la velocidad es el de celeridad. En el lenguaje cotidiano se emplea frecuentemente el término velocidad para referirse a la celeridad. En física se hace una distinción entre ambas, ya que la celeridad es una magnitud escalar que representa el módulo de la velocidad. De manera muy sencilla, si se dice que una partícula se mueve con una velocidad de 10 m/s, se está haciendo referencia a su celeridad; por el contrario, si además se especifica la dirección en que se mueve, se está haciendo referencia a su velocidad.  Nociones generales Inicialmente, la noción de velocidad se basó la noción de velocidad media en un intervalo. Con el advenimiento del cálculo diferencial se introdujo la noción moderna. Dividir una distancia entre un tiempo parecía tan falso como la suma de estos dos valores podría parecer hoy. Así, para saber si un cuerpo iba más rápido que otro, Galileo (1564-1642) comparó la relación de las distancias recorridas por estos cuerpos con la relación de los tiempos correspondientes. Por ello aplicó la siguiente equivalencia:  La noción de velocidad instantánea fue definida formalmente por primera vez por Pierre Varignon (1654-1722) el 5 de julio de 1698, como la relación de una longitud infinitamente pequeña dx respecto a un tiempo infinitamente pequeño dt emprendido en el reconocimiento de esta longitud. Por ello, sirve el formalismo del círculo diferencial que ha sido planteado en el punto, catorce años antes de Leibniz. (1646-1716).  Velocidad constante vs. aceleración Para tener una velocidad constante, un objeto debe tener una velocidad constante en una dirección constante. La dirección constante obliga al objeto a moverse en una trayectoria recta, por lo que una velocidad constante significa un movimiento en línea recta a una velocidad constante.  Por ejemplo, un coche que se mueve a una velocidad constante de 20 kilómetros por hora en una trayectoria circular tiene una velocidad angular constante, pero no tiene una velocidad constante porque su dirección cambia. Por lo tanto, se considera que el coche está sufriendo una aceleración.  Diferencia entre velocidad y rapidez La rapidez (o celeridad) es el módulo del vector de velocidad, denota únicamente la celeridad con la que se mueve un objeto.[4][5]  Velocidad en mecánica clásica Velocidad media La velocidad media se define como el cambio de posición durante un intervalo de tiempo considerado. Se calcula dividiendo el vector desplazamiento (Δr) entre el  escalar tiempo (Δt) empleado en efectuarlo:         v ¯    =    Δ  r    Δ t      {\displaystyle \mathbf {\bar {v}} ={\frac {\Delta \mathbf {r} }{\Delta t}}}    De acuerdo con esta definición, la velocidad media es una magnitud vectorial (ya que es el resultado de dividir un vector entre un escalar).  Por otra parte, si se considera la distancia recorrida sobre la trayectoria durante un intervalo de tiempo dado, tenemos la velocidad media sobre la trayectoria o celeridad media, la cual es una magnitud escalar. La expresión anterior se escribe en la forma:         v ¯    =    Δ s   Δ t      {\displaystyle {\bar {v}}={\frac {\Delta s}{\Delta t}}}    El módulo del vector velocidad media, en general, es diferente al valor de la velocidad media sobre la trayectoria. Solo serán iguales si la trayectoria es rectilínea y si el móvil solo avanza (en uno u otro sentido) sin retroceder.  Por ejemplo, si un objeto recorre una distancia de 10 m sobre la trayectoria en un lapso de 3 s, el módulo de su velocidad media sobre la trayectoria es:      v =    Δ s   Δ t    =   10 3   = 3 ,    3 ^       m/s    {\displaystyle v={\frac {\Delta s}{\Delta t}}={\frac {10}{3}}=3,{\hat {3}}\,\,{\text{m/s}}}    Velocidad instantánea La velocidad instantánea es un vector tangente a la trayectoria, corresponde a la derivada del vector posición respecto al tiempo.  Permite conocer la velocidad de un móvil que se desplaza sobre una trayectoria cuando el intervalo de tiempo es infinitamente pequeño, siendo entonces el espacio recorrido también muy pequeño, representando un punto de la trayectoria. La velocidad instantánea es siempre tangente a la trayectoria.       v  =  lim  Δ t → 0      Δ  r    Δ t    =    d   r     d t      {\displaystyle \mathbf {v} =\lim _{\Delta t\to 0}{\frac {\Delta \mathbf {r} }{\Delta t}}={\frac {d{\mathbf {r} }}{dt}}}    En forma vectorial, la velocidad es la derivada del vector posición respecto al tiempo:       v  =    d s   d t        u   t   =    d   r     d t      {\displaystyle \mathbf {v} ={\frac {ds}{dt}}\ \mathbf {u} _{t}={\frac {d{\mathbf {r} }}{dt}}}    donde       u   t     {\displaystyle \mathbf {u} _{t}}   es un vector (vector de módulo unidad) de dirección tangente a la trayectoria del cuerpo en cuestión y      r    {\displaystyle \mathbf {r} }   es el vector posición, ya que en el límite los diferenciales de espacio recorrido y posición coinciden.  donde  Velocidad promedio La velocidad promedio es el promedio de la magnitud de la velocidad final e inicial concluyendo a la aceleración constante:  Celeridad instantánea La celeridad o rapidez instantánea es una magnitud escalar definida como el módulo de la velocidad instantánea, esto es, el módulo del vector velocidad en un instante dado. Se la expresa como:  (6)     celeridad  = v =  |  v  |  =  |    d  r    d t    |  =    d s   d t        u    t      {\displaystyle {\text{celeridad}}=v=\left|\mathbf {v} \right|=\left|{\frac {d\mathbf {r} }{dt}}\right|={\frac {ds}{dt}}{{\mathbf {u} }_{t}}}    de modo que también podemos expresar la velocidad en función de la celeridad en la forma:  (7)     v  = v      u    t      {\displaystyle \mathbf {v} =v\,{{\mathbf {u} }_{t}}}    siendo       u   t     {\displaystyle \mathbf {u} _{t}}   el versor tangente a la trayectoria en ese instante.  Velocidad relativa El cálculo de velocidades relativas en mecánica clásica es aditivo y encaja con la intuición común sobre velocidades; de esta propiedad de la aditividad surge el método de la velocidad relativa. La velocidad relativa entre dos observadores A y B es el valor de la velocidad de un observador medida por el otro. Las velocidades relativas medidas por A y B serán iguales en valor absoluto, pero de signo contrario. Denotaremos al valor la velocidad relativa de un observador B respecto a otro observador A como       v   BA      {\displaystyle \mathbf {v} _{\text{BA}}\;}  .  Dadas dos partículas A y B, cuyas velocidades medidas por un cierto observador son       v   A      {\displaystyle \mathbf {v} _{\text{A}}\,}   y       v   B      {\displaystyle \mathbf {v} _{\text{B}}\,}  , la velocidad relativa de B con respecto a A se denota como       v   BA      {\displaystyle \mathbf {v} _{\text{BA}}\;}   y viene dada por:  (8)      v   BA   =   v   B   −   v   A     {\displaystyle \mathbf {v} _{\text{BA}}=\mathbf {v} _{\text{B}}-\mathbf {v} _{\text{A}}}    Naturalmente, la velocidad relativa de A con respecto a B se denota como       v   AB      {\displaystyle \mathbf {v} _{\text{AB}}\;}   y viene dada por:  (9)      v   AB   =   v   A   −   v   B     {\displaystyle \mathbf {v} _{\text{AB}}=\mathbf {v} _{\text{A}}-\mathbf {v} _{\text{B}}}    de modo que las velocidades relativas       v   BA      {\displaystyle \mathbf {v} _{\text{BA}}\;}   y       v   AB      {\displaystyle \mathbf {v} _{\text{AB}}\;}   tienen el mismo módulo, pero dirección contraria.  De la expresiones anteriores obtenemos:  (10)             v    A    =     v    AB    +     v    B             v    B    =     v    BA    +     v    A          {\displaystyle {\begin{aligned}&{{\mathbf {v} }_{\text{A}}}={{\mathbf {v} }_{\text{AB}}}+{{\mathbf {v} }_{\text{B}}}\\&{{\mathbf {v} }_{\text{B}}}={{\mathbf {v} }_{\text{BA}}}+{{\mathbf {v} }_{\text{A}}}\\\end{aligned}}}    que nos permiten calcular vectorialmente la velocidad de A cuando se conoce su velocidad respecto de B y la velocidad de B. A estas expresiones se las denomina ley de adición de velocidades.  Velocidad angular La velocidad angular no es propiamente una velocidad en el sentido anteriormente definido, ya que no se refiere al desplazamiento de un cuerpo sobre una trayectoria a un movimiento de rotación. Aunque no es propiamente una velocidad una vez conocida la velocidad de un punto de un sólido y la velocidad angular del sólido se puede determinar la velocidad instantánea del resto de puntos del sólido.  En el tratamiento angular de los movimientos circulares, se produce una velocidad angular a la variación del ángulo -o posición angular- en el tiempo. La velocidad angular se representa por la letra griega     ω   {\displaystyle \omega }   y se mide, en el SI, en radianes por segundo (rad/s). Se define matemáticamente como:  Porque la velocidad angular ya está completamente determinada y no tiene que ver con el valor que se ha expresado antes. Falta identificar en qué lugar de la trayectoria está el punto y en qué sentimiento gira. Esto se hace con el vector velocidad angular. La magnitud anterior es el módulo de este vector. La dirección es la de la recta perpendicular al plano de la trayectoria y el sentido de avance que gira en el mismo sentido que el punto.  En un movimiento circular la velocidad angular se relaciona con la velocidad tangencial a través de las expresiones:  Donde R es el radio de la circunferencia que describe la trayectoria y la velocidad angular está expresada en radianes por segundo.  Composición de velocidades La composición de velocidades consiste en calcular la velocidad que tiene un punto medido en un determinado sistema de referencia, por ejemplo un sistema galileano, respecto del que se dice que es la velocidad absoluta vab, a partir de la velocidad respectiva de un otro sistema de referencia que se hace respecto a la primera velocidad relativa anomenada v rel saben el movimiento del sistema de referencia relativo respecto al absoluto.  En general es té:[6]  donde 'varr se  denomina 'velocidad de arrastre' "
fisica,"En física, la aceleración es una magnitud derivada vectorial que nos indica la variación de velocidad por unidad de tiempo. En el contexto de la mecánica vectorial newtoniana se representa normalmente por        a →       {\displaystyle {\vec {a}}\,}   o      a     {\displaystyle \mathbf {a} \,}   y su módulo por     a    {\displaystyle a\,}  .  Las aceleraciones son cantidades vectoriales (en el sentido de que tienen magnitud y dirección).[1]​[2]​  La magnitud de la aceleración de un objeto, como la describe la Segunda Ley de Newton,[3]​ es el efecto combinado de dos causas:  Sus dimensiones son      [ L ] ⋅ [  T  − 2   ]    {\displaystyle \scriptstyle [L]\cdot [T^{-2}]}  . Su unidad en el Sistema Internacional es m/s².  Por ejemplo, cuando un vehículo arranca estando detenido (velocidad cero, en un marco de referencia inercial) y viaja en línea recta a velocidades crecientes, está acelerando en la dirección de la marcha. Si el vehículo gira, se produce una aceleración hacia la nueva dirección y cambia su vector de movimiento. La aceleración del vehículo en su dirección actual de movimiento se llama aceleración lineal (o tangencial durante los movimientos circulares), la reacción que experimentan los pasajeros a bordo como una fuerza que los empuja hacia atrás en sus asientos. Al cambiar de dirección, la aceleración que efectúa se llama aceleración radial (ortogonal durante los movimientos circulares), la reacción que experimentan los pasajeros como una fuerza centrífuga. Si la velocidad del vehículo disminuye, esto es una aceleración en la dirección opuesta y matemáticamente negativa, a veces llamada desaceleración, y los pasajeros experimentan la reacción a la desaceleración como una fuerza inercial que los empuja hacia adelante. Estas aceleraciones negativas a menudo se logran mediante la combustión de retrocohetes en naves espaciales.[4]​ Tanto la aceleración como la desaceleración se tratan de la misma manera, ambos son cambios de velocidad. Los pasajeros sienten cada una de estas aceleraciones (tangencial, radial, desaceleración) hasta que su velocidad relativa (diferencial) se neutraliza con respecto al vehículo.  Enfoque intuitivo Así como la velocidad describe la modificación de la posición de un objeto en el tiempo, la aceleración describe la «modificación de la velocidad en el tiempo» (que las matemáticas formalizan con la noción de derivada ). En la vida cotidiana, hay tres casos que el físico agrupa bajo el concepto único de aceleración:  desde un punto de vista matemático, la aceleración es positiva, es decir que el vector de aceleración tiene una componente en la dirección de la velocidad;  la aceleración es negativa, o el vector de aceleración tiene una componente opuesta a la dirección de la velocidad;  el vector de aceleración tiene una componente perpendicular a la velocidad; aquí nos interesa la variación de la dirección del vector velocidad, no la variación de su norma.  Cuando una persona está sometida a una aceleración, se siente un esfuerzo: fuerza que presiona contra el asiento cuando el coche acelera (va más rápido), fuerza que empuja hacia el parabrisas cuando el coche frena, fuerza que empuja a un lado cuando el coche frena o está girando (fuerza centrífuga). Se siente esta tensión de manera similar al peso. La relación entre aceleración y esfuerzo es el dominio de la dinámica ; pero la aceleración es una noción de cinemática, es decir que se define solo a partir del movimiento, sin involucrar las fuerzas.  Introducción Según la mecánica newtoniana, una partícula no puede seguir una trayectoria curva a menos que sobre ella actúe una cierta aceleración como consecuencia de la acción de una fuerza, ya que si esta no existiese, su movimiento sería rectilíneo. Asimismo, una partícula en movimiento rectilíneo solo puede cambiar su velocidad bajo la acción de una aceleración en la misma dirección de su velocidad (dirigida en el mismo sentido si acelera; o en sentido contrario si desacelera).       a  =         d   V           d  t        {\displaystyle \mathbf {a} ={\cfrac {d\,\mathbf {V} }{d\,t}}}    En la mecánica newtoniana, para un cuerpo con masa constante, la aceleración del cuerpo medida por un observador inercial es proporcional a la fuerza que actúa sobre el mismo (segunda ley de Newton):       F  = m  a   →   a  =          F           m        {\displaystyle \mathbf {F} =m\mathbf {a} \quad \to \quad \mathbf {a} ={\cfrac {\mathbf {F} }{m}}}    donde F es la fuerza resultante que actúa sobre el cuerpo, m es la masa del cuerpo, y a es la aceleración. La relación anterior es válida en cualquier sistema de referencia inercial.  Algunos ejemplos del concepto de aceleración son:      v = a t = g t = 9 , 8  t   {\displaystyle v=at=gt=9,8\,t}    Aceleración en cinemática puntual El caso más común El vector de aceleración de un punto material en cualquier momento se encuentra mediante una diferenciación temporal única del vector velocidad de un punto material (o diferenciación doble del vector radio )  Si se conocen las coordenadas del punto en la trayectoria        r →    (  t  0   ) =     r →     0     {\displaystyle {\vec {r}}(t_{0})={\vec {r}}_{0}}   y el vector de velocidad        v →    (  t  0   ) =     v →     0     {\displaystyle {\vec {v}}(t_{0})={\vec {v}}_{0}}   en cualquier momento del tiempo t0,así como la dependencia de la aceleración en el tiempo        a →    ( t ) ,   {\displaystyle {\vec {a}}(t),}   al integrar esta ecuación, se pueden obtener las coordenadas y la velocidad del punto en cualquier momento del tiempo t (tanto antes como después del momento t0)  La derivada temporal de la aceleración, es decir, el valor que caracteriza la tasa de cambio en la aceleración, se llama Sobreaceleración:  Aceleración media e instantánea Para cada instante o punto de la trayectoria, queda definido un vector velocidad que, en general, cambia tanto en módulo como en dirección al pasar de un punto a otro de la trayectoria. La dirección de la velocidad cambiará debido a que la velocidad es tangente a la trayectoria y esta, por lo general, no es rectilínea. En la Figura se representan los vectores velocidad correspondientes a los instantes t y t+Δt, cuando la partícula pasa por los puntos P y Q, respectivamente. El cambio vectorial en la velocidad de la partícula durante ese intervalo de tiempo está indicado por Δv, en el triángulo vectorial al pie de la figura. Se define la aceleración media de la partícula, en el intervalo de tiempo Δt, como el cociente:      ⟨  a  ⟩ =    a ¯    =    Δ  v    Δ t      {\displaystyle \langle \mathbf {a} \rangle =\mathbf {\bar {a}} ={\frac {\Delta \mathbf {v} }{\Delta t}}}    Que es un vector paralelo a Δv y dependerá de la duración del intervalo de tiempo Δt considerado. La aceleración instantánea se la define como el límite al que tiende el cociente incremental Δv/Δt cuando Δt→0; esto es la derivada del vector velocidad con respecto al tiempo:       a  =  lim  Δ t → 0      Δ  v    Δ t    =    d  v    d t      {\displaystyle \mathbf {a} =\lim _{\Delta t\to 0}{\frac {\Delta \mathbf {v} }{\Delta t}}={\frac {d\mathbf {v} }{dt}}}    Puesto que la velocidad instantánea v a su vez es la derivada del vector posición r respecto al tiempo, la aceleración es la derivada segunda de la posición con respecto del tiempo:       a  =     d  2    r    d  t  2        {\displaystyle \mathbf {a} ={\frac {d^{2}\mathbf {r} }{dt^{2}}}}    De igual forma se puede definir la velocidad instantánea a partir de la aceleración como:       v  −   v   0   =  ∫   t  0     t    (     d   v     d  t    )    d  t   {\displaystyle \mathbf {v} -\mathbf {v} _{0}=\int _{t_{0}}^{t}\left({\mathrm {d} \mathbf {v}  \over \mathrm {d} t}\right)\,\mathrm {d} t}    Se puede obtener la velocidad a partir de la aceleración mediante integración:       v  =  ∫  0   t    a     d  t +   v   0     {\displaystyle \mathbf {v} =\int _{0}^{t}\mathbf {a} \ \mathrm {d} t+\mathbf {v} _{0}}    Medición de la aceleración La medida de la aceleración puede hacerse con un sistema de adquisición de datos y un simple acelerómetro. Los acelerómetros electrónicos son fabricados para medir la aceleración en una, dos o tres direcciones. Cuentan con dos elementos conductivos, separados por un material que varía su conductividad en función de las medidas, que a su vez serán relativas a la aceleración del conjunto.  Unidades Las unidades de la aceleración son:  Valores de aceleración en algunos casos Valores de las aceleraciones de varios movimientos:[5]​  Nota: aquí g ≈ 10 m/s².  Componentes intrínsecas de la aceleración: aceleraciones tangencial y normal En tanto que el vector velocidad v es tangente a la trayectoria, el vector aceleración a puede descomponerse en dos componentes (llamadas componentes intrínsecas) mutuamente perpendiculares: una componente tangencial at (en la dirección de la tangente a la trayectoria), llamada aceleración tangencial, y una componente normal an (en la dirección de la normal principal a la trayectoria), llamada aceleración normal o centrípeta (este último nombre en razón a que siempre está dirigida hacia el centro de curvatura).  Derivando la velocidad con respecto al tiempo, teniendo en cuenta que el vector tangente cambia de dirección al pasar de un punto a otro de la trayectoria (esto significa que no es constante) obtenemos       a  =    d  v    d t    =   d  d t    ( v      e ^     t   ) =    d v   d t        e ^     t   + v    d     e ^     t     d t    =  a  t       e ^     t   + v (  ω  ×     e ^     t   )   {\displaystyle \mathbf {a} ={\frac {d\mathbf {v} }{dt}}={\frac {d}{dt}}(v\,\mathbf {\hat {e}} _{t})={\frac {dv}{dt}}\mathbf {\hat {e}} _{t}+v{\frac {d\mathbf {\hat {e}} _{t}}{dt}}=a_{t}\mathbf {\hat {e}} _{t}+v({\boldsymbol {\omega }}\times \mathbf {\hat {e}} _{\text{t}})}    siendo         e ^     t     {\displaystyle \mathbf {\hat {e}} _{t}}   el vector unitario tangente a la trayectoria en la misma dirección que la velocidad y      ω    {\displaystyle {\boldsymbol {\omega }}}   la velocidad angular. Resulta conveniente escribir la expresión anterior en la forma       a  =    d  v    d t    =  a  t       e ^     t   +    v  2   ρ       e ^     n   =  a  t       e ^     t   +  a  n       e ^     n     {\displaystyle \mathbf {a} ={\frac {d\mathbf {v} }{dt}}=a_{t}\mathbf {\hat {e}} _{t}+{\frac {v^{2}}{\rho }}\mathbf {\hat {e}} _{n}=a_{t}\mathbf {\hat {e}} _{t}+a_{n}\mathbf {\hat {e}} _{\text{n}}}    siendo  Las magnitudes de estas dos componentes de la aceleración son:       a  t   =    d v   d t        a  n   =    v  2   ρ     {\displaystyle a_{t}={\frac {dv}{dt}}\qquad \qquad \qquad a_{n}={\frac {v^{2}}{\rho }}}    Cada una de estas dos componentes de la aceleración tiene un significado físico bien definido. Cuando una partícula se mueve, su velocidad puede cambiar y este cambio lo mide la aceleración tangencial. Pero si la trayectoria es curva también cambia la dirección de la velocidad y este cambio lo mide la aceleración normal.  Los vectores que aparecen en las expresiones anteriores son los vectores del triedro de Frênet que aparece en la geometría diferencial de curvas del siguiente modo:  Movimiento circular uniforme Un movimiento circular uniforme es aquel en el que la partícula recorre una trayectoria circular de radio R con rapidez constante, es decir, que la distancia recorrida en cada intervalo de tiempo igual es la misma. Para ese tipo de movimiento el vector de velocidad mantiene su módulo y va variando la dirección siguiendo una trayectoria circular. Si se aplican las fórmulas anteriores, se tiene que la aceleración tangencial es nula y la aceleración normal es constante: a esta aceleración normal se la llama «aceleración centrípeta». En este tipo de movimiento la aceleración aplicada al objeto se encarga de modificar la trayectoria del objeto y no en modificar su velocidad.       a  =    d  v    d t    =    d v   d t        e ^     t   +    v  2   R       e ^     n   = 0 ⋅     e ^     t   +    v  2   R        e  ^     n   =  ω  2   R        e  ^     n     {\displaystyle \mathbf {a} ={\frac {d\mathbf {v} }{dt}}={\frac {dv}{dt}}\mathbf {\hat {e}} _{t}+{\frac {v^{2}}{R}}\mathbf {\hat {e}} _{n}=0\cdot \mathbf {\hat {e}} _{t}+{\frac {v^{2}}{R}}{\hat {\mathbf {e} }}_{n}=\omega ^{2}R\ {\hat {\mathbf {e} }}_{n}}    Movimiento rectilíneo acelerado Si se aplican las fórmulas anteriores al movimiento rectilíneo, en el que solo existe aceleración tangencial, al estar todos los vectores contenidos en la trayectoria, podemos prescindir de la notación vectorial y escribir simplemente:      a =    d v   d t      {\displaystyle a={\frac {dv}{dt}}}    Ya que en ese tipo de movimiento los vectores       a     {\displaystyle \scriptstyle \mathbf {a} }   y       v     {\displaystyle \scriptstyle \mathbf {v} }   son paralelos, satisfaciendo también la relación:      v ( t ) =  v  0   +  ∫  0   t   a ( τ )   d τ   {\displaystyle v(t)=v_{0}+\int _{0}^{t}a(\tau )\ d\tau }    La coordenadas de posición viene dada en este caso por:      x ( t ) =  x  0   +  v  0   t +  ∫  0   t   ( t − τ ) a ( τ )   d τ   {\displaystyle x(t)=x_{0}+v_{0}t+\int _{0}^{t}(t-\tau )a(\tau )\ d\tau }    Un caso particular de movimiento rectilíneo acelerado es el movimiento rectilíneo uniformemente acelerado donde la aceleración es además constante y por tanto la velocidad y la coordenadas de posición vienen dados por:      v ( t ) =  v  0   + a t ,  x ( t ) =  x  0   +  v  0   t +    a  t  2    2     {\displaystyle v(t)=v_{0}+at,\qquad x(t)=x_{0}+v_{0}t+{\frac {at^{2}}{2}}}    Aceleración en mecánica relativista Relatividad especial El análogo de la aceleración en mecánica relativista se llama cuadriaceleración y es un cuadrivector cuyas tres componentes espaciales para pequeñas velocidades coinciden con las de la aceleración newtoniana (la componente temporal para pequeñas velocidades resulta proporcional a la potencia de la fuerza dividida por la velocidad de la luz y la masa de la partícula).  En mecánica relativista la cuadrivelocidad y la cuadriaceleración son siempre ortogonales, eso se sigue de que la cuadrivelocidad tiene un (pseudo)módulo constante:       U  ⋅  U  =  c  2   ⇒ 2  U  ⋅    d  U    d τ    = 0 ⇒ 2  U  ⋅  A  = 0   {\displaystyle \mathbf {U} \cdot \mathbf {U} =c^{2}\Rightarrow 2\mathbf {U} \cdot {\frac {d\mathbf {U} }{d\tau }}=0\Rightarrow 2\mathbf {U} \cdot \mathbf {A} =0}    Donde c es la velocidad de la luz y el producto anterior es el producto asociado a la métrica de Minkowski:      V ⋅ W := η ( V , W ) =  η  μ ν    V  μ    V  ν     {\displaystyle V\cdot W:=\eta (V,W)=\eta _{\mu \nu }V^{\mu }V^{\nu }}    Relatividad general En teoría general de la relatividad el caso de la aceleración es más complicado, ya que debido a que el propio espacio-tiempo es curvo (ver curvatura del espacio-tiempo), una partícula sobre la que no actúa ninguna fuerza puede seguir una trayectoria curva, de hecho la línea curva que sigue una partícula sobre la que no actúa ninguna fuerza exterior es una línea geodésica, de hecho en relatividad general la fuerza gravitatoria no se interpreta como una fuerza sino como un efecto de la curvatura del espacio-tiempo que hace que las partículas no trayectorias rectas sino líneas geodéscias. En este contexto la aceleración no geodésica de una partícula es un vector cuyas cuatro componentes se calulan como:       A  α   =    d  U  α     d τ    +  ∑  β , γ    Γ  β γ   α    U  β    U  γ     {\displaystyle A^{\alpha }={\frac {dU^{\alpha }}{d\tau }}+\sum _{\beta ,\gamma }\Gamma _{\beta \gamma }^{\alpha }U^{\beta }U^{\gamma }}    Aquí      α = 0 , 1 , 2 , 3    {\displaystyle \scriptstyle \alpha =0,1,2,3}   (componente temporal y tres componentes espaciales). Se aprecia que cuando los símbolos de Christoffel      Γ  β γ   α     {\displaystyle \Gamma _{\beta \gamma }^{\alpha }}   una partícula puede tener aceleración cero aunque su cuadrivelocidad no sea constante, eso sucede cuando la partícula sigue una línea geodésica de un espacio-tiempo de curvatura no nula.  Importancia de la aceleración en Ingeniería Mecánica La ingeniería mecánica es el diseño y fabricación de máquinas , es decir, sistemas que realizan movimientos. Una parte importante es el dimensionamiento, es decir la elección de actuadores ( gatos , motores ) y piezas que soportan las fuerzas. Si las masas puestas en movimiento y / o las aceleraciones son grandes, los efectos dinámicos - las fuerzas necesarias para crear las aceleraciones, o las fuerzas resultantes de las aceleraciones - no son despreciables. Por tanto, determinar la aceleración instantánea durante un movimiento es fundamental para que las piezas resistan y para determinar el consumo energético del sistema.   En muchos casos, la especificación es ""llevar un objeto del punto A al punto B en un tiempo t, con el tiempo t a veces expresado como una tasa (realizando el movimiento n veces por hora). El diseño consiste en :  Véase también Referencias Bibliografía Enlaces externos"
fisica,"El colapso de la función de onda es un proceso físico relacionado con el problema de la medida de la mecánica cuántica consistente en la variación abrupta del estado de un sistema después de haber obtenido una medida.  La naturaleza de dicho proceso es intensamente discutida en diferentes interpretaciones de la mecánica cuántica. Algunos autores sostienen que el proceso de decoherencia cuántica de hecho podría explicar cómo aparentemente el estado de un sistema «colapsa» de acuerdo con el postulado IV de la mecánica cuántica, aunque realmente el sistema formado por el sistema cuántico más el resto de universo, incluyendo el aparato de medida, no ha sufrido efectivamente un «colapso». En esta interpretación el colapso sería aparente, mientras que la función de onda global del universo habría seguido evolucionando de manera unitaria.  Introducción El aspecto no local de la naturaleza sugerido por el teorema de Bell, se ajusta a la teoría cuántica por medio del colapso de la función de onda, que es un cambio repentino y global de la función de onda como sistema. Se produce cuando alguna parte del sistema es observada. Es decir, cuando se hace una observación/medición del sistema en una región, la función de onda varía instantáneamente, y no solo en esa región de la medida sino en cualquier otra por muy distante que esté.  En la interpretación de Copenhague, este comportamiento se considera natural en una función que describe probabilidades. Puesto que las probabilidades dependen de lo que se conoce como el sistema, si el conocimiento que se tiene del sistema cambia como consecuencia del resultado de una observación, en ese caso la función de probabilidad deberá cambiar. Por esta razón, ante el aumento de información, un cambio de la función de probabilidad en una región distante es normal incluso en la física clásica. Refleja el hecho de que las partes de un sistema están correlacionadas entre sí y, por lo tanto, un incremento de la información aquí está acompañado por un incremento de la función del sistema en cualquier otra parte. Sin embargo en la teoría cuántica este colapso de la función de onda es tal que aquello que ocurre en un lugar distante, en muchos casos tiene que depender de lo que el observador eligió observar. Lo que uno ve allí depende de lo que yo hago aquí. Este es un efecto completamente no-local, no-clásico (véase entrelazamiento cuántico)."
fisica,"La electricidad (del griego ήλεκτρον élektron, cuyo significado es ‘ámbar’)[1]​ es el conjunto de fenómenos físicos relacionados con la presencia y flujo de cargas eléctricas. Se manifiesta en una gran variedad de fenómenos como los rayos, la electricidad estática, la inducción electromagnética o el flujo de corriente eléctrica. Es una forma de energía tan versátil que tiene un sinnúmero de aplicaciones, por ejemplo: transporte, climatización, iluminación e Informática.[2]​  La electricidad se manifiesta mediante varios fenómenos y propiedades físicas:  Historia La historia de la electricidad se refiere al estudio de la electricidad, al descubrimiento de sus leyes como fenómeno físico y a la invención de artefactos para su uso práctico. Como también se denomina electricidad a la rama de la ciencia que estudia el fenómeno y a la rama de la tecnología que lo aplica, la historia de la electricidad es la rama de la historia de la ciencia y de la historia de la tecnología que se ocupa de su surgimiento y evolución. El fenómeno de la electricidad se ha estudiado desde la antigüedad, pero su estudio científico comenzó en los siglos XVII y XVIII. A finales del siglo XIX, los ingenieros lograron aprovecharla para uso doméstico e industrial. La rápida expansión de la tecnología eléctrica la convirtió en la columna vertebral de la sociedad industrial moderna.[3]​  Mucho antes de que existiera algún conocimiento sobre la electricidad, la humanidad era consciente de las descargas eléctricas producidas por peces eléctricos. Textos del Antiguo Egipto que datan del 2750 a. C. se referían a estos peces como «los tronadores del Nilo», descritos como los protectores de los otros peces. Posteriormente, los peces eléctricos también fueron descritos por los romanos, griegos, árabes, naturalistas y físicos.[4]​ Autores antiguos como Plinio el Viejo o Escribonio Largo,[5]​[6]​ describieron el efecto adormecedor de las descargas eléctricas producidas por peces eléctricos y rayas eléctricas. Además, sabían que estas descargas podían transmitirse por materias conductoras.[7]​ Los pacientes de enfermedades como la gota y el dolor de cabeza se trataban con peces eléctricos, con la esperanza de que la descarga pudiera curarlos.[6]​ La primera aproximación al estudio del rayo y a su relación con la electricidad se atribuye a los árabes, que antes del siglo XV tenían una palabra para rayo (raad) aplicado a la raya eléctrica.[8]​  En culturas antiguas del Mediterráneo se sabía que al frotar ciertos objetos, como una barra de ámbar, con lana o piel, se obtenían pequeñas cargas (efecto triboeléctrico) que atraían pequeños objetos, y frotando mucho tiempo podía causar la aparición de una chispa. Cerca de la antigua ciudad griega de Magnesia se encontraban las denominadas piedras de Magnesia, que incluían magnetita y los antiguos griegos observaron que los trozos de este material se atraían entre sí, y también a pequeños objetos de hierro. Las palabras magneto (equivalente en español a imán) y magnetismo derivan de ese topónimo. Hacia el año 600 a. C., el filósofo griego Tales de Mileto hizo una serie de observaciones sobre electricidad estática. Concluyó que la fricción dotaba de magnetismo al ámbar, al contrario que minerales como la magnetita, que no necesitaban frotarse.[9]​[10]​[11]​ Tales se equivocó al creer que esta atracción la producía un campo magnético, aunque más tarde la ciencia probaría la relación entre el magnetismo y la electricidad. Según una teoría controvertida, los partos podrían haber conocido la electrodeposición, basándose en el descubrimiento en 1936 de la batería de Bagdad,[12]​ similar a una celda voltaica, aunque es dudoso que el artefacto fuera de naturaleza eléctrica.[13]​  Esas especulaciones y registros fragmentarios fueron el tratamiento casi exclusivo (con la notable excepción del uso del magnetismo para la brújula) que hay desde la Antigüedad hasta la Revolución científica del siglo XVII; aunque todavía entonces pasaba por ser poco más que una curiosidad para mostrar en los salones. Las primeras aportaciones que pueden entenderse como aproximaciones sucesivas al fenómeno eléctrico fueron realizadas como William Gilbert, que realizó un estudio cuidadoso de electricidad y magnetismo. Diferenció el efecto producido por trozos de magnetita, de la electricidad estática producida al frotar ámbar.[11]​ Además, acuñó el término neolatino electricus (que, a su vez, proviene de ήλεκτρον [elektron], la palabra griega para ámbar) para referirse a la propiedad de atraer pequeños objetos después de haberlos frotado.[14]​ Esto originó los términos eléctrico y electricidad, que aparecen por vez primera en 1646 en la publicación Pseudodoxia Epidemica de Thomas Browne.[15]​  Esos estudios fueron seguidas por investigadores sistemáticos como von Guericke, Cavendish,[16]​[17]​ Du Fay,[18]​ van Musschenbroek[19]​ (botella de Leyden) o William Watson.[20]​ Las observaciones sometidas a método científico empiezan a dar sus frutos con Galvani,[21]​ Volta,[22]​ Coulomb[23]​ y Franklin,[24]​ y, ya a comienzos del siglo XIX, con Ampère,[25]​ Faraday[26]​ y Ohm. Los nombres de estos pioneros terminaron bautizando las unidades hoy utilizadas en la medida de las distintas magnitudes del fenómeno. La comprensión final de la electricidad se logró recién con su unificación con el magnetismo en un único fenómeno electromagnético descrito por las ecuaciones de Maxwell (1861-1865).[27]​  Los desarrollos tecnológicos que produjeron la Primera Revolución Industrial no hicieron uso de la electricidad. Su primera aplicación práctica generalizada fue el telégrafo eléctrico de Samuel Morse (1833) —precedido por Gauss y Weber, 1822—, que revolucionó las telecomunicaciones.[28]​ La generación industrial de electricidad comenzó partir del cuarto final del siglo XIX, cuando se extendió la iluminación eléctrica de las calles y de las viviendas. La creciente sucesión de aplicaciones de esta forma de energía hizo de la electricidad una de las principales fuerzas motrices de la Segunda Revolución Industrial.[29]​ Más que de grandes teóricos como lord Kelvin, fue el momento de grandes ingenieros e inventores, como Gramme,[30]​ Tesla, Sprague, Westinghouse,[31]​ von Siemens[32]​ Graham Bell,[33]​ y, sobre todo, Alva Edison y su revolucionaria manera de entender la relación entre investigación científico-técnica y mercado capitalista, que convirtió la innovación tecnológica en una actividad industrial.[34]​[35]​ Los sucesivos cambios de paradigma de la primera mitad del siglo XX (relativista y cuántico) estudiarán la función de la electricidad en una nueva dimensión: atómica y subatómica.  La electrificación no solo fue un proceso técnico, sino un verdadero cambio social de implicaciones extraordinarias, comenzando por el alumbrado y siguiendo por todo tipo de procesos industriales (motor eléctrico, metalurgia, refrigeración...) y de comunicaciones (telefonía, radio). Lenin, durante la Revolución bolchevique, definió el socialismo como la suma de la electrificación y el poder de los soviets,[36]​ pero fue sobre todo la sociedad de consumo que nació en los países capitalistas, la que dependió en mayor medida de la utilización doméstica de la electricidad en los electrodomésticos, y fue en estos países donde la retroalimentación entre ciencia, tecnología y sociedad desarrolló las complejas estructuras que permitieron los actuales sistemas de I+D e I+D+I, en que la iniciativa pública y privada se interpenetran, y las figuras individuales se difuminan en los equipos de investigación.  La energía eléctrica es esencial para la sociedad de la información de la tercera revolución industrial que se viene produciendo desde la segunda mitad del siglo XX (transistor, televisión, computación, robótica, internet...). Únicamente puede comparársele en importancia la motorización dependiente del petróleo (que también es ampliamente utilizado, como los demás combustibles fósiles, en la generación de electricidad). Ambos procesos exigieron cantidades cada vez mayores de energía, lo que está en el origen de la crisis energética y medioambiental y de la búsqueda de nuevas fuentes de energía, la mayoría con inmediata utilización eléctrica (energía nuclear y energías alternativas, dadas las limitaciones de la tradicional hidroelectricidad). Los problemas que tiene la electricidad para su almacenamiento y transporte a largas distancias, y para la autonomía de los aparatos móviles, son retos técnicos aún no resueltos de forma suficientemente eficaz.  El impacto cultural de lo que Marshall McLuhan denominó Edad de la Electricidad, que seguiría a la Edad de la Mecanización (por comparación a cómo la Edad de los Metales siguió a la Edad de Piedra), radica en la altísima velocidad de propagación de la radiación electromagnética (300 000 km/s) que hace que se perciba de forma casi instantánea. Este hecho conlleva posibilidades antes inimaginables, como la simultaneidad y la división de cada proceso en una secuencia. Se impuso un cambio cultural que provenía del enfoque en «segmentos especializados de atención» (la adopción de una perspectiva particular) y la idea de la «conciencia sensitiva instantánea de la totalidad», una atención al «campo total», un «sentido de la estructura total». Se hizo evidente y prevalente el sentido de «forma y función como una unidad», una «idea integral de la estructura y configuración». Estas nuevas concepciones mentales tuvieron gran impacto en todo tipo de ámbitos científicos, educativos e incluso artísticos (por ejemplo, el cubismo). En el ámbito de lo espacial y político, «la electricidad no centraliza, sino que descentraliza... mientras que el ferrocarril requiere un espacio político uniforme, el avión y la radio permiten la mayor discontinuidad y diversidad en la organización espacial».[37]​  Coulomb (1736-1806), estableció las leyes cuantitativas de la electrostática  Galvani (1737-1798), famoso por sus investigaciones sobre los efectos de la electricidad en los músculos de los animales  Volta (1745-1827), inventor de la pila  Ampère (1775-1836), uno de los descubridores del electromagnetismo  Faraday (1791-1867), descubridor de la inducción electromagnética  Usos La electricidad se usa para generar:  Conceptos Carga eléctrica La carga eléctrica es una propiedad de la materia que se manifiesta mediante fuerzas de atracción y repulsión. La carga se origina en el átomo, que está compuesto de partículas subatómicas cargadas como el electrón y el protón.[38]​ La carga puede transferirse entre los cuerpos por contacto directo o al pasar por un material conductor, generalmente metálico.[39]​ El término electricidad estática se refiere a la presencia de carga en un cuerpo, por lo general causado por dos materiales distintos que se frotan entre sí, transfiriéndose carga uno al otro.[40]​  La presencia de carga da lugar a la fuerza electromagnética: una carga ejerce una fuerza sobre las otras. Este efecto era conocido en la antigüedad, pero no comprendido.[41]​ Una bola liviana, suspendida de un hilo, podía cargarse al contacto con una barra de vidrio cargada previamente por fricción con un tejido. Se encontró que si una bola similar se cargaba con la misma barra de vidrio, se repelían entre sí. A finales del siglo XVIII, Charles-Augustin de Coulomb investigó este fenómeno. Dedujo que la carga se manifiesta de dos formas opuestas.[42]​ Este descubrimiento trajo el conocido axioma «objetos con la misma polaridad se repelen y con diferente polaridad se atraen».[41]​[43]​  La fuerza actúa en las partículas cargadas entre sí, y además la carga tiene tendencia a extenderse sobre una superficie conductora. La magnitud de la fuerza electromagnética, ya sea atractiva o repulsiva, se expresa por la ley de Coulomb, que relaciona la fuerza con el producto de las cargas y tiene una relación inversa al cuadrado de la distancia entre ellas.[44]​[45]​ La fuerza electromagnética es muy fuerte, la segunda después de la interacción nuclear fuerte,[46]​ con la diferencia que esa fuerza opera sobre todas las distancias.[47]​ En comparación con la débil fuerza gravitacional, la fuerza electromagnética que aleja a dos electrones es 1042 veces más grande que la atracción gravitatoria que los une.[48]​  Una carga puede expresarse como positiva o negativa. Las cargas de los electrones y de los protones tienen signos contrarios. Por convención, la carga que tiene electrones se asume negativa y la de los protones, positiva, una costumbre que empezó con el trabajo de Benjamin Franklin.[49]​ La cantidad de carga se representa por el símbolo Q y se expresa en culombios.[50]​ Todos los electrones tienen la misma carga, aproximadamente de -1.6022×10−19 culombios. El protón tiene una carga igual pero de signo opuesto +1.6022×10−19 coulombios. La carga no solo está presente en la materia, sino también en la antimateria: cada antipartícula tiene una carga igual y opuesta a su correspondiente partícula.[51]​  La carga puede medirse de diferentes maneras. Un instrumento muy antiguo es el electroscopio, que aún se usa para demostraciones en las aulas, aunque ahora está superado por el electrómetro electrónico.[52]​  Corriente eléctrica Se conoce como corriente eléctrica al desplazamiento de cargas eléctricas por un conductor. La corriente puede estar producida por cualquier partícula cargada eléctricamente en movimiento. Lo más frecuente es que sean electrones, pero cualquier otra carga en movimiento se puede definir como corriente.[53]​ Según el Sistema Internacional, la intensidad de una corriente eléctrica se mide en amperios, cuyo símbolo es A.[54]​  Históricamente, la corriente eléctrica se definió como un flujo de cargas positivas y se fijó como sentido convencional de circulación de la corriente el flujo de cargas desde el polo positivo al negativo. Más adelante se observó que, en los metales, los portadores de carga son electrones, con carga negativa, y que se desplazan en sentido contrario al convencional.[55]​ Lo cierto es que, dependiendo de las condiciones, una corriente eléctrica puede consistir en un flujo de partículas cargadas en una dirección, o incluso simultáneamente en ambas direcciones. La convención positivo-negativo se usa normalmente para simplificar esta situación.[53]​  El proceso por el cual la corriente eléctrica circula por un material se llama conducción eléctrica. Su naturaleza varía, dependiendo de las partículas cargadas y el material por el cual están circulando. Ejemplos de corrientes eléctricas son la conducción metálica, donde los electrones recorren un conductor eléctrico, como un metal; y la electrólisis, donde los iones (átomos cargados) fluyen a través de líquidos. Mientras que las partículas pueden moverse muy despacio, algunas veces con una velocidad media de deriva de solo fracciones de milímetro por segundo,[56]​ el campo eléctrico que las controla se propaga cercano a la velocidad de la luz, permitiendo que las señales eléctricas se transmitan rápidamente por los cables.[57]​  La corriente produce muchos efectos visibles, que han hecho que su presencia se reconozca a lo largo de la historia. En 1800, Nicholson y Carlisle descubrieron que el agua podía descomponerse por la corriente de una pila voltaica, en un proceso que se conoce como electrólisis. En 1833, Michael Faraday amplió este trabajo.[58]​ En 1840, James Prescott Joule descubrió que la corriente a través de una resistencia eléctrica aumenta la temperatura, fenómeno que en la actualidad se denomina Efecto Joule.[58]​  Campo eléctrico El concepto de campo eléctrico fue introducido por Michael Faraday. Un campo eléctrico se crea por un cuerpo cargado en el espacio que lo rodea, y produce una fuerza que ejerce sobre otras cargas ubicadas en el campo. Un campo eléctrico actúa entre dos cargas de modo muy parecido al campo gravitatorio que actúa sobre dos masas. Como él, se extiende hasta el infinito y su valor es inversamente proporcional al cuadrado de la distancia.[47]​ Sin embargo, hay una diferencia importante: mientras la gravedad siempre actúa como atracción, el campo eléctrico puede producir atracción o repulsión. Si un cuerpo grande como un planeta no tiene carga neta, el campo eléctrico a una distancia determinada es cero. Por ello, la gravedad es la fuerza dominante en el universo, a pesar de ser mucho más débil.[48]​  Un campo eléctrico varía en el espacio, y su intensidad en cualquier punto se define como la fuerza (por unidad de carga) que percibiría una carga si estuviera ubicada en ese punto.[59]​ La carga de prueba debe de ser insignificante, para evitar que su propio campo afecte el campo principal y también debe ser estacionaria para evitar el efecto de los campos magnéticos. Como el campo eléctrico se define en términos de fuerza, y una fuerza es un vector, entonces el campo eléctrico también es un vector, con magnitud y dirección. Específicamente, es un campo vectorial.[59]​  Potencial eléctrico El concepto de potencial eléctrico tiene mucha relación con el de campo eléctrico. Una carga pequeña ubicada en un campo eléctrico experimenta una fuerza, y para llevar esa carga a ese punto en contra de la fuerza necesita hacer un trabajo. El potencial eléctrico en cualquier punto se define como la energía requerida para mover una carga de ensayo ubicada en el infinito a ese punto.[60]​ Por lo general se mide en voltios, donde un voltio es el potencial que necesita un julio de trabajo para atraer una carga de un culombio desde el infinito. Esta definición formal de potencial tiene pocas aplicaciones prácticas. Un concepto más útil es el de diferencia de potencial, que se define como la energía requerida para mover una carga entre dos puntos específicos. El campo eléctrico tiene la propiedad especial de ser conservativo, es decir que no importa la trayectoria realizada por la carga de prueba; todas las trayectorias entre dos puntos específicos consumen la misma energía, y además con un único valor de diferencia de potencial.[60]​  Electromagnetismo Se denomina electromagnetismo a la teoría física que unifica los fenómenos eléctricos y magnéticos. Sus fundamentos son obra de Faraday, pero fueron formulados por primera vez de modo completo por Maxwell,[61]​[62]​ mediante cuatro ecuaciones diferenciales vectoriales, conocidas como ecuaciones de Maxwell. Relacionan el campo eléctrico, el campo magnético y sus respectivas fuentes materiales: densidad de carga eléctrica, corriente eléctrica, desplazamiento eléctrico y corriente de desplazamiento.[63]​  A principios del siglo XIX, Ørsted encontró evidencia empírica de que los fenómenos magnéticos y eléctricos estaban relacionados. A partir de esa base, en 1861 Maxwell unificó los trabajos de Ampère, Sturgeon, Henry, Ohm y Faraday, en un conjunto de ecuaciones que describían ambos fenómenos como uno solo, el fenómeno electromagnético.[64]​  Se trata de una teoría de campos. Sus explicaciones y predicciones se basan en magnitudes físicas vectoriales, que dependen de la posición en el espacio y del tiempo. El electromagnetismo describe los fenómenos físicos macroscópicos en los que intervienen cargas eléctricas en reposo y en movimiento, usando para ello campos eléctricos y magnéticos y sus efectos sobre la materia.  Circuitos eléctricos Un circuito eléctrico es una interconexión de dos o más componentes eléctricos tales que la carga eléctrica fluye en una trayectoria cerrada, por lo general para ejecutar alguna tarea útil.[65]​  Los componentes en un circuito eléctrico pueden ser muy variados, puede tener elementos como resistores, capacitores, interruptores, enchufes, transformadores y electrónicos. Los circuitos electrónicos contienen componentes activos, normalmente semiconductores, exhibiendo un comportamiento no lineal, que requiere análisis complejos. Los componentes eléctricos más simples son los pasivos y lineales.[66]​  El comportamiento de los circuitos eléctricos que contienen solamente resistencias y fuentes electromotrices de corriente continua está gobernado por las leyes de Kirchhoff. Para estudiarlo, el circuito se descompone en mallas eléctricas, estableciendo un sistema de ecuaciones lineales cuya resolución brinda los valores de las tensiones y corrientes que entran o salen de sus nudos.[67]​  La resolución de circuitos de corriente alterna requiere la ampliación del concepto de resistencia eléctrica, ahora ampliado por el de impedancia para incluir los comportamientos de bobinas y condensadores. La resolución de estos circuitos puede hacerse con generalizaciones de las leyes de Kirchoff, pero requiere usualmente métodos matemáticos avanzados, como el de Transformada de Laplace, para describir los comportamientos transitorios y estacionarios de los mismos.[67]​  Propiedades de la electricidad Origen microscópico La posibilidad de transmitir una corriente eléctrica en los materiales depende de la estructura e interacción de los átomos que los componen. Los átomos están constituidos por partículas cargadas positivamente (los protones), negativamente (los electrones) y neutras (los neutrones). La conducción eléctrica en los conductores, semiconductores, y aislantes, se debe a los electrones de la órbita exterior o portadores de carga, ya que tanto los neutrones interiores como los protones de los núcleos atómicos no pueden desplazarse con facilidad. Los materiales conductores por excelencia son metales que normalmente tienen un único electrón en la última capa electrónica, como el cobre. Estos electrones pueden pasar con facilidad a átomos contiguos, constituyendo los electrones libres responsables del flujo de corriente eléctrica.[68]​  En todos los materiales sometidos a campos eléctricos se modifican, en mayor o menor grado, las distribuciones espaciales relativas de las cargas negativas y positivas. Este fenómeno se denomina polarización eléctrica y es más notorio en los aislantes eléctricos debido a que gracias a este fenómeno se impide liberar cargas, y por lo tanto no conducen, característica principal de estos materiales.[69]​  Conductividad y resistividad La conductividad eléctrica es la propiedad de los materiales que cuantifica la facilidad con que las cargas pueden moverse cuando un material es sometido a un campo eléctrico.[70]​ La resistividad es una magnitud inversa a la conductividad, aludiendo al grado de dificultad que encuentran los electrones en sus desplazamientos, dando una idea de lo buen o mal conductor que es.[68]​ Un valor alto de resistividad indica que el material es mal conductor mientras que uno bajo indicará que es un buen conductor. Generalmente la resistividad de los metales aumenta con la temperatura, mientras que la de los semiconductores disminuye ante el aumento de la temperatura.[68]​  Los materiales se clasifican según su conductividad eléctrica o resistividad en conductores, dieléctricos, semiconductores y superconductores.  Producción y usos de la electricidad Generación y transmisión Hasta la invención de la pila voltaica en el siglo XVIII (Alessandro Volta, 1800) no se tenía una fuente viable de electricidad. La pila voltaica (y sus descendientes modernos, la pila eléctrica y la batería eléctrica), almacenaba energía químicamente y la entregaba según la demanda en forma de energía eléctrica.[73]​ La batería es una fuente común muy versátil que se usa para muchas aplicaciones, pero su almacenamiento de energía es limitado, y una vez descargado debe ser recargada (o, en el caso de la pila, reemplazada). Para una demanda eléctrica mucho más grande la energía debe generarse y transmitirse continuamente por líneas de transmisión conductoras.[74]​  Por lo general, la energía eléctrica se genera mediante generadores electromecánicos que son dispositivos que utilizan el movimiento para mantener una diferencia de potencial eléctrica entre dos puntos. Es decir que transforman la energía mecánica en eléctrica. Esta transformación se consigue por la acción de un campo magnético sobre los conductores eléctricos. Si se produce mecánicamente un movimiento relativo entre los conductores y el campo, se generará una fuerza electromotriz (FEM). Este sistema está basado en la ley de Faraday. Para lograr el movimiento se utiliza el del aire (eólica), agua (hidráulica), vapor u otros gases (térmica). La moderna turbina de vapor inventada por Charles Algernon Parsons en 1884 genera cerca del 80 % de la energía eléctrica en el mundo usando una gran variedad de fuentes de energía.  Otro dispositivo que genera electricidad es la célula fotovoltaica, y lo hace directamente a partir de la radiación solar mediante un dispositivo semiconductor.  Los conductores de electricidad ofrecen siempre una resistencia al paso de esta, por pequeña que sea, de modo que en el transporte se pierde carga eléctrica; a mayor distancia, mayor pérdida. Un aumento de tensión significa una disminución de la intensidad que circula por la línea, para transportar la misma potencia, y por tanto, las pérdidas por calentamiento de los conductores y por efectos electromagnéticos y, en consecuencia, menor pérdida energética. En consecuencia pueden utilizarse secciones menores de los conductores que la transportan, por eso, para llevar electricidad a grandes distancias, se debe hacer en la llamada Alta Tensión. Por el contrario en el uso corriente se debe emplear una tensión más reducida (normalmente entre 110 V y 240 V) y eso implica cambios (tranformaciones) de tensión. La invención a finales del siglo XIX del transformador permitió transmitir la energía eléctrica de una forma más eficiente. La transmisión eléctrica eficiente hizo posible generar electricidad en plantas generadoras, para después trasportarla a largas distancias, donde fuera necesaria.[75]​  Debido a que la energía eléctrica no puede ser almacenada fácilmente para atender la demanda a una escala nacional, la mayoría de las veces se produce la misma cantidad que la que se demanda. Esto requiere de una bolsa eléctrica que hace predicciones de la demanda eléctrica, y mantiene una coordinación constante con las plantas generadoras. Se mantiene una cierta reserva de capacidad de generación en reserva para soportar cualquier anomalía en la red.[76]​  Aplicaciones de la electricidad La electricidad tiene un sinfín de aplicaciones tanto para uso doméstico, industrial, medicinal y en el transporte. Solo para citar se puede mencionar a la iluminación y alumbrado, electrodomésticos, producción de calor, electrónica, robótica, telecomunicaciones, señales luminosas, climatización, máquinas frigoríficas, electrosoldadura, electroimanes, electroquímica, electroválvulas. También se aplica la inducción electromagnética para la construcción de motores movidos por energía eléctrica, que permiten el funcionamiento de innumerables dispositivos.[77]​  Electricidad en la naturaleza Mundo inorgánico El fenómeno eléctrico más común del mundo inorgánico son las descargas eléctricas atmosféricas denominadas rayos y relámpagos. Debido al rozamiento de las partículas de agua o hielo con el aire, se produce la creciente separación de cargas eléctricas positivas y negativas en las nubes, separación que genera campos eléctricos. Cuando el campo eléctrico resultante supera la rigidez dieléctrica del medio, se produce una descarga entre dos partes de una nube, entre dos nubes diferentes o entre la parte inferior de una nube y tierra. Esta descarga ioniza el aire por calentamiento y excita transiciones electrónicas moleculares. La brusca dilatación del aire genera el trueno, mientras que el decaimiento de los electrones a sus niveles de equilibrio genera radiación electromagnética, es decir, luz.[78]​  Aunque no se puede verificar experimentalmente, la existencia del campo magnético terrestre se debe casi seguramente a la circulación de cargas en el núcleo externo líquido de la Tierra. La hipótesis de su origen en materiales con magnetización permanente, como el hierro, parece desmentida por la constatación de las inversiones periódicas de su sentido en el transcurso de las eras geológicas, donde el polo norte magnético es remplazado por el sur y viceversa. Medido en tiempos humanos, sin embargo, los polos magnéticos son estables, lo que permite su uso, mediante el antiguo invento chino de la brújula, para la orientación en el mar y en la tierra.[79]​  El campo magnético terrestre desvía las partículas cargadas provenientes del Sol (viento solar). Cuando esas partículas chocan con los átomos y moléculas de oxígeno y nitrógeno de la magnetosfera, se produce un efecto fotoeléctrico mediante el cual parte de la energía de la colisión excita los átomos a niveles de energía tales que cuando dejan de estar excitados devuelven esa energía en forma de luz visible. Este fenómeno puede observarse a simple vista en las cercanías de los polos, en las auroras polares.[80]​  Mundo orgánico El bioelectromagnetismo estudia el fenómeno consistente en la producción de campos electromagnéticos producidos por la materia viva (células, tejidos u organismos). Los ejemplos de este fenómeno incluyen el potencial eléctrico de las membranas celulares y las corrientes eléctricas que fluyen en nervios y músculos como consecuencia de su potencial de acción.[81]​  Algunos organismos, como los tiburones, tienen la capacidad de detectar y responder a los cambios de los campos eléctricos, una habilidad conocida como electrorrecepción.[82]​ Mientras que otros, denominados electrogénicos, son capaces de producir grandes descargas eléctricas con fines defensivos u ofensivos. Algunos peces, como las anguilas y las rayas eléctricas pueden generar tensiones de hasta dos mil voltios y corrientes superiores a 1 A.[83]​ El potencial de acción también es responsable de la coordinación de actividades en ciertas plantas.[84]​  Véase también Referencias Bibliografía Enlaces externos"
fisica,"En física, la interferencia es un fenómeno en el que dos o más ondas se superponen para formar una onda resultante de mayor, menor o igual amplitud. El efecto de interferencia puede ser observado en todos los tipos de onda, como ondas de luz, radio, sonido, entre otros.  La ecuación de la onda es la suma algebraica de las funciones de las ondas que se están superponiendo.  Superposición de ondas de la misma frecuencia En la superposición de ondas con la misma frecuencia el resultado depende de la cantidad y de la diferencia de fase     δ   {\displaystyle \delta }  . Si sumamos dos ondas      y  1   = A sin ⁡  ( ω t − k x )    {\displaystyle y_{1}=A\sin {(\omega t-kx)}}   y      y  2   = A sin ⁡  ( ω t − k x + δ )    {\displaystyle y_{2}=A\sin {(\omega t-kx+\delta )}}  , la onda resultante tendrá la misma frecuencia, y en el caso de que     δ   {\displaystyle \delta }   sea 0, 2    π   {\displaystyle \pi }  , etc., la amplitud será     2 A   {\displaystyle 2A}  . Este tipo de interferencias da lugar a patrones de interferencia, ya que dependiendo de la fase, la interferencia será destructiva (las ondas se encuentran desfasadas 180 grados o     π   {\displaystyle \pi }   radianes) o constructiva (desfase de 0 grados/radianes).  La superposición de ondas de frecuencias      f  1     {\displaystyle f_{1}}   y      f  2     {\displaystyle f_{2}}   muy cercanas entre sí produce un fenómeno particular denominado pulsación o batido.  En esos casos nuestro sistema auditivo no es capaz de percibir separadamente las dos frecuencias presentes, sino que se percibe una frecuencia única promedio         f  1   +  f  2    2     {\displaystyle {\frac {f_{1}+f_{2}}{2}}}  , pero que cambia en amplitud a una frecuencia de         |   f  1   −  f  2    |   2     {\displaystyle {\frac {|f_{1}-f_{2}|}{2}}}  .  Es decir, si superponemos dos ondas senoidales de 300 Hz y 304 Hz, nuestro sistema auditivo percibirá un único sonido cuya altura corresponde a una onda de 302 Hz y cuya amplitud varía con una frecuencia de 2 Hz (es decir, dos veces por segundo).  Pulsaciones o batidos Si se da el caso de que la frecuencia de ambas ondas no es igual     (  f  1   ,  f  2   )   {\displaystyle (f_{1},f_{2})}  , pero si son valores muy cercanos entre sí, la onda resultante es una onda modulada en amplitud por la llamada ""frecuencia de batido"" cuyo valor corresponde a:      f  batido   = Δ f =  |   f  1   −  f  2    |    {\displaystyle f_{\text{batido}}=\Delta f=|f_{1}-f_{2}|}  , la frecuencia de esta onda modulada corresponde a la media de las frecuencias que interfieren.  Mecanismos El principio de superposición de ondas establece que cuando dos o más ondas de propagación del mismo tipo inciden en el mismo punto, la amplitud resultante en ese punto es igual al suma vectorial de las amplitudes de las ondas individuales.[1] Si una cresta de una onda se encuentra con una cresta de otra onda de la misma frecuencia en el mismo punto, entonces la amplitud es la suma de las amplitudes individuales-esto es interferencia constructiva. Si la cresta de una onda se encuentra con la depresión de otra onda, la amplitud es igual a la diferencia de las amplitudes individuales, lo que se conoce como interferencia destructiva.  La interferencia constructiva se produce cuando la diferencia de fase entre las ondas es un múltiplo par de  π (180°), mientras que la interferencia destructiva se produce cuando la diferencia es un múltiplo impar de π (180°). Si la diferencia entre las fases es intermedia entre estos dos extremos, entonces la magnitud del desplazamiento de las ondas sumadas se encuentra entre los valores mínimo y máximo.  Consideremos, por ejemplo, lo que ocurre cuando se dejan caer dos piedras idénticas en un estanque de agua sin movimiento en lugares diferentes. Cada piedra genera una onda circular que se propaga hacia el exterior desde el punto en el que se dejó caer la piedra. Cuando las dos ondas se superponen, el desplazamiento neto en un punto determinado es la suma de los desplazamientos de las ondas individuales. En algunos puntos, éstas estarán en fase y producirán un desplazamiento máximo. En otros lugares, las ondas estarán en antifase y no habrá desplazamiento neto en esos puntos. Por lo tanto, algunas partes de la superficie estarán inmóviles, lo que se ve en la figura de arriba y a la derecha como líneas estacionarias de color azul y verde que irradian desde el centro.  La interferencia de la luz es un fenómeno común que puede explicarse clásicamente por la superposición de ondas, sin embargo, una comprensión más profunda de la interferencia de la luz requiere el conocimiento de la dualidad onda-partícula de la luz que se debe a la mecánica cuántica.  Los principales ejemplos de interferencia de la luz son el famoso experimento de la doble rendija, el moteado del láser, los revestimientos antirreflectantes y los interferómetros.    Tradicionalmente se enseña el modelo de onda clásico como base para entender la interferencia óptica, basado en el principio de Huygens-Fresnel.  Interferencia óptica Dado que la frecuencia de las ondas de luz (~1014 Hz) es demasiado alta para ser detectada por los detectores disponibles actualmente, sólo es posible observar la intensidad de un patrón de interferencia óptica. La intensidad de la luz en un punto determinado es proporcional al cuadrado de la amplitud media de la onda.  Esto puede expresarse matemáticamente como sigue. El desplazamiento de las dos ondas en un punto r es:       U  1   (  r  , t ) =  A  1   (  r  )  e  i [  φ  1   (  r  ) − ω t ]     {\displaystyle U_{1}(\mathbf {r} ,t)=A_{1}(\mathbf {r} )e^{i[\varphi _{1}(\mathbf {r} )-\omega t]}}        U  2   (  r  , t ) =  A  2   (  r  )  e  i [  φ  2   (  r  ) − ω t ]     {\displaystyle U_{2}(\mathbf {r} ,t)=A_{2}(\mathbf {r} )e^{i[\varphi _{2}(\mathbf {r} )-\omega t]}}    donde A representa la magnitud del desplazamiento, φ representa la fase y ω representa la frecuencia angular.  El desplazamiento de las ondas sumadas es      U (  r  , t ) =  A  1   (  r  )  e  i [  φ  1   (  r  ) − ω t ]   +  A  2   (  r  )  e  i [  φ  2   (  r  ) − ω t ]   .   {\displaystyle U(\mathbf {r} ,t)=A_{1}(\mathbf {r} )e^{i[\varphi _{1}(\mathbf {r} )-\omega t]}+A_{2}(\mathbf {r} )e^{i[\varphi _{2}(\mathbf {r} )-\omega t]}.}    La intensidad de la luz en r viene dada por      I (  r  ) = ∫ U (  r  , t )  U  ∗   (  r  , t )  d t ∝  A  1   2   (  r  ) +  A  2   2   (  r  ) + 2  A  1   (  r  )  A  2   (  r  ) cos ⁡ [  φ  1   (  r  ) −  φ  2   (  r  ) ] .   {\displaystyle I(\mathbf {r} )=\int U(\mathbf {r} ,t)U^{*}(\mathbf {r} ,t)\,dt\propto A_{1}^{2}(\mathbf {r} )+A_{2}^{2}(\mathbf {r} )+2A_{1}(\mathbf {r} )A_{2}(\mathbf {r} )\cos[\varphi _{1}(\mathbf {r} )-\varphi _{2}(\mathbf {r} )].}    Esto se puede expresar en términos de las intensidades de las ondas individuales como      I (  r  ) =  I  1   (  r  ) +  I  2   (  r  ) + 2    I  1   (  r  )  I  2   (  r  )   cos ⁡ [  φ  1   (  r  ) −  φ  2   (  r  ) ] .   {\displaystyle I(\mathbf {r} )=I_{1}(\mathbf {r} )+I_{2}(\mathbf {r} )+2{\sqrt {I_{1}(\mathbf {r} )I_{2}(\mathbf {r} )}}\cos[\varphi _{1}(\mathbf {r} )-\varphi _{2}(\mathbf {r} )].}    Así, el patrón de interferencia mapea la diferencia de fase entre las dos ondas, con máximos que se producen cuando la diferencia de fase es un múltiplo de 2{pi}}. Si los dos haces son de igual intensidad, los máximos son cuatro veces más brillantes que los haces individuales, y los mínimos tienen intensidad cero.  Las dos ondas deben tener la misma polarización para dar lugar a franjas de interferencia, ya que no es posible que las ondas de distinta polarización se anulen entre sí o se sumen. En cambio, cuando se suman ondas de diferente polarización, dan lugar a una onda de diferente estado de polarización.  Requisitos de la fuente de luz La discusión anterior supone que las ondas que interfieren entre sí son monocromáticas, es decir, tienen una única frecuencia, lo que requiere que sean infinitas en el tiempo.  Sin embargo, esto no es ni práctico ni necesario. Dos ondas idénticas de duración finita cuya frecuencia es fija en ese período darán lugar a un patrón de interferencia mientras se superponen.  Dos ondas idénticas que consisten en un estrecho espectro de ondas de frecuencia de duración finita (pero más corta que su tiempo de coherencia), darán lugar a una serie de patrones de franjas de espaciamientos ligeramente diferentes, y siempre que la dispersión de los espaciamientos sea significativamente menor que el espaciamiento medio de las franjas, se observará de nuevo un patrón de franjas durante el tiempo en que las dos ondas se superponen.  Las fuentes de luz convencionales emiten ondas de diferentes frecuencias y en diferentes momentos desde diferentes puntos de la fuente. Si la luz se divide en dos ondas y se vuelve a combinar, cada onda luminosa individual puede generar un patrón de interferencia con su otra mitad, pero los patrones de franjas individuales generados tendrán fases y espaciamientos diferentes, y normalmente no se observará ningún patrón de franjas global.  Sin embargo, las fuentes de luz de un solo elemento, como la sodio o la lámpara de vapor de mercurio tienen líneas de emisión con espectros de frecuencia bastante estrechos.  Cuando éstas se filtran espacialmente y por colores, y luego se dividen en dos ondas, pueden superponerse para generar franjas de interferencia.[2] Toda la interferometría anterior a la invención del láser se realizaba utilizando este tipo de fuentes y tenía una amplia gama de aplicaciones exitosas.  Un rayo láser generalmente se aproxima mucho más a una fuente monocromática, y por lo tanto es mucho más sencillo generar franjas de interferencia utilizando un láser. La facilidad con la que se pueden observar las franjas de interferencia con un rayo láser a veces puede causar problemas, ya que las reflexiones parásitas pueden dar lugar a franjas de interferencia espurias que pueden dar lugar a errores.  Normalmente, en la interferometría se utiliza un único rayo láser, aunque se han observado interferencias utilizando dos láseres independientes cuyas frecuencias estaban lo suficientemente ajustadas para satisfacer los requisitos de fase.[3] Esto también se ha observado para la interferencia de campo amplio entre dos fuentes láser incoherentes.[4]  También es posible observar franjas de interferencia utilizando luz blanca.  Se puede considerar que un patrón de franjas de luz blanca está formado por un ""espectro"" de patrones de franjas, cada uno de ellos con un espaciado ligeramente diferente. Si todos los patrones de franjas están en fase en el centro, entonces las franjas aumentarán de tamaño a medida que la longitud de onda disminuya y la intensidad sumada mostrará de tres a cuatro franjas de distinto color.  Young describe esto de forma muy elegante en su discusión sobre la interferencia de dos rendijas. Dado que las franjas de luz blanca se obtienen sólo cuando las dos ondas han recorrido distancias iguales desde la fuente de luz, pueden ser muy útiles en interferometría, ya que permiten identificar la franja de diferencia de trayectoria cero.[5]  Arreglos ópticos Para generar franjas de interferencia, la luz de la fuente tiene que dividirse en dos ondas que luego tienen que volver a combinarse. Tradicionalmente, los interferómetros se han clasificado como sistemas de división de amplitud o de división de frente de onda.  En un sistema de división de amplitud, se utiliza un divisor de haz para dividir la luz en dos haces que viajan en diferentes direcciones, que luego se superponen para producir el patrón de interferencia. El interferómetro de Michelson y el interferómetro de Mach-Zehnder son ejemplos de sistemas de división de amplitud.  En los sistemas de división del frente de onda, la onda se divide en el espacio; ejemplos de ello son el experimento de Young y el espejo de Lloyd.  "
fisica,"En física, el efecto Lamb, llamado así en honor de Willis Lamb, proviene de una pequeña diferencia observada en la energía asociada a dos niveles de energía     2  s  1  /  2     {\displaystyle 2s_{1/2}}   y     2  p  1  /  2     {\displaystyle 2p_{1/2}}   en el átomo de hidrógeno.  En mecánica cuántica, según las teorías de Dirac y de Schrödinger, los estados energéticos del hidrógeno que poseen los mismos números cuánticos     n   {\displaystyle n}   y     j   {\displaystyle j}  , pero que difieren en el número cuántico     l   {\displaystyle l}  , deben estar degenerados.  Introducción La teoría de Dirac aplicada al átomo de un electrón (hidrógeno) proporciona niveles con una energía que depende del número cuántico radial     n   {\displaystyle n}   y del momento angular total     j   {\displaystyle j}  . Como consecuencia de esto aparecen niveles degenerados en energía con valores diferentes del momento angular orbital,     l = 0   {\displaystyle l=0}  , y     l = 1   {\displaystyle l=1}  . Los niveles     2  s  1  /  2     {\displaystyle 2s_{1/2}}   y     2  p  1  /  2     {\displaystyle 2p_{1/2}}   son un ejemplo de esta situación.  Se podría pensar que la teoría de Dirac, incluidas todas las correcciones asociadas a las propiedades nucleares, debería explicar perfectamente el espectro del átomo de hidrógeno. Sin embargo, en medidas espectrales muy precisas se detectan desviaciones de las predicciones hechas por esta teoría.  Definición En 1951 Lamb descubre que, debido a que el estado     2  p  1  /  2     {\displaystyle 2p_{1/2}}   es ligeramente más bajo que el     2  s  1  /  2     {\displaystyle 2s_{1/2}}  , aparece un débil desplazamiento de la correspondiente línea orbital (desplazamiento Lamb). Más concretamente podemos decir que la energía del estado     2  s  1  /  2     {\displaystyle 2s_{1/2}}   es de 4,372×10−6 eV por encima del estado     2  p  1  /  2     {\displaystyle 2p_{1/2}}  , siendo     l = 0   {\displaystyle l=0}   en el primer caso, y     l = 1   {\displaystyle l=1}   en el caso del estado     2  p  1  /  2     {\displaystyle 2p_{1/2}}  .  Historia Las primeras evidencias en este sentido son detectadas por W. V. Houston en 1937 y R. C. Willians en 1938 quienes comprueban experimentalmente que los niveles     2  s  1  /  2     {\displaystyle 2s_{1/2}}   y     2  p  1  /  2     {\displaystyle 2p_{1/2}}   no son degenerados.  Éstos concluyen que el estado     2  s  1  /  2     {\displaystyle 2s_{1/2}}  , está ligeramente por encima del     2  p  3  /  2     {\displaystyle 2p_{3/2}}  . Sin embargo los intentos de ratificación, realizados en las mismas fechas, no detectan esta desviación debido principalmente a las dificultades de medir diferencias tan pequeñas en el número de onda por métodos espectroscópicos directos, ya que son enmascaradas por efectos difíciles de controlar, como es el caso del Doppler que sufre la radiación emitida por el átomo debido a su movimiento de traslación.  Trabajo experimental La cuestión es definitivamente resuelta experimentalmente en 1947 por W. E. Lamb y R. C. Retherford quienes idean un experimento que minimiza el ensanchamiento Doppler de las líneas. Los puntos claves del experimento son:  1. En lugar de resolver espectroscópicamente la estructura fina, utilizan técnicas de microondas para estimular directamente la transición entre los niveles     2  s  1  /  2     {\displaystyle 2s_{1/2}}   y     2  p  1  /  2     {\displaystyle 2p_{1/2}}   (que es dipolar eléctrica).  2. El éxito del experimento de Lamb y Retherford radica en que el nivel     2  s  1  /  2     {\displaystyle 2s_{1/2}}   es metaestable, ya que el único estado energético más bajo es el     1  s  1  /  2     {\displaystyle 1s_{1/2}}  , no estando permitida una transición dipolar eléctrica entre ellos.  3. El mecanismo más probable de desexcitación es mediante la emisión de dos fotones, con una vida media de 1/7s. Así pues en ausencia de perturbaciones externas la vida media del     2  s  1  /  2     {\displaystyle 2s_{1/2}}   es mucho mayor que la del     2  p  1  /  2     {\displaystyle 2p_{1/2}}   que es de 1,6×10−9s.  Willis Lamb midió el desplazamiento en la región de las microondas. Ubicó átomos en el estado     2  s  1  /  2     {\displaystyle 2s_{1/2}}  . Estos átomos no se podían desexcitar adoptando directamente el estado     1  s  1  /  2     {\displaystyle 1s_{1/2}}   a causa de que las reglas de selección prohíben mantener el mismo valor del momento angular en una transición.   Introduciendo los átomos en un campo magnético, para separar los niveles por efecto Zeeman, expuso los àtomos a una radiación de microondas a 2395 MHz (no muy lejos de la frecuencia de un horno corriente, que es de 2560 MHz).   Entonces varió el campo magnético hasta que una frecuencia que produjo transiciones desde el nivel     2  p  1  /  2     {\displaystyle 2p_{1/2}}   hasta el nivel     2  p  3  /  2     {\displaystyle 2p_{3/2}}  . Entonces pudo medir la transición permitida desde el nivel     2  p  3  /  2     {\displaystyle 2p_{3/2}}   hasta el nivel     1  s  1  /  2     {\displaystyle 1s_{1/2}}  .   Estos resultados fueron usados para determinar que el campo magnético cero, divisorio de estos dos niveles, corresponde a 1057 MHz. Utilizando la relación de Planck se demuestra que la energía de separación es de 4,372×10−6 eV.  Evitando algunos detalles técnicos, podríamos decir que el procedimiento para realizar el experimento es el siguiente:  Se usa un haz de hidrógeno molecular a alta temperatura, para obtener los átomos de H cuyo espectro se quiere analizar, (a una temperatura de 2500 K la disociación es del 60%).   Los átomos de hidrógeno se seleccionan haciéndolos pasar por una rendija, al mismo tiempo que se bombardean con electrones de energía cinética mayor que 10,2 eV, para conseguir que el sistema pase al estado     2  s  1  /  2     {\displaystyle 2s_{1/2}}  .   Por ese procedimiento se obtiene una pequeña fracción (1 en 108) de átomos en los estados     2  s  1  /  2     {\displaystyle 2s_{1/2}}  ,     2  p  1  /  2     {\displaystyle 2p_{1/2}}   y     2  p  3  /  2     {\displaystyle 2p_{3/2}}   a una velocidad media de 8×105 cm/s.   Dada la alta vida media del estado     2  s  1  /  2     {\displaystyle 2s_{1/2}}   respecto de los otros dos estados     p   {\displaystyle p}  , los átomos en dicho estado recorren una distancia del orden de los 10 cm mientras que los otros sólo recorren 1,3×10−3 cm antes de desexcitarse.   El detector es una lámina de wolframio en la que el átomo en el estado     2  s  1  /  2     {\displaystyle 2s_{1/2}}   puede depositar su electrón absorbiendo su energía de ionización.  Si el haz de átomos en el estado     2  s  1  /  2     {\displaystyle 2s_{1/2}}   se pasa a través de una región de interacción con un campo de radiofrecuencias que provoque la transición desde el estado     2  s  1  /  2     {\displaystyle 2s_{1/2}}   a los estados     2  p  1  /  2     {\displaystyle 2p_{1/2}}   o     2  p  3  /  2     {\displaystyle 2p_{3/2}}  , se origina una rápida caída de la población de átomos en el estado     2  s  1  /  2     {\displaystyle 2s_{1/2}}   al abrir de forma forzada un canal de transición.   Esto provoca una rápida reducción de los átomos en el estado     2  s  1  /  2     {\displaystyle 2s_{1/2}}   que llegan al detector, naturalmente esto ocurre sólo cuando la radiofrecuencia coincide con la que corresponde a la energía de la transición     2  s  1  /  2     {\displaystyle 2s_{1/2}}  →     2  p  1  /  2     {\displaystyle 2p_{1/2}}   o     2  s  1  /  2     {\displaystyle 2s_{1/2}}   →     2  p  3  /  2     {\displaystyle 2p_{3/2}}  .   Por tanto, la diferencia de energía entre los niveles es igual a la frecuencia de la radiación que hace que se detecte una disminución en la población de los estados     2  s  1  /  2     {\displaystyle 2s_{1/2}}   que llega al detector.   Con esta base experimental y algunos detalles más como la aplicación de un campo magnético variable para estabilizar el campo de microondas, Lamb y Retherford obtienen que el nivel     2  s  1  /  2     {\displaystyle 2s_{1/2}}   está 1000MHz por encima del     2  p  1  /  2     {\displaystyle 2p_{1/2}}  .   Experimentos posteriores más precisos han establecido esta diferencia en 1057,90 ± 0,06 MHz (Robiscoe y Shyn 1970), 1057,893 ± 0,020 MHz (Lundeen y Pipkin 1975), 1057,862 ± 0,020 MHz (Andrews y Newton 1976).  Explicación teórica La explicación teórica de estos resultados no fue en principio evidente y llevó a la revisión de conceptos fundamentales como la renormalización de la masa y de la carga, y a la formulación de teorías como la electrodinámica cuántica (Bethe, Tomonaga, Schwinger, Feynman y Dyson) que superaba la mecánica cuántica relativista de Dirac.  Es en el contexto de la electrodinámica cuántica, que es la teoría cuántica de campos de la interacción electromagnética entre partículas cargadas, donde el desdoblamiento Lamb aparece en el cálculo de las denominadas correcciones radiativas.  Los cálculos en electrodinámica cuántica son perturbativos, y las correcciones radiativas son los efectos de segundo orden. En particular, estos efectos son los denominados autoenergía del fotón o polarización del vacío, autoenergía del electrón y correcciones de vértice.   Estas perturbaciones de segundo orden originan un renormalización de la masa y de la carga del electrón, que hacen que los valores que se miden experimentalmente sean distintos de los que se obtendrían de no existir la interacción electromagnética, o de no acoplarse el campo eléctrico de los electrones con el de los fotones. En el caso del efecto Lamb, la contribución principal proviene de la autoenergía del electrón, que proporciona un desdoblamiento del orden de 1000 MHz.   Los otros diagramas, dan una contribución menor, del orden de los 30 MHz. Los cálculos de este efecto en la electrodinámica cuántica son especialmente difíciles, pues el electrón está en un estado ligado y las teorías cuánticas de campos están formuladas fundamentalmente para estados de colisión.  En cualquier caso, y debido a la importancia de este efecto, la situación actual es que los cálculos teóricos más precisos son 1057,916 ± 0,010 MHz (Erickson 1971), 1057,864 ± 0,014 MHz (Mohr 1976), los que pueden compararse con los resultados experimentales mencionados antes.  Lecturas complementarias Una explicación más detallada de este efecto, aunque no muy exhaustiva desde un punto de vista teórico, la podemos encontrar en Introduction to Elementary Particles de D. E. Griffiths.  Cálculos basados en la electrodinámica cuántica los podemos encontrar por ejemplo en Quantum Field Theory de Mandl y Shaw. Con nivel más básico, aunque en forma más rigurosa, en Quantum Field Theory de Itzykson y Zuber.  Energía del punto cero Una interpretación cualitativa de este efecto la propuso Welton en 1948. Un campo de radiación cuantizado en su estado de más baja energía no implica un campo cero, sino que existen fluctuaciones cuánticas de campo cero similares a las del estado fundamental del oscilador armónico.   Esto supone que aún en el vacío existen fluctuaciones de campo que provocan movimientos rápidamente oscilatorios del electrón, de manera que el electrón no es percibido como puntual por la carga del núcleo, sino como una distribución de carga con un cierto radio.   Como consecuencia de esto, el electrón no se ve tan fuertemente atraído por el núcleo a cortas distancias, por lo que los electrones en orbitales inferiores son los que más se ven afectados por este aspecto dinámico, perdiendo algo de energía de ligadura.  Formulación matemática Esta peculiar diferencia es el efecto de un loop del cuanto electromagnético, y puede ser interpretada por la influencia de un fotón virtual que es emitido y reabsorbido por el propio átomo. En electrodinámica cuántica (EDC) el campo electromagnético está cuantificado y, como en el caso del oscilador armónico de la mecánica cuántica, su estado de menor energía no es cero. Debido a esto existen unas pequeñas oscilaciones del punto cero que causan que el electrón ejecute rápidos movimientos de oscilación. El electrón resulta, pues, ""difuminado"" y el radio cambia "
fisica,"La permitividad eléctrica (llamada también constante dieléctrica para dieléctricos homogéneos) es un parámetro físico de los materiales que describe cuánto son afectados por un campo eléctrico. La información del campo eléctrico está contenida en los vectores E y D, donde E es la intensidad y D es el desplazamiento eléctrico o densidad de flujo eléctrico. Es este último el que cuenta la influencia del campo sobre un material (análogo al vector B del campo magnético el cual es llamado ""densidad de flujo magnético"" o ""inducción magnética"" sobre un material).  La permitividad eléctrica del vacío es constante y está dada por      ε  0     {\displaystyle \varepsilon _{0}}   = 8.8541878176x10-12 C2 / (N·m²).   La permitividad está determinada por la tendencia de un material a polarizarse ante la aplicación de un campo eléctrico y de esa forma anular parcialmente el campo interno del material. Está directamente relacionada con la susceptibilidad eléctrica. Por ejemplo, en un condensador una alta permitividad hace que la misma cantidad de carga eléctrica se almacene con un campo eléctrico menor y, por ende, a un potencial menor, llevando a una mayor capacidad del mismo.  Explicación En electromagnetismo se define el campo de desplazamiento eléctrico D o flujo eléctrico      Φ  E     {\displaystyle \Phi _{E}}  , como el campo eléctrico E multiplicado por la permitividad eléctrica del medio. De este modo el D solo es inducido por las cargas libres y no por las cargas dipolares. La relación de ambos campos (para medios lineales) con la permitividad es:       D  = ε ⋅  E    {\displaystyle \mathbf {D} =\varepsilon \cdot \mathbf {E} }    donde ε es un escalar si el medio es isótropo o un tensor de segundo orden en otros casos. Y su relación con el vector polarización P de la que deriva esta última viene dada por la siguiente ecuación:[1]​       D  =  ε  0   ⋅  E  +  P    {\displaystyle \mathbf {D} =\varepsilon _{0}\cdot \mathbf {E} +\mathbf {P} }    A partir de la cual y empleando los resultados del estudio del campo eléctrico y equilibrio en un dieléctrico, conseguimos de la ecuación que en un dieléctrico relaciona al campo a escala macroscópica con el campo medio creado por todos los dipolos y el campo local (el percibido por cada dipolo):       E  =<  E  > +   E  L O C      {\displaystyle \mathbf {E} =<\mathbf {E} >+\mathbf {E_{LOC}} }    En esta expresión gracias a haber calculado el campo medio creado por todos los dipolos podemos relacionar <E> (campo medio) con la polarización eléctrica (P):      <  E  >= −    P   3  ε  0        {\displaystyle <\mathbf {E} >=-{\frac {\mathbf {P} }{3\varepsilon _{0}}}}    de este modo tenemos:       E  =<  E  > +   E  L O C    = −    P   3  ε  0      +   E  L O C      {\displaystyle \mathbf {E} =<\mathbf {E} >+\mathbf {E_{LOC}} =-{\frac {\mathbf {P} }{3\varepsilon _{0}}}+\mathbf {E_{LOC}} }    Como sabemos que la polarización se puede escribir en función del momento dipolar y este relacionarse con el campo eléctrico macroscópico (E) tenemos:       P  = N ⋅  p  = N ⋅ α ⋅   E  L O C    = N ⋅ α ⋅ (  E  +    P   3  ε  0      )   {\displaystyle \mathbf {P} =N\cdot \mathbf {p} =N\cdot \alpha \cdot \mathbf {E_{LOC}} =N\cdot \alpha \cdot (\mathbf {E} +{\frac {\mathbf {P} }{3\varepsilon _{0}}})}    Donde α es la polarizabilidad, N el número de dipolos y p el momento dipolar. De esta expresión podemos despejar P de la siguiente manera:       P  =    N ⋅ α   1 −    N ⋅ α   3 ⋅  ε  0         ⋅  E  =  χ  e   ⋅  ε  0   ⋅  E    {\displaystyle \mathbf {P} ={\frac {\mathbf {N\cdot \alpha } }{1-{\frac {N\cdot \alpha }{3\cdot \varepsilon _{0}}}}}\cdot \mathbf {E} =\chi _{e}\cdot \varepsilon _{0}\cdot \mathbf {E} }    siendo la      χ  e     {\displaystyle \chi _{e}}   susceptibilidad eléctrica de este modo podemos llegar a la relación inicial entre D y E sabiendo que la permitividad relativa      ε  r   = 1 +  χ  e     {\displaystyle \varepsilon _{r}=1+\chi _{e}}   y que la permitividad     ε =  ε  r   ⋅  ε  0     {\displaystyle \varepsilon =\varepsilon _{r}\cdot \varepsilon _{0}}   por lo tanto podemos llegar a nuestra relación a partir de la expresión que relaciona campo de desplazamiento eléctrico D, como el campo eléctrico E y polarización P que ya habíamos visto:       D  =  ε  0   ⋅  E  +  P  =  ε  0   ⋅ ( 1 +  χ  e   ) ⋅  E  =  ε  0   ⋅  ε  r   ⋅  E  = ε ⋅  E    {\displaystyle \mathbf {D} =\varepsilon _{0}\cdot \mathbf {E} +\mathbf {P} =\varepsilon _{0}\cdot (1+\chi _{e})\cdot \mathbf {E} =\varepsilon _{0}\cdot \varepsilon _{r}\cdot \mathbf {E} =\varepsilon \cdot \mathbf {E} }    La permitividad, tomada en función de la frecuencia, puede tomar valores reales o complejos. Generalmente no es una constante, ya que puede variar con la posición en el medio, la frecuencia del campo aplicado, la humedad o la temperatura, entre otros parámetros. En un medio no lineal, la permitividad puede depender de la magnitud del campo eléctrico.  La unidad de medida en el SI es el faradio por metro (F/m). D se mide en culombios por metro cuadrado (C/m²), mientras que E se mide en voltios por metro (V/m).  D y E representan el mismo fenómeno, la interacción entre objetos cargados. D está relacionado con las densidades de carga asociada a esta interacción. E se relaciona con las fuerzas y diferencias de potencial involucradas. La permitividad del vacío      ε  0     {\displaystyle \varepsilon _{0}}  , es el factor de escala que relaciona los valores de D y E en ese medio.      ε  0     {\displaystyle \varepsilon _{0}}   es igual a 8.8541878176...×10-12 F/m.  Permitividad del vacío La permitividad del vacío      ε  0     {\displaystyle \varepsilon _{0}}   es el cociente de los campos D/E en ese medio. También aparece en la ley de Coulomb como parte de la constante de fuerza de Coulomb,        1  4 π  ε  0         {\displaystyle {\tfrac {1}{4\pi \varepsilon _{0}}}}  , que expresa la atracción entre dos cargas unitarias en el vacío.  donde     c   {\displaystyle c}   es la velocidad de la luz y      μ  0     {\displaystyle \mu _{0}}   es la permeabilidad magnética del vacío. Estas tres constantes están totalmente definidas en unidades del SI.  Permitividades absoluta y relativa La permitividad de un material se da normalmente en relación con la del vacío, denominándose permitividad relativa,      ε  r     {\displaystyle \varepsilon _{r}}   (también llamada constante dieléctrica en algunos casos). La permitividad absoluta se calcula multiplicando la permitividad relativa por la del vacío:  donde       χ  e     {\displaystyle \,\chi _{e}}   es la susceptibilidad eléctrica del material. En la siguiente tabla se muestran las permitividades relativas de algunos dieléctricos:  La permitividad en los medios En el caso común de un medio isótropo, D y E son vectores paralelos y     ε   {\displaystyle \varepsilon }   es un escalar, pero en medios anisótropos, este no es el caso y     ε   {\displaystyle \varepsilon }   es un tensor de rango 2 (lo que causa birrefringencia). La permitividad eléctrica     ε   {\displaystyle \varepsilon }   y la permeabilidad magnética     μ   {\displaystyle \mu }   de un medio determinan la velocidad de fase v de radiación electromagnética dentro del mismo:  Cuando un campo eléctrico es aplicado a un medio, una corriente fluye. La corriente total que discurre por un material real está, en general, compuesta de dos partes: una corriente de conducción y una de desplazamiento. La corriente de desplazamiento puede pensarse como la respuesta elástica de un material al campo eléctrico aplicado. Al aumentar la magnitud del campo eléctrico, la corriente de desplazamiento es almacenada en el material, y cuando la intensidad del campo disminuye, el material libera la corriente. El desplazamiento eléctrico se puede separar entre una contribución del vacío y una del material:  donde P es la polarización del medio y     χ   {\displaystyle \chi }   es la susceptibilidad eléctrica. Se deduce que la permitividad relativa y la susceptibilidad de un material están relacionadas,   Absorción En electricidad, se llama absorción a la propiedad de un dieléctrico utilizado en un condensador por el cual fluye una pequeña corriente de carga después de que las placas han alcanzado el potencial final, y por el que también fluye una pequeña corriente de descarga después de haber sido cortocircuitadas las placas, al haber interrumpido el cortocircuito durante unos minutos, y nuevamente haber sido cortocircuitadas. Denominada también saturación dieléctrica."
fisica,"Un campo electromagnético es un campo físico, de tipo tensorial, producido por aquellos elementos cargados eléctricamente, que afecta a partículas con carga eléctrica.[1]  Convencionalmente, dado un sistema de referencia, el campo electromagnético se divide en una ""parte eléctrica"" y en una ""parte magnética"". Sin embargo, esta distinción no puede ser universal sino dependiente del observador. Así un observador en movimiento relativo respecto al sistema de referencia medirá efectos eléctricos y magnéticos diferentes, que un observador en reposo respecto a dicho sistema. Esto ilustra la relatividad de lo que se denomina ""parte eléctrica"" y ""parte magnética"" del campo electromagnético. Como consecuencia de lo anterior tenemos que ni el ""vector"" campo eléctrico ni el ""vector"" de inducción magnética se comportan genuinamente como magnitudes físicas de tipo vectorial, sino que juntos constituyen un tensor para el que sí existen leyes de transformación físicamente esperables.  El campo puede verse como la combinación de un campo eléctrico y un campo magnético. El campo eléctrico es producido por cargas estacionarias y el campo magnético por cargas en movimiento (corrientes); estos dos se describen a menudo como las fuentes del campo. La forma en que las cargas y las corrientes interactúan con el campo electromagnético se describe mediante las ecuaciones de Maxwell y la ley de fuerza de Lorentz.[2]  Desde la perspectiva de la clásica en la historia del electromagnetismo, el campo electromagnético puede considerarse como un campo suave y continuo, que se propaga de forma ondulatoria. En cambio, desde la perspectiva de la teoría cuántica de campos, este campo se ve como cuantizado; lo que significa que el campo cuántico libre (es decir no interactuante) puede expresarse como la suma de Fourier de operadores de creación y aniquilación en el espacio energía-momento, mientras que los efectos del campo cuántico interactuante pueden analizarse en teoría de perturbaciones a través de la matriz S con ayuda de toda una serie de técnicas matemáticas como la serie de Dyson, Teorema de Wick, funciones de correlación, operador de evolución temporal,  diagramas de Feynman, etc. El campo cuantizado sigue siendo espacialmente continuo, pero sus estados energéticos son discretos y múltiplos enteros de     h f   {\displaystyle hf}   - cuantos de energía llamados fotones, creados por los operadores de creación del campo cuántico. En general, la frecuencia     f   {\displaystyle f}   del campo cuantizado puede ser cualquier valor por encima de cero, y por tanto el valor del cuanto de energía (fotón) puede ser cualquier valor por encima de cero, o incluso variar continuamente en el tiempo.  .  Estructura El campo electromagnético puede considerarse de dos formas distintas: una estructura continua o una estructura discreta.  Estructura continua Clásicamente, se considera que los campos eléctricos y magnéticos son producidos por movimientos suaves de objetos cargados. Por ejemplo, las cargas oscilantes producen variaciones en los campos eléctricos y magnéticos que pueden verse de forma ""suave"", continua y ondulatoria. En este caso, se considera que la energía se transfiere continuamente a través del campo electromagnético entre dos lugares cualesquiera. Por ejemplo, los átomos metálicos de un radiotransmisor parecen transferir energía continuamente. Esta visión es útil hasta cierto punto (radiación de baja frecuencia), sin embargo, se encuentran problemas a altas frecuencias (véase catástrofe ultravioleta).[3]  Estructura discreta Se puede pensar en el campo electromagnético de una forma más ""gruesa"". Los experimentos revelan que en algunas circunstancias la transferencia de energía electromagnética se describe mejor como transportada en forma de paquetes llamados quanta con una frecuencia fija. La relación de Planck relaciona la  energía fotónica E de un fotón con su frecuencia f mediante la ecuación:[4]     E =  h f   {\displaystyle E=\,hf}    donde h es la constante de Planck, y f es la frecuencia del fotón. Aunque la óptica cuántica moderna nos dice que también existe una explicación semiclásica del efecto fotoeléctrico-la emisión de electrones desde superficies metálicas sometidas a radiación electromagnética-, el fotón se ha utilizado históricamente (aunque no necesariamente) para explicar ciertas observaciones. Se ha comprobado que el aumento de la intensidad de la radiación incidente (siempre que se permanezca en el régimen lineal) sólo aumenta el número de electrones expulsados, y no tiene casi ningún efecto sobre la distribución energética de su expulsión. Sólo la frecuencia de la radiación es relevante para la energía de los electrones expulsados.  Esta imagen cuántica del campo electromagnético (que lo trata como análogo a  osciladores armónicos) ha tenido mucho éxito, dando lugar a la electrodinámica cuántica, una teoría cuántica de campos que describe la interacción de la radiación electromagnética con la materia cargada. También da lugar a la óptica cuántica, que se diferencia de la electrodinámica cuántica en que la propia materia se modela utilizando la mecánica cuántica en lugar de la teoría cuántica de campos.  Dinámica En el pasado, se pensaba que los objetos cargados eléctricamente producían dos tipos de campo diferentes, no relacionados, asociados a su propiedad de carga. Un campo eléctrico se produce cuando la carga está estacionaria con respecto a un observador que mide las propiedades de la carga, y un campo magnético además de un campo eléctrico se produce cuando la carga se mueve, creando una corriente eléctrica con respecto a este observador. Con el tiempo, se llegó a la conclusión de que los campos eléctrico y magnético son dos partes de un todo mayor: el campo electromagnético. Hasta 1820, cuando el físico danés H. C. Ørsted demostró el efecto de la corriente eléctrica sobre la aguja de una brújula, la electricidad y el magnetismo se consideraban fenómenos no relacionados. [5] En 1831, Michael Faraday hizo la observación seminal de que los campos magnéticos variables en el tiempo podían inducir corrientes eléctricas y luego, en 1864, James Clerk Maxwell publicó su famoso artículo ""Una teoría dinámica del campo electromagnético"".[6]  Una vez que este campo electromagnético se ha producido a partir de una distribución de carga dada, otros objetos cargados o magnetizados en este campo pueden experimentar una fuerza. Si estas otras cargas y corrientes son comparables en tamaño a las fuentes que producen el campo electromagnético anterior, entonces se producirá un nuevo campo electromagnético neto. Así pues, el campo electromagnético puede considerarse como una entidad dinámica que provoca el movimiento de otras cargas y corrientes, y que también se ve afectada por ellas. Estas interacciones se describen mediante las ecuaciones de Maxwell y la fuerza de Lorentz.  Bucle de realimentación El comportamiento del campo electromagnético puede dividirse en cuatro partes diferentes de un bucle:[7]  Un malentendido común es que (a) los cuantos de los campos actúan de la misma manera que (b) las partículas cargadas, como los electrones, que generan los campos. En nuestro mundo cotidiano, los electrones viajan lentamente a través de conductores con una velocidad de deriva de una fracción de centímetro por segundo y a través de un  tubo de vacío a velocidades de alrededor de 1000 km/s,[8] pero los campos se propagan a la velocidad de la luz, aproximadamente a 300 000 kilómetros (o 186 000 millas) por segundo. La relación de velocidades entre las partículas cargadas en un conductor y los cuantos de campo es del orden de uno a un millón. Las ecuaciones de Maxwell relacionan (a) la presencia y el movimiento de partículas cargadas con (b) la generación de campos. Esos campos pueden entonces afectar a la fuerza sobre, y pueden entonces mover otras partículas cargadas que se mueven lentamente. Las partículas cargadas pueden moverse a velocidades relativistas cercanas a las velocidades de propagación del campo, pero, como demostró Albert Einstein[cita requerida], esto requiere enormes energías de campo, que no están presentes en nuestras experiencias cotidianas con la electricidad, el magnetismo, la materia y el tiempo y el espacio.  El bucle de retroalimentación se puede resumir en una lista, incluyendo los fenómenos que pertenecen a cada parte del bucle:[cita requerida]  Campo electromagnético clásico Una partícula de carga     q   {\displaystyle q}   moviéndose en presencia de un campo electromagnético a una velocidad        v →      {\displaystyle {\vec {v}}}   experimenta una fuerza de Lorentz dada por la siguiente ecuación:         F →    = q (    E →    +    v →    ×    B →    )   {\displaystyle {\vec {F}}=q({\vec {E}}+{\vec {v}}\times {\vec {B}})}    dónde        E →      {\displaystyle {\vec {E}}}   es el campo eléctrico y        B →      {\displaystyle {\vec {B}}}   es el campo magnético y el símbolo     ∧   {\displaystyle \land }  representa producto cruz. El campo electromagnético es el conjunto (       E →      {\displaystyle {\vec {E}}}  ,       B →      {\displaystyle {\vec {B}}}  ) de dos campos vectoriales, los cuales se pueden medir independientemente. Ambas identidades son indisociables. El comportamiento de este campo es descrito por las ecuaciones de Maxwell de manera clásica. Para el caso más general, se hace referencia a la electrodinámica cuántica.  Campo electromagnético en teoría de la relatividad En electrodinámica clásica y sobre todo en teoría de la relatividad el campo electromagnético se representa por un tensor 2-covariante y anti-simétrico, cuyas componentes son aquellas que en cada sistema de referencia se reflejan como parte eléctrica y parte magnética del campo:       F  =   (     F  00      F  01      F  02      F  03        F  10      F  11      F  12      F  13        F  20      F  21      F  22      F  23        F  30      F  31      F  32      F  33      )   =   (    0    E  x    /  c    E  y    /  c    E  z    /  c     −  E  x    /  c   0   −  B  z      B  y       −  E  y    /  c    B  z     0   −  B  x       −  E  z    /  c   −  B  y      B  x     0    )     {\displaystyle \mathbf {F} ={\begin{pmatrix}F_{00}&F_{01}&F_{02}&F_{03}\\F_{10}&F_{11}&F_{12}&F_{13}\\F_{20}&F_{21}&F_{22}&F_{23}\\F_{30}&F_{31}&F_{32}&F_{33}\end{pmatrix}}={\begin{pmatrix}0&E_{x}/c&E_{y}/c&E_{z}/c\\-E_{x}/c&0&-B_{z}&B_{y}\\-E_{y}/c&B_{z}&0&-B_{x}\\-E_{z}/c&-B_{y}&B_{x}&0\end{pmatrix}}}    Fuerza de Lorentz La fuerza de Lorentz puede escribirse de forma mucho más sencilla gracias al tensor de campo electromagnético que en su escritura vectorial clásica:       f  = e (  E  +  v  ×  B  )   {\displaystyle \mathbf {f} =e(\mathbf {E} +\mathbf {v} \times \mathbf {B} )}   (expresión vectorial)       f  α   =  ∑  β   e    F  α β      u  β      {\displaystyle f_{\alpha }=\sum _{\beta }e\ F_{\alpha \beta }\ u^{\beta }\,}   (expresión tensorial relativista)  Ecuaciones de Maxwell Las ecuaciones de Maxwell también toman formas muy sencillas en términos del tensor de campo electromagnético:       F  , γ   α β   +  F  , α   β γ   +  F  , β   γ α   =    ∂  F  α β     ∂  x  γ      +    ∂  F  β γ     ∂  x  α      +    ∂  F  γ α     ∂  x  β      = 0   {\displaystyle F_{,\gamma }^{\alpha \beta }+F_{,\alpha }^{\beta \gamma }+F_{,\beta }^{\gamma \alpha }={\frac {\partial F^{\alpha \beta }}{\partial x^{\gamma }}}+{\frac {\partial F^{\beta \gamma }}{\partial x^{\alpha }}}+{\frac {\partial F^{\gamma \alpha }}{\partial x^{\beta }}}=0}         F  , β   α β   =    ∂  F  α β     ∂  x  β      =  μ  0    J  α     {\displaystyle F_{,\beta }^{\alpha \beta }={\frac {\partial F^{\alpha \beta }}{\partial x^{\beta }}}=\mu _{0}J^{\alpha }}    Donde en la última expresión se ha usado el convenio de sumación de Einstein y donde la magnitud Jα es el cuadrivector de corriente que viene dado por:       J  α   =   (    c ρ    J  x      J  y      J  z      )     {\displaystyle J^{\alpha }={\begin{pmatrix}c\rho &J_{x}&J_{y}&J_{z}\end{pmatrix}}}    Potencial vector La forma de las ecuaciones de Maxwell permite que sobre un dominio simplemente conexo (estrellado) el campo electromagnético puede expresarse como la derivada exterior de un potencial vector, lo cual facilita enormemente la resolución de dichas ecuaciones. Usando el convenio de sumación de Einstein tenemos"
fisica,"El efecto Kerr es una birrefringencia creada en un material por un campo eléctrico exterior. Fue descubierto en 1875 por el físico escocés John Kerr, y se caracteriza por la existencia de dos índices de refracción diferentes: un haz luminoso se divide en dos haces cuando penetra en este material.[1][2][3] Los fenómenos físicos responsables de este efecto en la materia pueden ser, dependiendo del material, la electroestricción, efecto fotorrefractivo y la orientación molecular, entre otros.  La birrefringencia creada, a diferencia de la del efecto Pockels, varía según el cuadrado del campo eléctrico aplicado. Los materiales presentan en general un efecto Kerr muy débil, sin embargo, algunos líquidos presentan un efecto Kerr medible.  Descripción Un campo eléctrico aplicado a un material genera una birrefringencia en este material: la luz tiene un índice de refracción diferente según su polarización sea ortogonal o paralela al campo. La diferencia entre estos dos índices vale:      Δ n = λ K  E  2       {\displaystyle \Delta n=\lambda KE^{2}\ }    donde:  Efecto electroóptico de Kerr El efecto electroóptico de Kerr, o efecto DC Kerr, es el caso especial en el que se aplica un campo eléctrico externo que varía lentamente, por ejemplo, mediante un voltaje en los electrodos a través del material de muestra. Bajo esta influencia, la muestra se vuelve birrefringente, con diferentes índices de refracción para la luz polarizada paralela o perpendicular al campo aplicado. La diferencia en el índice de refracción, Δn, está dada por:       Δ n = λ K  E  2   ,     {\displaystyle \Delta n=\lambda KE^{2},\ }    donde λ es la longitud de onda de la luz, K es la constante de Kerr y E es la fuerza del campo eléctrico. Esta diferencia en el índice de refracción hace que el material actúe como una placa de ondas cuando la luz incide sobre él en una dirección perpendicular al campo eléctrico. Si el material se coloca entre dos polarizadores lineales ""cruzados"" (perpendiculares), no se transmitirá luz cuando se apague el campo eléctrico, mientras que casi toda la luz se transmitirá para un valor óptimo del campo eléctrico. Los valores más altos de la constante de Kerr permiten lograr una transmisión completa con un campo eléctrico aplicado más pequeño.  Algunos líquidos polares, como el nitrotolueno (C7 H7 NO2 ) y el nitrobenceno (C6 H5 NO2 ) exhiben constantes de Kerr muy grandes. Una celda de vidrio llena de uno de estos líquidos se llama celda de Kerr. Estos se utilizan con frecuencia para modular la luz, ya que el efecto Kerr responde muy rápidamente a los cambios en el campo eléctrico. La luz se puede modular con estos dispositivos a frecuencias de hasta 10  GHz. Debido a que el efecto Kerr es relativamente débil, una celda Kerr típica puede requerir voltajes tan altos como 30  kV para lograr una transparencia total. Esto contrasta con las celdas de Pockels, que pueden operar a voltajes mucho más bajos. Otra desventaja de las células de Kerr es que el mejor material disponible, el nitrobenceno, es venenoso. También se han utilizado algunos cristales transparentes para la modulación de Kerr, aunque tienen constantes de Kerr más pequeñas.  En medios que carecen de simetría de inversión, el efecto Kerr generalmente está enmascarado por el efecto Pockels mucho más fuerte. Sin embargo, el efecto Kerr todavía está presente y, en muchos casos, se puede detectar independientemente de las contribuciones del efecto Pockels.[4]  Efecto Kerr óptico El efecto óptico Kerr, o efecto AC Kerr, es el caso en el que el campo eléctrico se debe a la propia luz. Esto provoca una variación en el índice de refracción que es proporcional a la irradiancia local de la luz.[5] Esta variación del índice de refracción es responsable de los efectos ópticos no lineales del autoenfoque, la modulación de la auto-fase y la inestabilidad modulacional, y es la base del modelo de lentes de Kerr. Este efecto solo se vuelve significativo con rayos muy intensos como los de los láseres. También se ha observado que el efecto Kerr óptico altera dinámicamente las propiedades de acoplamiento de modo en fibra multimodo, una técnica que tiene aplicaciones potenciales para mecanismos de conmutación totalmente ópticos, sistemas nanofotónicos y dispositivos fotosensores de baja dimensión.[6][7]  Teoría Efecto Kerr en corriente continua Para un material no lineal, el campo de polarización eléctrica P dependerá del campo eléctrico E:  donde ε0 es la permitividad del vacío y χ(n) es la componente de orden n de la permisividad susceptibilidad]] del medio. El símbolo "":"" representa el producto escalar entre matrices. Podemos escribir esa relación explícitamente; la i-ésima componente del vector P se puede expresar como:  donde     i = 1 , 2 , 3   {\displaystyle i=1,2,3}  . A menudo se supone que      P  1   =  P  x     {\displaystyle P_{1}=P_{x}}  , es decir, la componente paralela a x del campo de polarización;      E  2   =  E  y     {\displaystyle E_{2}=E_{y}}   y así sucesivamente.  Para un medio lineal, solo el primer término de esta ecuación es significativo y la polarización varía linealmente con el campo eléctrico.  Para los materiales que exhiben un efecto Kerr no despreciable, el tercer término χ(3) es significativo, y los términos de orden par generalmente desaparecen debido a la simetría de inversión del medio de Kerr. Considere el campo eléctrico neto E producido por una onda de luz de frecuencia ω junto con un campo eléctrico externo E0:  donde Eω es el vector de amplitud de la onda.  La combinación de estas dos ecuaciones produce una expresión compleja para P. Para el efecto DC Kerr, podemos ignorar todos excepto los términos lineales y aquellos en      χ  ( 3 )    |    E   0     |   2     E   ω     {\displaystyle \chi ^{(3)}|\mathbf {E} _{0}|^{2}\mathbf {E} _{\omega }}   :  que es similar a la relación lineal entre la polarización y un campo eléctrico de una onda, con un término adicional de susceptibilidad no lineal proporcional al cuadrado de la amplitud del campo externo.  Para medios no simétricos (por ejemplo, líquidos), este cambio inducido de susceptibilidad produce un cambio en el índice de refracción en la dirección del campo eléctrico:  donde λ0 es el vacío longitud de onda y K es la constante de Kerr para el medio. El campo aplicado induce birrefringencia en el medio en la dirección del campo. Una celda de Kerr con un campo transversal puede actuar como una placa de ondas conmutable, rotando el plano de polarización de una onda que viaja a través de ella. En combinación con polarizadores, se puede utilizar como obturador o modulador.  Los valores de K dependen del medio y son aproximadamente 9,4×10−14 m·V−2 para agua, y 4,4×10−12 m·V−2 para nitrobenceno.[8]  Para cristales, la susceptibilidad del medio será en general un tensor, y el efecto Kerr produce una modificación de este tensor.  Efecto Kerr con corriente alterna En el efecto óptico o AC Kerr, un haz de luz intenso en un medio puede por sí mismo proporcionar el campo eléctrico modulador, sin necesidad de aplicar un campo externo. En este caso, el campo eléctrico viene dado por:  donde Eω es la amplitud de la onda como antes.  Combinando esto con la ecuación para la polarización, y tomando solo términos lineales y aquellos en χ(3)|Eω|3< /sup>:[9]: 81–82   Como antes, esto parece una susceptibilidad lineal con un término no lineal adicional:  y desde:  donde n0=(1+χLIN)1/2 es el índice de refracción lineal. Usando una expansión de Taylor ya que χNL << n02, esto da una intensidad refractiva dependiente índice (IDRI) de:  donde n2 es el índice de refracción no lineal de segundo orden e I es la intensidad de la onda. El cambio del índice de refracción es, por lo tanto, proporcional a la intensidad de la luz que viaja a través del medio.  Los valores de n2 son relativamente pequeños para la mayoría de los materiales, del orden de 10−20 m2 W −1 para vasos típicos. Por lo tanto, las intensidades del haz (irradiancias) del orden de 1 GW cm−2 (como las producidas por los láseres) son necesarias para producir variaciones significativas en el índice de refracción a través del efecto Kerr con corriente alterna.  El efecto Kerr óptico se manifiesta temporalmente como modulación de fase propia, un cambio de fase y frecuencia autoinducido de un pulso de luz a medida que viaja a través de un medio. Este proceso, junto con la dispersión, puede producir solitones ópticos.  Espacialmente, un haz intenso de luz en un medio producirá un cambio en el índice de refracción del medio que imita el patrón de intensidad transversal del haz. Por ejemplo, un haz gaussiano da como resultado un perfil de índice de refracción gaussiano, similar al de una lente de índice de gradiente. Esto hace que el haz se enfoque a sí mismo, un fenómeno conocido como autoenfoque.  A medida que el haz se autoenfoca, la intensidad máxima aumenta, lo que, a su vez, hace que se produzca más autoenfoque. Los efectos no lineales como la ionización multifotónica, que se vuelven importantes cuando la intensidad se vuelve muy alta, impiden que el haz se autoenfoque indefinidamente. A medida que la intensidad del punto autoenfocado aumenta más allá de cierto valor, el medio es ionizado por el alto campo óptico local. Esto reduce el índice de refracción, desenfocando el haz de luz que se propaga. Luego, la propagación continúa en una serie de pasos repetidos de enfoque y desenfoque"
fisica,"La fotometría es la rama de la Astronomía que se dedica a medir el brillo de los diferentes astros: estrellas, planetas, satélites, asteroides, cometas, etc. La escala de brillos de las estrellas fue establecida por el astrónomo griego Hiparco de Nicea, quien dividió estos brillos en cinco grados o magnitudes; más tarde, con la invención del telescopio por Galileo en 1609, se amplió la escala para incluir estos astros telescópicos, invisibles al ojo humano por su extrema debilidad.  Los astros más brillantes (como el Sol) tienen magnitud negativa mientras que los más débiles la tienen positiva, siendo esta tanto mayor cuanto más débiles son: el Sol tiene magnitud -26,8, Sirio -1,5, la Estrella Polar 2,12, Urano 5,8, Neptuno 7,2 y Plutón 13,6. Las estrellas más débiles que un telescopio profesional puede capturar es superior a la 25.  En el siglo XIX, Norman Pogson determinó correctamente la escala de magnitudes, de tal manera que el salto de una magnitud a otra (por ejemplo, de la 1.ª a la 2.ª, o de la 2.ª a la 3.ª) corresponde a un cambio igual a 2,512 veces, siendo este valor la raíz quinta de 100.  Existen distintos métodos: fotometría visual, fotográfica, con fotómetro fotoeléctrico (fotometría fotoeléctrica) y más reciente con cámaras CCD (fotometría CCD); todos ellos trabajan en distintas bandas (Banda V, Banda B, etc.) según el filtro utilizado al efectuar las mediciones.  Para efectuar estas mediciones se han definido unos sistemas fotométricos, los más conocidos de los cuales son el UBV de W. W. Morgan y Harold Johnson y el UBVRI de A. Cousins y J. Menzies.  Si la precisión con la que se medían las magnitudes a mediados del s. XX era de una centésima, con el uso de la fotometría CCD se ha ampliado esta precisión a milésimas de magnitud: en 2006, a diezmilésimas de magnitud, en un estudio fotométrico del cúmulo abierto M67. En 2009 el satélite Kepler se lanzó al espacio con un sensor capaz de detectar cambios de 20 partes por millón (1/50 000)[1]  La fotometría también se utiliza en la observación de estrellas variabless,[2] mediante diversas técnicas como la fotometría diferencial, que mide simultáneamente el brillo de un objeto y de las estrellas cercanas en el campo estelar[3] o fotometría relativa comparando el brillo del objeto objetivo con estrellas de magnitudes fijas conocidas.[4] El uso de múltiples filtros de paso de banda con fotometría relativa se denomina fotometría absoluta. Un trazado de la magnitud frente al tiempo produce una curva de luz, arrojando información considerable sobre el proceso físico que causa los cambios de brillo.[5] Los fotómetros fotoeléctricos de precisión pueden medir la luz de las estrellas en torno a 0,001 de magnitud.[6]  La técnica de fotometría de superficie también puede utilizarse con objetos extendidos como planetas, cometas, nebulosae o galaxias que mide la magnitud aparente en términos de magnitudes por segundo de arco cuadrado.[7] Conociendo el área del objeto y la intensidad media de la luz a través del objeto astronómico se determina el brillo superficial en términos de magnitudes por segundo de arco cuadrado, mientras que integrando la luz total del objeto extendido se puede entonces calcular el brillo en términos de su magnitud total, producción de energía o luminosidad por unidad de superficie.  Métodos Los fotómetros emplean el uso de filtros estándar especializados de banda de paso en las longitudes de onda ultravioleta, visible e infrarrojo del espectro electromagnético. [2] Cualquier conjunto adoptado de filtros con propiedades de transmisión de la luz conocidas se denomina sistema fotométrico, y permite establecer propiedades particulares sobre las estrellas y otros tipos de objetos astronómicos.[8] Se utilizan regularmente varios sistemas importantes, como el sistema UBV[9] (o el sistema UBVRI ampliado[10][11]), infrarrojo cercano JHK[12] o el Sistema uvbyβ de Strömgren.[8]  Históricamente, la fotometría en el cercano-infrarrojo hasta el ultravioleta de longitud de onda corta se realizaba con un fotómetro fotoeléctrico, un instrumento que medía la intensidad de la luz de un solo objeto dirigiendo su luz hacia una célula fotosensible como un tubo fotomultiplicador. [2] Se han sustituido en gran medida por las cámaras CCD que pueden tomar imágenes simultáneamente de múltiples objetos, aunque los fotómetros fotoeléctricos se siguen utilizando en situaciones especiales,[13] such as where fine time resolution is required.[14]  Índices de magnitud y color Los métodos fotométricos modernos definen las magnitudes y colores de los objetos astronómicos mediante el uso de fotómetros electrónicos analizados mediante filtors pasabanda coloreados estándar. Ello se diferencia de otras modalidades de magnitud visual aparente[5] observadas por el ojo humano u obtenidas mediante fotografía:[2] que por lo general aparecen en textos y catálogos astronómicos antiguos.  Aplicaciones Hay muchas aplicaciones astronómicas que se utilizan con los sistemas fotométricos. Las mediciones fotométricas pueden combinarse con la ley de la inversa del cuadrado para determinar la luminosidad de un objeto si se puede determinar su distancia, o su distancia si se conoce su luminosidad. Otras propiedades físicas de un objeto, como su temperatura o su composición química, también pueden determinarse mediante espectrofotometría de banda ancha o estrecha.  La fotometría también se utiliza para estudiar las variaciones de luz de objetos como estrellas variables, planetas menores,  núcleos galácticos activos y supernovas,[5] o para detectar planeta extrasolar en tránsito. Las mediciones de estas variaciones pueden utilizarse, por ejemplo, para determinar el período orbital y el radio de los miembros de un sistema de estrella binaria deslizante, el período de rotación de un planeta menor o una estrella, o la producción total de energía de las supernovas.[5]  Fotometría CCD Una cámara CCD es esencialmente una red de fotómetros, que miden y registran simultáneamente los fotones procedentes de todas las fuentes del campo de visión. Dado que cada imagen CCD registra la fotometría de múltiples objetos a la vez, se pueden realizar varias formas de extracción fotométrica de los datos registrados; normalmente relativa, absoluta y diferencial. Las tres requerirán la extracción de la imagen cruda magnitud del objeto objetivo, y un objeto de comparación conocido.  La señal observada de un objeto cubrirá típicamente muchos píxeles según la función de dispersión de puntos (PSF) del sistema. Este ensanchamiento se debe tanto a la óptica del telescopio como al seeing astronómico. Cuando se obtiene fotometría de una fuente puntual, el flujo se mide sumando toda la luz registrada del objeto y restando la luz debida al cielo.[15] La técnica más sencilla, conocida como fotometría de apertura, consiste en sumar los recuentos de píxeles dentro de una apertura centrada en el objeto y restar el producto del recuento medio del cielo cercano por píxel y el número de píxeles dentro de la apertura.[15][16] Esto dará como resultado el valor del flujo bruto del objeto objetivo.  Cuando se hace fotometría en un campo muy poblado, como un cúmulo globular, donde los perfiles de las estrellas se superponen significativamente, hay que utilizar técnicas de desmezcla, como el ajuste de PSF para determinar los valores de flujo individuales de las fuentes superpuestas.[17]  Calibraciones Después de determinar el flujo de un objeto en cuentas, el flujo se convierte normalmente en magnitud instrumental. A continuación, la medición se calibra de alguna manera. Las calibraciones que se utilicen dependerán en parte del tipo de fotometría que se realice. Normalmente, las observaciones se procesan para la fotometría relativa o diferencial.[18] La fotometría relativa es la medición del brillo aparente de múltiples objetos en relación con los demás. La fotometría absoluta es la medición del brillo aparente de un objeto en un sistema fotométrico estándar; estas mediciones pueden compararse con otras mediciones fotométricas absolutas obtenidas con diferentes telescopios o instrumentos. La fotometría diferencial es la medición de la diferencia de brillo de dos objetos. En la mayoría de los casos, la fotometría diferencial puede realizarse con la mayor precisión, mientras que la fotometría absoluta es la más difícil de realizar con alta precisión. Además, la fotometría precisa suele ser más difícil cuando el brillo aparente del objeto es más débil.  Fotometría absoluta Para realizar una fotometría absoluta, se deben corregir las diferencias entre la banda de paso efectiva a través de la cual se observa un objeto y la banda de paso utilizada para definir el sistema fotométrico estándar. Esto es a menudo además de todas las otras correcciones discutidas anteriormente. Por lo general, esta corrección se realiza observando los objetos de interés a través de múltiples filtros y también observando una serie de estrellas estándar fotométricas . Si las estrellas estándar no pueden observarse simultáneamente con los objetivos, esta corrección debe realizarse en condiciones fotométricas, cuando el cielo está despejado y la extinción es una función simple de la masa de aire.  Fotometría relativa Para realizar la fotometría relativa, uno compara la magnitud del instrumento del objeto con un objeto de comparación conocido y luego corrige las mediciones para las variaciones espaciales en la sensibilidad del instrumento y la extinción atmosférica. A menudo, esto se suma a la corrección de sus variaciones temporales, particularmente cuando los objetos que se comparan están demasiado separados en el cielo para observarlos simultáneamente. [6] Cuando se realiza la calibración a partir de una imagen que contiene tanto el objetivo como los objetos de comparación muy cerca, y se usa un filtro fotométrico que coincide con la magnitud del catálogo del objeto de comparación, la mayoría de las variaciones de medición se reducen a cero.  Fotometría diferencial La fotometría diferencial es la más simple de las calibraciones y la más útil para las observaciones de series de tiempo.[3] Cuando se usa la fotometría CCD, tanto el objetivo como los objetos de comparación se observan al mismo tiempo, con los mismos filtros, usando el mismo instrumento y se ven a través de la misma ruta óptica. La mayoría de las variables observacionales desaparecen y la magnitud diferencial es simplemente la diferencia entre la magnitud del instrumento del objeto objetivo y el objeto de comparación (∆Mag = C Mag – T Mag). Esto es muy útil cuando se traza el cambio de magnitud a lo largo del tiempo de un objeto de destino y, por lo general, se compila en una curva de luz.[3]  Fotometría de superficie Para los objetos espacialmente extendidos, como las galaxias, a menudo es interesante medir la distribución espacial del brillo dentro de la galaxia en lugar de simplemente medir el brillo total de la galaxia. El brillo superficial de un objeto es su brillo por unidad de ángulo sólido visto en proyección en el cielo, y la medición del brillo superficial se conoce como fotometría de superficie.[7] Una aplicación común sería la medición del perfil de brillo superficial de una galaxia, es decir, su brillo superficial en función de la distancia desde el centro de la galaxia. Para ángulos sólidos pequeños, una unidad útil de ángulo sólido es el arcosegundo, y el brillo superficial se expresa a menudo en magnitudes por arcosegundo cuadrado"
fisica,"En física la ley de Gauss, relacionada con el teorema de la divergencia o teorema de Gauss,[1] establece que el flujo de ciertos campos a través de una superficie cerrada es proporcional a la magnitud de las fuentes de dicho campo que hay en el interior de la misma superficie. Estos campos son aquellos cuya intensidad decrece como la distancia a la fuente al cuadrado. La constante de proporcionalidad depende del sistema de unidades empleado.  Se aplica al campo electrostático y al gravitatorio. Sus fuentes son la carga eléctrica y la masa, respectivamente. También puede aplicarse al campo magnetostático.  La ley fue formulada por Carl Friedrich Gauss en 1835, pero no fue publicado hasta 1867.[2] Esta es una de las cuatro ecuaciones de Maxwell, que forman la base de electrodinámica clásica (las otras tres son la ley de Gauss para el magnetismo, la ley de Faraday de la inducción y la ley de Ampère con la corrección de Maxwell). La ley de Gauss puede ser utilizada para obtener la ley de Coulomb,[3] y viceversa.  Flujo del campo eléctrico El flujo (denotado como     Φ   {\displaystyle \Phi }  ) es una propiedad de cualquier campo vectorial referida a una superficie hipotética que puede ser cerrada o abierta. Para un campo eléctrico, el flujo (     Φ  E     {\displaystyle \Phi _{E}}  ) se mide por el número de líneas de fuerza que atraviesan la superficie.  Para definir al flujo eléctrico con precisión considérese la figura, que muestra una superficie cerrada arbitraria ubicada dentro de un campo eléctrico.  La superficie se encuentra dividida en cuadrados elementales     Δ S   {\displaystyle \Delta S}  , cada uno de los cuales es lo suficientemente pequeño como para que pueda ser considerado como un plano. Estos elementos de área pueden ser representados como vectores         Δ S  →      {\displaystyle {\vec {\Delta S}}}  , cuya magnitud es la propia área, la dirección es perpendicular a la superficie y hacia afuera.  En cada cuadrado elemental también es posible trazar un vector de campo eléctrico        E →      {\displaystyle {\vec {E}}}  . Ya que los cuadrados son tan pequeños como se quiera,     E   {\displaystyle E}   puede considerarse constante en todos los puntos de un cuadrado dado.         E →      {\displaystyle {\vec {E}}}   y         Δ S  →      {\displaystyle {\vec {\Delta S}}}   caracterizan a cada cuadrado y forman un ángulo     θ   {\displaystyle \theta }   entre sí y la figura muestra una vista amplificada de dos cuadrados.  El flujo, entonces, se define como sigue:  (1)      Φ   E   = ∑    E →    ⋅ Δ    S →      {\displaystyle {\Phi }_{E}=\sum {\vec {E}}\cdot \Delta {\vec {S}}}    O sea:  (2)      Φ   E   =  ∮  S      E →    ⋅ d    s →      {\displaystyle {\Phi }_{E}=\oint _{S}{\vec {E}}\cdot d{\vec {s}}}    Flujo para una superficie cilíndrica en presencia de un campo uniforme Supóngase una superficie cilíndrica colocada dentro de un campo uniforme        E →      {\displaystyle {\vec {E}}}   tal como muestra la figura:  El flujo       Φ   E     {\displaystyle {\Phi }_{E}}   puede escribirse como la suma de tres términos, (a) una integral en la tapa izquierda del cilindro, (b) una integral en la superficie cilíndrica y (c) una integral en la tapa derecha:  (3)      Φ   E   = ∮    E →    ⋅ d    s →    =   ∫   ( a )      E →    ⋅ d    S →    +   ∫   ( b )      E →    ⋅ d    S →    +   ∫   ( c )      E →    ⋅ d    S →      {\displaystyle {\Phi }_{E}=\oint {\vec {E}}\cdot d{\vec {s}}={\int }_{(a)}{\vec {E}}\cdot d{\vec {S}}+{\int }_{(b)}{\vec {E}}\cdot d{\vec {S}}+{\int }_{(c)}{\vec {E}}\cdot d{\vec {S}}}    Para la tapa izquierda, el ángulo     θ   {\displaystyle \theta }  , para todos los puntos, es de     π   {\displaystyle \pi }  ,     E   {\displaystyle E}   tiene un valor constante y los vectores     d S   {\displaystyle dS}   son todos paralelos.  Entonces:  (4)      ∫   ( a )      E →    ⋅ d    S →    = ∫ E cos ⁡ ( π ) d S = − E ∫ d S = − E S   {\displaystyle {\int }_{(a)}{\vec {E}}\cdot d{\vec {S}}=\int E\cos(\pi )dS=-E\int dS=-ES}    siendo     S = π  R  2     {\displaystyle S=\pi R^{2}}  el área de la tapa. Análogamente, para la tapa derecha:  (5)      ∫   ( c )      E →    ⋅ d    S →    = ∫ E cos ⁡ ( 0 ) d S = E ∫ d S = E S   {\displaystyle {\int }_{(c)}{\vec {E}}\cdot d{\vec {S}}=\int E\cos(0)dS=E\int dS=ES}    Finalmente, para la superficie cilíndrica:  (6)      ∫   ( b )      E →    ⋅ d    S →    = ∫ E cos ⁡   (     π 2     )   d S = 0   {\displaystyle {\int }_{(b)}{\vec {E}}\cdot d{\vec {S}}=\int E\cos {\bigg (}{\pi  \over 2}{\bigg )}dS=0}    Por consiguiente: da cero ya que las mismas líneas de fuerza que entran, después salen del cilindro.  (7)      Φ   E   = − E S + 0 + E S   = 0   {\displaystyle {\Phi }_{E}=-ES+0+ES\,\!=0}    Flujo para una superficie esférica con una carga puntual en su interior Considérese una superficie esférica de radio r con una carga puntual q en su centro tal como muestra la figura. El campo eléctrico        E →      {\displaystyle {\vec {E}}}   es paralelo al vector superficie         d S  →      {\displaystyle {\vec {dS}}}  , y el campo es constante en todos los puntos de la superficie esférica.  En consecuencia:  (8)     Φ  E   =  ∫  S      E →    ⋅ d    S →    =  ∫  S   E cos ⁡ θ d S =  ∫  S   E cos ⁡ ( 0 ) d S = E  ∫  S   d S = E 4 π  r  2     {\displaystyle \Phi _{E}=\int _{S}{\vec {E}}\cdot d{\vec {S}}=\int _{S}E\cos \theta dS=\int _{S}E\cos(0)dS=E\int _{S}dS=E4\pi r^{2}}    Deducciones Deducción de la ley de Gauss a partir de la ley de Coulomb Este teorema aplicado al campo eléctrico creado por una carga puntual es equivalente a la ley de Coulomb de la interacción electrostática.  La ley de Gauss puede deducirse matemáticamente a través del uso del concepto de ángulo sólido, que es un concepto muy similar a los factores de vista conocidos en la transferencia de calor por radiación.  El ángulo sólido     Δ  Ω    {\displaystyle \Delta {\Omega }}   que es subtendido por     Δ  A    {\displaystyle \Delta {A}}   sobre una superficie esférica, se define como:      Δ  Ω  =    Δ  A    r  2       {\displaystyle \Delta {\Omega }={\frac {\Delta {A}}{r^{2}}}}    siendo     r   {\displaystyle r}   el radio de la esfera.  como el área total de la esfera es     4 π  r  2     {\displaystyle 4\pi r^{2}}   el ángulo sólido para ‘’toda la esfera’’ es:      Δ  Ω  =    Δ  A    r  2     =    4 π  r  2     r  2     = 4 π   {\displaystyle \Delta {\Omega }={\frac {\Delta {A}}{r^{2}}}={\frac {4\pi r^{2}}{r^{2}}}=4\pi }    la unidad de este ángulo es el estereorradián (sr)  Si el área     Δ  A    {\displaystyle \Delta {A}}   no es perpendicular a las líneas que salen del origen que subtiende a     Δ  Ω    {\displaystyle \Delta {\Omega }}  , se busca la proyección normal, que es:      Δ  Ω  =    Δ  A         n ^    ⋅    r ^      r  2     =    Δ  A  cos ⁡  θ    r  2       {\displaystyle \Delta {\Omega }={\frac {\Delta {A}{~}{\hat {n}}\cdot {\hat {r}}}{r^{2}}}={\frac {\Delta {A}\cos {\theta }}{r^{2}}}}    Si se tiene una carga ""q"" rodeada por una superficie cualquiera, para calcular el flujo que atraviesa esta superficie es necesario encontrar        E →    ⋅    n ^      Δ  A    {\displaystyle {\vec {E}}\cdot {\hat {n}}{}\Delta {A}}   para cada elemento de área de la superficie, para luego sumarlos. Como la superficie que puede estar rodeando a la carga puede ser tan compleja como quiera, es mejor encontrar una relación sencilla para esta operación:      Δ  ϕ  =    E →    ⋅    n ^        Δ  A  =    K q   r  2        r ^    ⋅    n ^    Δ  A  = K q Δ  Ω    {\displaystyle \Delta {\phi }={\vec {E}}\cdot {\hat {n}}{~}\Delta {A}={\frac {Kq}{r^{2}}}{\hat {r}}\cdot {\hat {n}}\Delta {A}=Kq\Delta {\Omega }}    De esta manera     Δ  Ω    {\displaystyle \Delta {\Omega }}   es el mismo ángulo sólido subentendido por una superficie esférica. como se mostró un poco más arriba     Δ  Ω  = 4 π   {\displaystyle \Delta {\Omega }=4\pi }   para cualquier esfera, de cualquier radio. de esta forma al sumar todos los flujos que atraviesan a la superficie queda:       ϕ  n e t o   =  ∮  S      E →    ⋅    n ^    d  A  = K q  ∮  0   4 π   d  Ω  = 4 π K q =   q  ϵ  0       {\displaystyle \phi _{neto}=\oint _{S}{\vec {E}}\cdot {\hat {n}}d{A}=Kq\oint _{0}^{4\pi }d{\Omega }=4\pi Kq={\frac {q}{\epsilon _{0}}}}    que es la forma integral de la ley de Gauss. La ley de Coulomb también puede deducirse a través de Ley de Gauss.  Forma diferencial e integral de la Ley de Gauss Forma diferencial de la ley de Gauss Tomando la ley de Gauss en forma integral.  Aplicando al primer término el teorema de Gauss de la divergencia queda  Como ambos lados de la igualdad poseen diferenciales volumétricas, y esta expresión debe ser cierta para cualquier volumen, solo puede ser que:  Que es la forma diferencial de la Ley de Gauss (en el vacío).  Esta ley se puede generalizar cuando hay un dieléctrico presente, introduciendo el campo de desplazamiento eléctrico        D →      {\displaystyle {\vec {D}}}  , de esta manera la Ley de Gauss se puede escribir en su forma más general como  Finalmente es de esta forma en que la ley de Gauss es realmente útil para resolver problemas complejos de maneras relativamente sencillas.  Forma integral de la ley de Gauss Su forma integral utilizada en el caso de una distribución extensa de carga puede escribirse de la manera siguiente:  donde     Φ   {\displaystyle \Phi }   es el flujo eléctrico,        E →      {\displaystyle {\vec {E}}}   es el campo eléctrico,     d    A →      {\displaystyle d{\vec {A}}}   es un elemento diferencial del área A sobre la cual se realiza la integral,      Q   A      {\displaystyle Q_{\mathrm {A} }}   es la carga total encerrada dentro del área A,     ρ   {\displaystyle \rho }   es la densidad de carga en un punto de     V   {\displaystyle V}   y      ϵ  o     {\displaystyle \epsilon _{o}}   es la permitividad eléctrica del vacío.  Interpretación La ley de Gauss puede ser utilizada para demostrar que no existe campo eléctrico dentro de una jaula de Faraday. La ley de Gauss es la equivalente electrostática a la ley de Ampère, que es una ley de magnetismo. Ambas ecuaciones fueron posteriormente integradas en las ecuaciones de Maxwell.  Esta ley puede interpretarse, en electrostática, entendiendo el flujo como una medida del número de líneas de campo que atraviesan la superficie en cuestión. Para una carga puntual este número es constante si la carga está contenida por la superficie y es nulo si está fuera (ya que hay el mismo número de líneas que entran como que salen). Además, al ser la densidad de líneas proporcional a la magnitud de la carga, resulta que este flujo es proporcional a la carga, si está encerrada, o nulo, si no lo está.  Cuando tenemos una distribución de cargas, por el principio de superposición, sólo tendremos que considerar las cargas interiores, resultando la ley de Gauss.  Sin embargo, aunque esta ley se deduce de la ley de Coulomb, es más general que ella, ya que se trata de una ley universal, válida en situaciones no electrostáticas en las que la ley de Coulomb no es aplicable.  Aplicaciones Distribución lineal de carga Sea una recta cargada a lo largo del eje z. Tomemos como superficie cerrada un cilindro de radio r y altura h con su eje coincidente al eje z. Expresando el campo en coordenadas cilíndricas tenemos que debido a la simetría de reflexión respecto a un plano z=cte el campo no tiene componente en el eje z y la integración a las bases del cilindro no contribuye, de modo que aplicando la ley de Gauss:  Debido a la simetría del problema el campo tendrá dirección radial y podemos sustituir el producto escalar por el producto de módulos (ya que la dirección de la superficie "
fisica,"La radiación electromagnética es un tipo de campo electromagnético variable, es decir, una combinación de campos eléctricos y magnéticos oscilantes, que se propagan a través del espacio transportando energía de un lugar a otro.[1] Desde el punto de vista clásico, la radiación electromagnética son las ondas electromagnéticas generadas por las fuentes del campo electromagnético y que se propagan a la velocidad de la luz. La generación y la propagación de estas ondas son compatibles con el modelo de ecuaciones matemáticas definido en las ecuaciones de Maxwell.  La radiación de tipo electromagnético puede manifestarse de diversas maneras, como ondas de radio, microondas, radiación infrarroja, luz visible, radiación ultravioleta, rayos X y rayos gamma. A diferencia de otros tipos de onda, como el sonido, que necesitaran un medio material para propagarse, la radiación electromagnética se puede propagar en el vacío. En el siglo XIX se pensaba que existía una sustancia indetectable, llamada éter, que ocupaba el vacío y servía de medio de propagación de las ondas electromagnéticas. El estudio teórico de la radiación electromagnética se denomina electrodinámica y es un subcampo del electromagnetismo.  Las ondas electromagnéticas pueden ser generadas por distintas fuentes como son: cargas aceleradas, dipolos oscilantes, corrientes variables en distintos tipos de antenas, entre otras. La forma de las ondas electromagnéticas depende de la fuente que las genera y de la distancia recorrida por las mismas.[2]  Historia del descubrimiento La radiación electromagnética de longitudes de onda distintas a las de la luz visible se descubrió a principios del siglo XIX. El descubrimiento de la radiación infrarroja se atribuye al astrónomo William Herschel, quien publicó sus resultados en 1800 ante la Royal Society of London.[3] Herschel utilizó un prisma de vidrio para refractar la luz del Sol y detectó rayos invisibles que provocaban un calentamiento más allá de la parte roja del espectro, mediante un aumento de la temperatura registrada con un termómetro. Estos «rayos caloríficos» se denominaron posteriormente infrarrojos.[4]  En 1801, el físico alemán Johann Wilhelm Ritter descubrió la luz ultravioleta en un experimento similar al de Herschel, utilizando luz solar y un prisma de vidrio. Ritter notó que los rayos invisibles cerca del borde violeta de un espectro solar dispersado por un prisma triangular oscurecían las preparaciones de cloruro de plata más rápidamente que la luz violeta. Los experimentos de Ritter fueron los primeros precursores de lo que se convertiría en la fotografía. Ritter señaló que los rayos ultravioleta (que al principio se llamaron «rayos químicos») eran capaces de provocar reacciones químicas.[5]  En 1862-1864, James Clerk Maxwell desarrolló ecuaciones para el campo electromagnético que sugerían que las ondas en el campo viajarían con una velocidad muy cercana a la velocidad de la luz. Por lo tanto, Maxwell sugirió que la luz visible (así como los rayos infrarrojos y ultravioleta invisibles por inferencia) consistían en propagar perturbaciones (o radiación) en el campo electromagnético. Las ondas de radio fueron producidas deliberadamente por primera vez por Heinrich Hertz en 1887, utilizando circuitos eléctricos calculados para producir oscilaciones a una frecuencia mucho más baja que la de la luz visible. Hertz también desarrolló formas de detectar estas ondas y produjo y caracterizó lo que luego se denominaron ondas de radio y microondas.[6]  Wilhelm Röntgen descubrió y nombró radiografías. Después de experimentar con altos voltajes aplicados a un tubo de vacío el 8 de noviembre de 1895, notó una fluorescencia en una placa cercana de vidrio revestido. En un mes, descubrió las principales propiedades de los rayos X.[6]  La última parte del espectro electromagnético que se descubrió se asoció con la radiactividad. Henri Becquerel descubrió que las sales de uranio causaban el empañamiento de una placa fotográfica no expuesta a través de un papel de cobertura de una manera similar a los rayos X, y Marie Curie descubrió que solo ciertos elementos emitían estos rayos de energía, y pronto descubrió la intensa radiación del radio. La radiación de la pechblenda fue diferenciada en rayos alfa (partículas alfa) y rayos beta (partículas beta) por Ernest Rutherford a través de una simple experimentación en 1899, pero se demostró que eran tipos de radiación de partículas cargadas. Sin embargo, en 1900, el científico francés Paul Villard descubrió un tercer tipo de radiación de radio con carga neutra y especialmente penetrante, y después de describirlo, Rutherford se dio cuenta de que debía ser un tercer tipo de radiación, que en 1903 denominó rayos gamma. En 1910, el físico británico William Henry Bragg demostró que los rayos gamma son radiación electromagnética, no partículas, y en 1914 Rutherford y Edward Andrade midieron sus longitudes de onda, encontrando que eran similares a los rayos X pero con longitudes de onda más cortas y mayor frecuencia.[6]  Fenómenos asociados a la radiación electromagnética Existen multitud de fenómenos físicos asociados con la radiación electromagnética que pueden ser estudiados de manera unificada, como la interacción de ondas electromagnéticas y partículas cargadas presentes en la materia. Entre estos fenómenos están por ejemplo la luz visible, el calor radiado, las ondas de radio y televisión o ciertos tipos de radioactividad por citar algunos de los fenómenos más destacados. Todos estos fenómenos consisten en la emisión de radiación electromagnética en diferentes rangos de frecuencias (o equivalentemente diferentes longitudes de onda), siendo el rango de frecuencia o longitud de onda el más usado para clasificar los diferentes tipos de radiación electromagnética. La ordenación de los diversos tipos de radiación electromagnética por frecuencia recibe el nombre de espectro electromagnético.  Luz visible La luz visible está formada por la radiación electromagnética cuyas longitudes de onda están comprendidas entre 400 y 700 nm. La luz es producida en la corteza atómica de los átomos, cuando un átomo por diversos motivos recibe energía puede que algunos de sus electrones pasen a capas electrónicas de mayor energía. Los electrones son inestables en capas altas de mayor energía si existen niveles energéticos inferiores desocupados, por lo que tienden a caer hacia estos, pero al decaer hacia niveles inferiores la conservación de la energía requiere la emisión de fotones, cuyas frecuencias suelen caer en los rangos asociados a la luz visible. Eso es precisamente lo que sucede en fenómenos de emisión primaria tan diversos como la llama del fuego, un filamento incandescente de una lámpara o la luz procedente del sol. Secundariamente la luz procedente de emisión primaria puede ser reflejada, refractada, absorbida parcialmente y esa es la razón por la cual objetos que no son fuentes de emisión primaria son visibles.  Radiación térmica Cuando se somete algún metal y otras sustancias a fuentes de calor estas se calientan y llegan a emitir luz visible. Para un metal este fenómeno se denomina calentar ""al rojo vivo"", ya que la luz emitida inicialmente es rojiza-anaranjada, si la temperatura se eleva más blanca-amarillenta. Conviene señalar que antes que la luz emitida por metales y otras sustancias sobrecalentadas sea visible, estos mismos cuerpos irradian calor en forma de radiación infrarroja que es un tipo de radiación electromagnética no visible directamente por el ojo humano.  Interacción entre radiación electromagnética y conductores Cuando un alambre o cualquier objeto conductor, tal como una antena, conduce corriente alterna, la radiación electromagnética se propaga en la misma frecuencia que la corriente.  De forma similar, cuando una radiación electromagnética incide en un conductor eléctrico, hace que los electrones de su superficie oscilen, generándose de esta forma una corriente alterna cuya frecuencia es la misma que la de la radiación incidente. Este efecto se usa en las antenas, que pueden actuar como emisores o receptores de radiación electromagnética.  Estudios mediante análisis del espectro electromagnético Se puede obtener mucha información acerca de las propiedades físicas de un objeto a través del estudio de su espectro electromagnético, ya sea por la luz emitida (radiación de cuerpo negro) o absorbida por él. Esto es la espectroscopia y se usa ampliamente en astrofísica y química. Por ejemplo, los átomos de hidrógeno tienen una frecuencia natural de oscilación, por lo que emiten ondas de radio, las cuales tiene una longitud de onda de 21,12 cm.  Penetración de la radiación electromagnética En función de la frecuencia, las ondas electromagnéticas pueden no atravesar medios conductores. Esta es la razón por la cual las transmisiones de radio no funcionan bajo el mar y los teléfonos móviles se queden sin cobertura dentro de una caja de metal. Sin embargo, como la energía no se crea ni se destruye, cuando una onda electromagnética choca con un conductor pueden suceder dos cosas. La primera es que se transformen en calor: este efecto tiene aplicación en los hornos de microondas. La segunda es que se reflejen en la superficie del conductor (como en un espejo).  Refracción La velocidad de propagación de la radiación electromagnética en el vacío es «c». La teoría electromagnética establece que:  donde      ε  0     {\displaystyle \varepsilon _{0}}   y      μ  0     {\displaystyle \mu _{0}}   son la permitividad eléctrica y la permeabilidad magnética del vacío respectivamente.  En un medio material la permitividad eléctrica     ε   {\displaystyle \varepsilon }   tiene un valor diferente a      ε  0     {\displaystyle \varepsilon _{0}}  . Lo mismo ocurre con la permeabilidad magnética     μ   {\displaystyle \mu }   y, por lo tanto, la velocidad de la luz en ese medio     v   {\displaystyle v}   será diferente a c. La velocidad de propagación de la luz en medios diferentes al vacío es siempre inferior a c.  Cuando la luz cambia de un medio a otro experimenta una desviación que depende del ángulo con que incide en la superficie que separa a los dos medios. Se habla, entonces, de un ángulo incidente y de un ángulo de transmisión. Este fenómeno, denominado refracción, es claramente apreciable en la desviación que presentan los haces de luz cuando inciden en el agua. La velocidad de la luz en un medio se puede calcular a partir de su permitividad eléctrica y de su permeabilidad magnética de la siguiente manera:  Dispersión La permitividad eléctrica y la permeabilidad magnética de un medio diferente del vacío dependen, además de la naturaleza del medio, de la longitud de onda de la radiación. De esto se desprende que la velocidad de propagación de la radiación electromagnética en un medio depende también de la longitud de onda de dicha radiación. Por tanto, la desviación de un rayo de luz al cambiar de medio será diferente para cada color (para cada longitud de onda). El ejemplo más claro es el de un haz de luz blanca que se ""descompone"" en colores al pasar por un prisma. La luz blanca es realmente la suma de haces de luz de distintas longitudes de onda, que son desviadas de manera diferente. Este fenómeno se llama dispersión. Es el causante de la aberración cromática, el halo de colores que se puede apreciar alrededor de los objetos al observarlos con instrumentos que utilizan lentes como prismáticos o telescopios.  Radiación por partículas aceleradas Una consecuencia importante de la electrodinámica clásica es que una partícula cargada en movimiento acelerado (rectilíneo, circular o de otro tipo) debe emitir ondas electromagnéticas siendo la potencia emitida proporcional al cuadrado de su aceleración, de hecho la fórmula de Larmor para la potencia emitida viene dada por:      P =     q  2    a  2     6 π  ε  0     c   3        {\displaystyle P={\frac {q^{2}a^{2}}{6\pi \varepsilon _{0}{\text{c}}^{3}}}}    Donde:  Un ejemplo de este fenómeno de emisión de radiación por parte de partículas cargadas es la radiación de sincrotrón.  Espectro electromagnético Atendiendo a su longitud de onda, la radiación electromagnética recibe diferentes nombres, y varía desde los energéticos rayos gamma (con una longitud de onda del orden de picómetros) hasta las ondas de radio (longitudes de onda del orden de kilómetros), pasando por el espectro visible (cuya longitud de onda está en el rango de las décimas de micrómetro). El rango completo de longitudes de onda es lo que se denomina el espectro electromagnético.  El espectro visible "
fisica,"La luz procedente de una estrella, conocida como luz blanca, es una superposición de luces de diferentes colores, las cuales presentan una longitud de onda y una frecuencia específicas. La dispersión de la luz es un fenómeno que se produce cuando un rayo de luz blanca atraviesa un medio transparente (por ejemplo el aire) y se refracta, mostrando a la salida de este los respectivos colores que la constituyen.  La dispersión tiene su origen en una disminución en la velocidad de propagación de la luz cuando atraviesa el medio. Debido a que el material absorbe y remite la luz cuya frecuencia es cercana a la frecuencia de oscilación natural de los electrones que están presentes en él, esta luz se propaga un poco más despacio en comparación a luz de frecuencias distintas. Estas variaciones en la velocidad de propagación dependen índice de refracción del material y el oxígeno estos hacen que la luz, para frecuencias diferentes, se refracte de manera diferente. En el caso de una doble refracción (como sucede en el prisma) se distinguen entonces de manera organizada los colores que componen la luz blanca: la desviación es progresiva, siendo mayor para frecuencias mayores (menores longitudes de onda); por lo tanto, la luz roja es desviada de su trayectoria original en menor medida que la luz azul.  Ejemplo La descomposición de la luz blanca en los diferentes colores que la componen (data del siglo XVIII) debido al físico, astrónomo y matemático Isaac Newton. La luz blanca se descompone en estos colores principales:  Esto demuestra que la luz blanca está constituida por la superposición de todos estos colores. Cada uno de los cuales experimenta una desviación distinta ya que el índice de refracción de, por ejemplo, el vidrio es diferente para cada uno de los colores.  Si la luz de un color específico, proveniente del espectro de la luz blanca, atravesara un prisma, esta no se descompondría en otros colores ya que cada color que compone el espectro es un color puro o monocromático.  Dispersión del material y de la guía de ondas La dispersión cromática suele referirse a la dispersión del material a granel, es decir, al cambio del índice de refracción con la frecuencia óptica. Sin embargo, en una guía de ondas también existe el fenómeno de dispersión en guía de ondas, en cuyo caso la velocidad de fase de una onda en una estructura depende de su frecuencia simplemente debido a la geometría de la estructura. En términos más generales, la dispersión de la guía de onda puede ocurrir para las ondas que se propagan a través de cualquier estructura no homogénea (por ejemplo, un cristal fotónico), independientemente de que las ondas estén confinadas en alguna región.{En una guía de ondas, ambos tipos de dispersión estarán generalmente presentes, aunque no son estrictamente aditivos. Por ejemplo, en la fibra óptica la dispersión del material y de la guía de onda pueden anularse mutuamente de forma efectiva para producir una longitud de onda de dispersión cero, importante para la comunicación por fibra óptica rápida.  Dispersión material en óptica La dispersión del material puede ser un efecto deseable o indeseable en las aplicaciones ópticas. La dispersión de la luz por prismas de vidrio se utiliza para construir espectrómetros y espectroradiómetros. Sin embargo, en las lentes, la dispersión provoca aberración cromática, un efecto no deseado que puede degradar las imágenes en microscopios, telescopios y objetivos fotográficos.  La velocidad de fase, v, de una onda es un dado medio uniforme se expresa como  donde c es la velocidad de la luz en el vacío y n es el índice de refracción del medio.  En general, el índice de refracción es una función de la frecuencia f de la luz, o sea n = n(f), o de forma alternativa, con respecto a la longitud de onda de la onda n = n(λ). La dependencia de la longitud de onda del índice de refracción del material es por lo general cuantificada por su número de Abbe o sus coeficientes en una fórmula empírica tales como las ecuaciones de Cauchy o Sellmeier.  Debido a las relaciones de Kramers-Kronig, la dependencia de la longitud de onda de la parte real del índice de refracción se encuentra relacionada con la absorción del material, descripta por la componente imaginaria del índice de refracción (también denominado el coeficiente de extinción). En particular, para el caso de materiales no-magnéticos (μ = μ0), la susceptibilidad χ que aparece en las relaciones de Kramers–Kronig es la susceptibilidad eléctrica χe = n2 − 1.  La consecuencia comúnmente observada de la dispersión en óptica es la separación de la luz blanca en el espectro de colores mediante un prisma. A partir de la ley de Snell se observa que el ángulo de refracción de la luz en un prisma depende del índice de refracción del material del prisma. Como ese índice de refracción varía con la longitud de onda, se deduce que el ángulo con el que se refracta la luz también variará con la longitud de onda, provocando una separación angular de los colores conocida como dispersión angular.  Para la luz visible, los índices de refracción n de la mayoría de los materiales transparentes (por ejemplo, el aire, los vidrios) disminuyen al aumentar la longitud de onda λ:  o alternativamente:  En este caso, se dice que el medio tiene una dispersión normal. Mientras que, si el índice aumenta con el incremento de la longitud de onda (lo que suele ocurrir en el ultravioleta[1]), se dice que el medio tiene una dispersión anómala.  En la interfaz de dicho material con el aire o el vacío (índice de ~1), la ley de Snell predice que la luz que incide en un ángulo θ con respecto a la normal se refractará en un ángulo arcsin(sin θ/n). Por lo tanto, la luz azul, con un índice de refracción más alto, se refractará más fuertemente que la luz roja, dando como resultado el conocido patrón del arco iris.  Dispersión de la velocidad de grupo Más allá de la simple descripción de un cambio en la velocidad de fase con respecto a la longitud de onda, una consecuencia más grave de la dispersión en muchas aplicaciones se denomina dispersión de la velocidad de grupo (GVD). Mientras que la velocidad de fase v se define como  v = c/n, esto describe sólo un componente de frecuencia. Cuando se combinan diferentes componentes de frecuencia, como cuando se considera una señal o un pulso, uno suele estar más interesado en la velocidad de grupo que describe la velocidad a la que se propaga un pulso o información superpuesta a una onda (modulación). En la animación adjunta, se puede ver que la propia onda (naranja-marrón) viaja a una velocidad de fase que es mucho más rápida que la velocidad de la envoltura (negro) que corresponde a la velocidad de grupo. Este pulso podría ser una señal de comunicaciones, por ejemplo, y su información sólo viaja a la velocidad de grupo aunque esté formada por frentes de onda que avanzan a una velocidad mayor (la velocidad de fase).  Es posible calcular la velocidad de grupo a partir de la curva del índice de refracción n(ω) o más directamente a partir del número de onda k = ωn/c donde ω es la frecuencia en radianes ω=2πf. Mientras que una expresión de la velocidad de fase es vp=ω/k, la velocidad de grupo se puede expresar utilizando la derivada: vg=dω/dk. O en función de la velocidad de fase vp,  Cuando hay dispersión, la velocidad de grupo no sólo no es igual a la velocidad de fase, sino que generalmente varía con la longitud de onda. Esto se conoce como dispersión de la velocidad de grupo y hace que un pulso corto de luz se ensanche, ya que los diferentes componentes de frecuencia dentro del pulso viajan a diferentes velocidades. La dispersión de la velocidad de grupo se cuantifica como la derivada del recíproco de la velocidad de grupo con respecto a la frecuencia del radián, lo que resulta en la dispersión de la velocidad de grupo = d2k/dω2.  Si un pulso de luz se propaga a través de un material con dispersión de velocidad de grupo positiva, los componentes de menor longitud de onda viajan más lentamente que los componentes de mayor longitud de onda. Por lo tanto, el pulso se convierte en un ""chirrido positivo"", o ""chirrido ascendente"", aumentando la frecuencia con el tiempo. Por otro lado, si un pulso viaja a través de un material con dispersión de velocidad de grupo negativa, los componentes de longitud de onda más cortos viajan más rápido que los más largos, y el pulso se vuelve negativamente chirpado, o down-chirped, disminuyendo en frecuencia con el tiempo.  Un ejemplo cotidiano de señal con chirrido negativo en el ámbito acústico es el de un tren que se aproxima y choca con las deformaciones de una vía soldada. El sonido provocado por el propio tren es impulsivo y viaja mucho más rápido en las vías metálicas que en el aire, por lo que el tren se oye mucho antes de llegar. Sin embargo, desde lejos no se oye como si provocara impulsos, sino que da lugar a un distintivo chirrido descendente, en medio de la reverberación causada por la complejidad de los modos vibratorios de la vía. La dispersión de la velocidad del grupo se percibe en que el volumen de los sonidos permanece audible durante un tiempo sorprendentemente largo, hasta varios segundos.  El parámetro de dispersión de la velocidad de grupo:  a menudo se utiliza para cuantificar GVD, que es proporcional a D a través de un factor negativo:  Según algunos autores se dice que un medio tiene una dispersión normal/dispersión anormal para determinadas longitudes de onda en vacío λ0 si la segunda derivada del índice de refracción calculada en λ0 es positiva/negativa o, de manera equivalente, si D(λ0) es negativo/positivo.[2] Esta definición se refiere a la dispersión de la velocidad de grupo y no debe confundirse con la dada en la sección anterior. Las dos definiciones no coinciden en general, por lo que el lector debe entender el contexto.  Formulación generalizada de los altos órdenes de dispersión - Óptica de Lah-Laguerre La descripción de la dispersión cromática de forma perturbativa mediante coeficientes de Taylor es ventajosa para los problemas de optimización en los que es necesario equilibrar la dispersión de varios sistemas diferentes. Por ejemplo, en los amplificadores láser de pulsos chirp, los pulsos se estiran primero en el tiempo mediante un estirador para evitar daños ópticos. Después, en el proceso de amplificación, los pulsos acumulan inevitablemente fase lineal y no lineal al pasar por los materiales esto desenfoca el punto radial del espejo por eso se ve así con esos colores. Y por último, los pulsos se comprimen en varios tipos de compresores. Para cancelar cualquier orden residual superior en la fase acumulada, normalmente se miden y equilibran los órdenes individuales. Sin embargo, para los sistemas uniformes, esta descripción perturbadora no suele ser necesaria (por ejemplo, la propagación en guías de onda).  Los órdenes de dispersión se han generalizado de una manera computacionalmente amigable, en forma de transformadas de tipo Lah-Laguerre."
fisica,"La fuerza de fricción  es la fuerza que existe entre dos superficies en contacto, que se opone al deslizamiento (fuerza de fricción estática y cinética). Se genera debido a las imperfecciones, que en mayor parte son microscópicas, entre las superficies en contacto,[cita requerida] aunque también existen fenómenos de interacción electrostática entre superficies. Estas imperfecciones hacen que la fuerza perpendicular R entre ambas superficies no lo sea perfectamente, sino que forme un ángulo  con la normal N (el ángulo de rozamiento). Por tanto, la fuerza resultante se compone de la fuerza normal N (perpendicular a las superficies en contacto) y de la fuerza de rozamiento F, paralela a las superficies en contacto.  Rozamiento entre superficies de dos sólidos En el rozamiento entre dos cuerpos se ha observado los siguientes hechos:  El rozamiento puede variar en una medida mucho menor debido a otros factores:  Algunos autores sintetizan las leyes del comportamiento de la fricción en los siguientes dos postulados básicos:[1]​  La segunda ley puede ilustrarse arrastrando un bloque sobre una superficie plana. La fuerza de arrastre será la misma aunque el bloque descanse sobre la cara ancha o sobre un borde más angosto. Estas leyes fueron establecidas primeramente por Leonardo da Vinci al final del siglo XV, olvidándose después durante largo tiempo; posteriormente fueron redescubiertas por el ingeniero francés Amontons en 1699. Frecuentemente se les denomina también leyes de Amontons.  Tipos de fricción Existen dos tipos de rozamiento o fricción, la fricción estática (Fe) y la fricción dinámica (Fd). El primero es la resistencia que se debe superar para poner en movimiento un cuerpo con respecto a otro que se encuentra en contacto. El segundo, es la resistencia, de magnitud considerada constante, que se opone al movimiento pero una vez que este ya comenzó. En resumen, lo que diferencia a un roce con el otro, es que el estático actúa cuando los cuerpos están en reposo relativo en tanto que el dinámico lo hace cuando ya están en movimiento.  La fuerza de fricción estática, que depende de la magnitud de las fuerzas tangenciales que se apliquen, es siempre menor o igual al coeficiente de rozamiento entre los dos objetos (número medido empíricamente y que se encuentra tabulado) multiplicado por la fuerza normal. La fuerza cinética, en cambio, es igual al coeficiente de rozamiento dinámico, denotado por la letra griega     μ    {\displaystyle \mu \,}  , por la normal en todo instante.  No se tiene una idea perfectamente clara de la diferencia entre el rozamiento dinámico y el estático, pero se tiende a pensar que el estático es algo mayor que el dinámico, porque al permanecer en reposo ambas superficies pueden aparecer enlaces iónicos, o incluso microsoldaduras entre las superficies, factores que desaparecen en estado de movimiento. Este fenómeno es tanto mayor cuanto más perfectas son las superficies. Un caso más o menos común es el del gripaje de un motor por estar mucho tiempo parado (no solo se arruina por una temperatura muy elevada), ya que las superficies del pistón y la camisa, al permanecer en contacto y reposo durante largo tiempo, pueden llegar a soldarse entre sí.  Un ejemplo bastante común de fricción dinámica es la ocurrida entre los neumáticos de un auto y el pavimento en un frenado abrupto.  Como comprobación de lo anterior, se realiza el siguiente ensayo, sobre una superficie horizontal se coloca un cuerpo, y le aplica una fuerza horizontal F , muy pequeña en un principio, se puede ver que el cuerpo no se desplaza, la fuerza de rozamiento iguala a la fuerza aplicada y el cuerpo permanece en reposo, en la gráfica se representa en el eje horizontal la fuerza F aplicada, y en el eje vertical la fuerza de rozamiento Fr.  Entre los puntos O y A, ambas fuerzas son iguales y el cuerpo permanece estático; al sobrepasar el punto A el cuerpo súbitamente se comienza a desplazar, la fuerza ejercida en A es la máxima que el cuerpo puede soportar sin deslizarse, se denomina Fe o fuerza estática de fricción; la fuerza necesaria para mantener el cuerpo en movimiento una vez iniciado el desplazamiento es Fd o fuerza dinámica, es menor que la que fue necesaria para iniciarlo (Fe). La fuerza dinámica permanece constante.  Si la fuerza de rozamiento Fr es proporcional a la normal N, y a la constante de proporcionalidad se la llama     μ    {\displaystyle \mu \,}  :       F  r   = μ N    {\displaystyle F_{r}=\mu N\,}    Y permaneciendo la fuerza normal constante, se puede calcular dos coeficientes de rozamiento: el estático y el dinámico como:       μ  e   =    F  e   N   ,   μ  d   =    F  d   N     {\displaystyle \mu _{e}={\frac {F_{e}}{N}},\qquad \mu _{d}={\frac {F_{d}}{N}}}    donde el coeficiente de rozamiento estático      μ  e      {\displaystyle \mu _{e}\,}   corresponde al de la mayor fuerza que el cuerpo puede soportar inmediatamente antes de iniciar el movimiento y el coeficiente de rozamiento dinámico      μ  d      {\displaystyle \mu _{d}\,}   corresponde a la fuerza necesaria para mantener el cuerpo en movimiento una vez iniciado.  Es la fuerza que se opone al inicio del deslizamiento sobre un cuerpo en reposo, al que se aplica una fuerza F, intervienen cuatro fuerzas:  Si suponemos que la fuerza tractriz forma un ángulo     α   {\displaystyle \alpha }   con el plano tangente entre sólidos y el peso forma un ángulo     β   {\displaystyle \beta }   con la dirección perpendicular a dicho plano tenemos las relaciones, entre la fuerza tractriz aplicada, la fuerza de rozamiento y la fuerza normal de contacto:        {    P cos ⁡ β + F sin ⁡ α = N     F cos ⁡ β + P sin ⁡ α =  F  r           {\displaystyle {\begin{cases}P\cos \beta +F\sin \alpha =N\\F\cos \beta +P\sin \alpha =F_{r}\end{cases}}}    Se sabe que el peso del cuerpo P es el producto de su masa por la aceleración de la gravedad (g), y que la fuerza de rozamiento es el coeficiente estático por la normal:  Si     α = 0   {\displaystyle \alpha =0}   y     β = 0   {\displaystyle \beta =0}  , la fuerza tractiz aplicada F horizontal máxima que se puede aplicar a un cuerpo en reposo es igual al coeficiente de rozamiento estático por su masa y por la aceleración de la gravedad.  Dado un cuerpo en movimiento sobre una superficie horizontal, deben considerarse las siguientes fuerzas:  Como equilibrio dinámico, se puede establecer que:        {    P = N     F =  F  a   −  F  r           {\displaystyle {\begin{cases}P=N\\F=F_{a}-F_{r}\end{cases}}}    Sabiendo que:  prescindiendo de los signos para tener en cuenta solo las magnitudes, se puede reescribir la segunda ecuación de equilibrio dinámico como:        F   a   =  μ  d   m  g  + m  a  ,  ⇒   a  =     F   a   m   −  μ  d    g    {\displaystyle \mathbf {F} _{a}=\mu _{d}m\mathbf {g} +m\mathbf {a} ,\quad \Rightarrow \quad \mathbf {a} ={\frac {\mathbf {F} _{a}}{m}}-\mu _{d}\mathbf {g} }    Es decir, la fuerza de empuje aplicada sobre el cuerpo       F   a     {\displaystyle \mathbf {F} _{a}}   es igual a la fuerza resultante      F    {\displaystyle \mathbf {F} }   menos la fuerza de rozamiento       F   r     {\displaystyle \mathbf {F} _{r}}   que el cuerpo opone a ser acelerado. De esa misma expresión se deduce que la aceleración      a    {\displaystyle \mathbf {a} }   que sufre el cuerpo, al aplicarle una fuerza       F   a     {\displaystyle \mathbf {F} _{a}}   mayor que la fuerza de rozamiento       F   r     {\displaystyle \mathbf {F} _{r}}   con la superficie sobre la que se apoya.  Rozamiento en un plano inclinado Si sobre una línea horizontal r, se tiene un ángulo     α    {\displaystyle \alpha \,}  , y sobre este plano inclinado se coloca un cuerpo con rozamiento, se tendrán tres fuerzas que intervienen:  Si el cuerpo está en equilibrio, no se desliza, la suma vectorial de estas tres fuerzas es cero:       P  +   F   r   +  N  = 0   {\displaystyle \mathbf {P} +\mathbf {F} _{r}+\mathbf {N} =0}    Lo que gráficamente sería un triángulo cerrado formado por estas tres fuerzas, puestas una a continuación de otra, como se ve en la figura.  El peso puede descomponerse en una componente normal al plano Pn y una componentes tangente al plano Pt y la ecuación anterior puede escribirse componente a componentes simplemente como:  Dividiendo la primera componente entre la segunda se obtiene como resultado:  El coeficiente de rozamiento estático es igual a la tangente del ángulo del plano inclinado, en el que el cuerpo se mantiene en equilibrio sin deslizar, ello permite calcular los distintos coeficientes de rozamiento, simplemente colocando un cuerpo de un material concreto sobre un plano inclinado del material con el que se pretende calcular su coeficiente de rozamiento, inclinando el plano progresivamente se observa el momento en el que el cuerpo comienza a deslizarse, la tangente de este ángulo es el valor del coeficiente de rozamiento. Del mismo modo conocido el coeficiente de rozamiento entre dos materiales podemos saber el ángulo máximo de inclinación que puede soportar sin deslizar.  En el caso de rozamiento dinámico en un plano inclinado, se tiene un cuerpo que se desliza, y siendo que está en movimiento, el coeficiente que interviene es el dinámico      μ  d      {\displaystyle \mu _{d}\,}  , así como una fuerza de inercia Fi, que se opone al movimiento, el equilibrio de fuerzas se da cuando:       P  +   F   r   +  N  +   F   i   = 0   {\displaystyle \mathbf {P} +\mathbf {F} _{r}+\mathbf {N} +\mathbf {F} _{i}=0}    descomponiendo los vectores en sus componentes normales y tangenciales se tiene:        {     P  n   = N      P  t   −  F  r   =  F  i           {\displaystyle {\begin{cases}P_{n}=N\\P_{t}-F_{r}=F_{i}\end{cases}}}    teniendo en cuenta que:  y como en el caso de equilibrio estático, se tiene:  Con estas ecuaciones se determina las condiciones de equilibrio dinámico del cuerpo con fricción en un plano inclinado. Si el cuerpo se desliza sin aceleración (a velocidad constante) su fuerza de inercia Fi será cero, y se puede ver que:  esto es, de forma semejante al caso estático:  Con lo que se puede decir que el coeficiente de rozamiento dinámico      μ  d      {\displaystyle \mu _{d}\,}   de un cuerpo con la superficie de un plano inclinado, es igual a la tangente del ángulo del plano inclinado con el que el cuerpo se desliza sin aceleración, con velocidad constante, por el plano.  Valores de los coeficientes de fricción En la tabla se listan los coeficientes de rozamiento de algunas sustancias donde  Los coeficientes de rozamiento, por ser relaciones entre dos fuerzas son magnitudes adimensionales.  Rozamiento entre sólido y fluido La fricción aerodinámica depende del régimen o tipo de flujo que exista alrededor del cuerpo en movimiento:  Rozamiento con lubricación Una cuestión de interés práctico es un problema mixto donde pueden aparecer tanto fenómenos de rozamiento entre sólidos como entre fluido y sólido, dependiendo de la velocidad. Se trata del caso de dos superficies sólidas entre las cuales existe una fina capa de fluido. Stribeck[2]​  demostró que a muy bajas velocidades predomina un rozamiento como el que ocurre entre dos superficies secas, y a velocidades muy altas predomina un rozamiento hidrodinámico. La mínima fricción se alcanza para una velocidad intermedia dependiente de la presión del fluido, su ""viscosidad cinemática"".  Rozamiento en medios fluidos La viscosidad es una medida de la resistencia de un fluido que está siendo deformado por una presión, una tensión tangencial o una combinación de tensiones internas. En términos generales, es la resistencia de un líquido a fluir, comúnmente dicho, es su ""espesor"". Viscosidad describe la resistencia interna de un líquido a fluir y puede ser pensado como una medida de la fricción del fluido. Así, el agua es ""delgada"", ya que tiene baja viscosidad, mientras que el aceite vegetal es ""densa"", con una mayor viscosidad. Todos los fluidos reales (excepto los superfluidos) tienen cierta resistencia a la tensión. Un fluido que no tiene resistencia al esfuerzo cortante se conoce como un fluido ideal o líquido no viscoso.  Por ejemplo, un magma de alta viscosidad creará un volcán alto, porque no se puede propagar hacia abajo con suficiente rapidez; la lava de baja viscosidad va a crear un volcán en escudo, que es grande y ancho. El estudio de la viscosidad se conoce como reología.   El modelo más simple de fluido viscoso lo constituyen los fluidos newtonianos en los cuales el vector tensión, debido al rozamiento entre unas capas de fluido y otras, viene dado por:       τ  i j   = μ  (     ∂  u  i     ∂  x  j      +    ∂  u  j     ∂  x  i       )    {\displaystyle \tau _{ij}=\mu \left({\frac {\partial u_{i}}{\partial x_{j}}}+{\frac {\partial u_{j}}{\partial x_{i}}}\right)}    Donde:  Para un flujo unidimensional la anterior ecuación se reduce a la conocida expresión:      τ = μ    d u   d x      {\displaystyle \tau =\mu {\frac {du}{dx}}}    Véase también Referencias Enlaces externos"
fisica,"En física, la inercia (del latín inertĭa) es la propiedad que tienen los cuerpos de permanecer en su estado de reposo o movimientos relativos. Dicho, de forma general, es la resistencia que opone la materia a que se modifique su estado de movimiento, incluyendo cambios en la velocidad o en la dirección del movimiento. Como consecuencia, un cuerpo conserva su estado de reposo relativo o movimiento rectilíneo uniforme relativo si no hay una fuerza que, actuando sobre él, logre cambiar su estado de movimiento.  En la naturaleza no existe el reposo absoluto, siempre toda la materia está en movimiento, por eso cuando se habla de reposo o Movimiento Rectilíneo Uniforme (MRU) se debe añadir la palabra ""relativo"" (relativo a un sistema de referencia). El cuerpo está en reposo o en MRU solo con respecto de ese sistema de referencia. Cuando un cuerpo está en reposo relativo sobre la superficie de la Tierra, también está participando de los distintos movimientos que realiza el planeta si se considera un marco de referencia centrado en el Sol. Así mismo, está sometido a diferentes fuerzas como las gravitatorias de la Tierra, el Sol, La Luna y otros cuerpos, así como la resistencia mecánica que impide que se hunda en la tierra, o se deslice. Se puede decir que el cuerpo se encuentra en equilibrio sobre la superficie de la Tierra y, por lo tanto, en reposo relativo.  Podríamos decir que es la resistencia que opone un sistema de partículas a modificar su estado dinámico.  En física se dice que un sistema tiene más inercia cuando resulta más difícil lograr un cambio en el estado físico del mismo. Los dos usos más frecuentes en física son la inercia mecánica y la inercia térmica.  La primera de ellas aparece en mecánica y es una medida de dificultad para cambiar el estado de movimiento o reposo de un cuerpo. La inercia mecánica depende de la cantidad de masa y del tensor de inercia.  La inercia térmica mide la dificultad con la que un cuerpo cambia su temperatura al estar en contacto con otros cuerpos o ser calentado. La inercia térmica depende de la capacidad calorífica.  Las llamadas fuerzas de inercia son fuerzas ficticias o aparentes que un observador percibe en un sistema de referencia no-inercial.  Interpretaciones de la inercia Hay investigadores que consideran la inercia mecánica como manifestación de la masa, y están interesados en las ideas de la física de partículas sobre el bosón de Higgs. De acuerdo con el modelo estándar de la física de partículas, todas las partículas elementales carecen prácticamente de masa. Sus masas (y por lo tanto su inercia) provienen del Mecanismo de Higgs vía intercambio con un campo omnipresente de Higgs. Esto lleva a deducir la existencia de una partícula elemental, el bosón de Higgs.  Otros están inclinados a ver la inercia como una característica conectada con la masa, y trabajan a lo largo de otros caminos. El número de los investigadores que entregan nuevas ideas aquí es reducido. Muchas de las ideas presentadas al respecto todavía son miradas como protociencia, pero ilustra cómo está avanzando la formación de teorías en esta área.  Una publicación reciente del físico sueco-americano C. Johan Masreliez propone que el fenómeno de la inercia puede ser explicado, si los coeficientes métricos en la línea elemento de Minkowskian son cambiados como consecuencia de la aceleración. Cierto factor de posicionamiento modela la inercia como efecto de tipo gravitacional.[1]  En un artículo sucesivo para Physica Scripta, explica cómo la relatividad especial puede ser compatible con un cosmos con un marco cosmológico fijo y único de la referencia. La transformación de Lorentz modela la formación de la estructura (""“orphing""” de las partículas móviles, que pudieran preservar sus características cambiando sus geometrías del espacio-tiempo local. Con esto la geometría se convierte en dinámica y una parte integral de movimiento. Masreliez dice que es esta geometría la que cambia para ser la fuente de la inercia; ergo, para generar la fuerza de inercia.[2] Si fuera aceptada, la inercia podría conectar la relatividad especial con la relatividad general. Sin embargo, aunque los marcos inerciales siguen siendo físicamente equivalentes y las leyes de la física se aplican igualmente, no modelan el mismo espacio-tiempo. Estas nuevas ideas, SEC han sido comprobadas hasta ahora no solo por el proponente ,sino también por algunos miembros de la comunidad científica.[3] La teoría de la SEC es controvertida, ya que refuta la hipótesis del Big Bang  Otro acercamiento ha sido sugerido por Emil Marinchev (2002).[4]  Historia y desarrollo del concepto de inercia Primeros conocimientos sobre el movimiento inercial John H. Lienhard señala a Mozi -basado en un texto chino del periodo de los Reinos Combatientes (475-221 a. C.- como el autor de la primera descripción de la inercia.[5] Antes del Renacimiento europeo, la teoría del movimiento predominante en la filosofía occidental era la de Aristóteles (335 a. C.-322 a. C.). En la superficie de la Tierra, la propiedad de inercia de los objetos físicos suele quedar enmascarada por la gravedad y los efectos de la fricción y la resistencia del aire, que tienden a disminuir la velocidad de los objetos en movimiento (comúnmente hasta el punto de reposo). Esto indujo al filósofo Aristóteles a creer que los objetos se moverían sólo mientras se les aplicara una fuerza.[6][7] Aristóteles afirmó que todos los objetos en movimiento (en la Tierra) acababan por detenerse a menos que una fuerza externa continuara moviéndolos.[8] Aristóteles explicó el movimiento continuado de los proyectiles, tras ser separados de su proyector, como una acción (en sí misma inexplicable) del medio circundante que continuaba moviendo el proyectil.[9]  A pesar de su aceptación general, el concepto de movimiento de Aristóteles[10] fue discutido en varias ocasiones por filósofos notables a lo largo de casi dos milenios. Por ejemplo, Lucrecio (siguiendo, presumiblemente, a Epicuro) afirmó que el ""estado por defecto"" de la materia era el movimiento, no la estasis (estancamiento).[11] En el siglo VI, Juan Filopon criticó la incoherencia entre la discusión de Aristóteles sobre los proyectiles, donde el medio mantiene a los proyectiles en movimiento, y su discusión sobre el vacío, donde el medio impediría el movimiento de un cuerpo. Filopón propuso que el movimiento no se mantenía por la acción de un medio circundante, sino por alguna propiedad impartida al objeto cuando se ponía en movimiento. Aunque no se trataba del concepto moderno de inercia, ya que seguía siendo necesario un poder para mantener un cuerpo en movimiento, resultó ser un paso fundamental en esa dirección.[12][13] Averroes y muchos filósofos escolásticos que apoyaban a Aristóteles se opusieron firmemente a este punto de vista. Sin embargo, este punto de vista no quedó sin respuesta en el mundo islámico, donde Filopón tuvo varios partidarios que desarrollaron sus ideas.  En el siglo XI, el polímata persa Ibn Sina (Avicena) afirmó que un proyectil en el vacío no se detendría a menos que se actuara sobre él.[14]  Teoría del ímpetu En el siglo XIV, Jean Buridan rechazó la noción de que una propiedad generadora de movimiento, que él denominó ímpetu, se disipara espontáneamente. La posición de Buridan era que un objeto en movimiento sería detenido por la resistencia del aire y el peso del cuerpo, que se opondrían a su ímpetu.[15] Buridan también sostenía que el ímpetu aumentaba con la velocidad; por tanto, su idea inicial de ímpetu era similar en muchos aspectos al concepto moderno de impulso. A pesar de las obvias similitudes con las ideas más modernas de inercia, Buridán consideraba su teoría sólo como una modificación de la filosofía básica de Aristóteles, manteniendo muchos otros puntos de vista peripatéticos, incluida la creencia de que seguía existiendo una diferencia fundamental entre un objeto en movimiento y un objeto en reposo. Buridán también creía que el ímpetu podía ser no sólo lineal, sino también circular por naturaleza, haciendo que los objetos (como los cuerpos celestes) se movieran en círculo. La teoría de Buridán fue seguida por su discípulo Alberto de Sajonia (1316-1390) y los Calculadores de Oxford, que realizaron varios experimentos que socavaron aún más el modelo aristotélico. Nicole Oresme, pionera en la práctica de ilustrar las leyes del movimiento con gráficos, desarrolló su trabajo.  Poco antes de la teoría de la inercia de Galileo, Giambattista Benedetti modificó la creciente teoría del ímpetu para implicar únicamente el movimiento lineal:  Benedetti cita el movimiento de una piedra en una honda como ejemplo del movimiento lineal inherente a los objetos, forzados a un movimiento circular.  Inercia clásica Según Charles Coulston Gillispie, la inercia ""entró en la ciencia como una consecuencia física de la geometrización del espacio-materia de Descartes, combinada con la inmutabilidad de Dios"".[17] El primer físico que rompió completamente con el modelo aristotélico del movimiento fue Isaac Beeckman en 1614.[18] El término ""inercia"" fue introducido por primera vez por Johannes Kepler en su Epitome Astronomiae Copernicanae[19] (publicado en tres partes de 1617 a 1621); sin embargo, el significado del término de Kepler (que derivó de la palabra latina para ""ociosidad"" o ""pereza"") no era exactamente el mismo que su interpretación moderna. Kepler definió la inercia sólo en términos de resistencia al movimiento, basándose una vez más en la presunción de que el reposo era un estado natural que no necesitaba explicación. No fue hasta los trabajos posteriores de Galileo y Newton, que unificaron el reposo y el movimiento en un principio, cuando el término ""inercia"" pudo aplicarse a estos conceptos tal y como se hace hoy.[20] El principio de inercia, tal y como lo formuló Aristóteles para los ""movimientos en el vacío"",[21] incluye que un objeto mundano tiende a resistirse a un cambio de movimiento. La división aristotélica del movimiento en mundano y celeste se hizo cada vez más problemática ante las conclusiones de Nicolás Copérnico en el siglo XVI, quien sostenía que la Tierra nunca está en reposo, sino en movimiento constante alrededor del Sol.[22]  Galileo, en su posterior desarrollo del modelo copernicano, reconoció estos problemas con la naturaleza entonces aceptada del movimiento y, al menos parcialmente, como resultado, incluyó una reformulación de la descripción de Aristóteles del movimiento en el vacío como principio físico básico:   Galileo escribe que ""eliminados todos los impedimentos externos, un cuerpo pesado sobre una superficie esférica concéntrica con la Tierra se mantendrá en el estado en que se encontraba; si se le pone en movimiento hacia el oeste (por ejemplo), se mantendrá en ese movimiento"".[23]  Esta noción, denominada ""inercia circular"" o ""inercia circular horizontal"" por los historiadores de la ciencia, es precursora pero distinta de la noción de inercia rectilínea de Newton.[24][25] Para Galileo, un movimiento es ""horizontal"" si no lleva al cuerpo en movimiento hacia el centro de la Tierra ni lo aleja de él, y para él, ""un barco, por ejemplo, habiendo recibido una vez cierto impulso a través del tranquilo mar, se movería continuamente alrededor de nuestro globo sin detenerse jamás"".[26] [27] También hay que señalar que Galileo llegó más tarde (en 1632) a la conclusión de que, basándose en esta premisa inicial de la inercia, es imposible distinguir entre un objeto en movimiento y uno inmóvil sin alguna referencia exterior con la que compararlo (sistema marco de referencia).[28] Esta observación acabó siendo la base para que Albert Einstein desarrollara la teoría de la relatividad especial.  Los conceptos de inercia de los escritos de Galileo serían refinados, modificados y codificados más tarde por Isaac Newton como la primera de sus Leyes del Movimiento (publicada por primera vez en la obra de Newton, Philosophiæ Naturalis Principia Mathematica, en 1687):  A pesar de haber definido el concepto con tanta elegancia en sus leyes del movimiento, Newton no utilizó realmente el término ""inercia"" para referirse a su Primera Ley. De hecho, originalmente consideraba que el fenómeno respectivo estaba causado por ""fuerzas innatas"" inherentes a la materia, que se resistían a cualquier aceleración. Desde esta perspectiva, y tomando prestado de Kepler, Newton atribuyó al término ""inercia"" el significado de ""la fuerza innata que posee un objeto y que se resiste a los cambios de movimiento""; de este modo, Newton definió la ""inercia"" "
fisica,"La energía potencial es la energía mecánica asociada a la localización de un cuerpo dentro de un campo de fuerzas (e.g. gravitatorio, electrostático, etc.) o a la existencia de un campo de fuerza en el interior de un cuerpo (energía elástica). La energía potencial de un cuerpo es una consecuencia de que el sistema de fuerzas que actúa sobre el mismo sea conservativo.  Independientemente de la fuerza que la origine, la energía potencial que posee el sistema físico representa la energía ""almacenada"" en virtud de su posición y/o configuración, por contraposición con la energía cinética que tiene y que representa su energía debido al movimiento. Para un sistema conservativo, la suma de energía cinética y potencial es constante, eso justifica el nombre de fuerzas conservativas, es decir, aquellas que hacen que la energía ""se conserve"". El concepto de energía potencial también puede usarse para sistemas físicos en los que intervienen fuerzas disipativas, y que por tanto no conservan la energía, solo que en ese caso la energía mecánica total no será constante, y para aplicar el principio de conservación de la energía es necesario contabilizar la disipación de energía.[1]  El valor de la energía potencial depende siempre del punto o configuración de referencia escogido para medirla, por esa razón se dice a veces que físicamente solo importa la variación de energía potencial entre dos configuraciones.[2].  La energía potencial interviene como se ha mencionado en el principio de conservación de la energía y su campo de aplicación es muy general. Está presente no solo en la física clásica, sino también de la física relativista y física cuántica. El concepto se ha generalizado también a la física de partículas, donde se han llegado a utilizar potenciales complejos con el objeto de incluir también la energía disipada por el sistema.[3]  Introducción Si bien la energía cinética (     E  c     {\displaystyle E_{c}}  ) de un cuerpo es una propiedad física que depende de su movimiento, la energía potencial (     E  p     {\displaystyle E_{p}}  ), en cambio, es un concepto de energía que va a depender del tipo de interacción que se ejerce sobre el cuerpo, de su posición y de la configuración en el espacio del citado cuerpo o cuerpos sobre los que se aplica. Así en una situación ideal en la que los objetos que constituyen el sistema físico en estudio estén ausentes de fricción, entonces la suma de ambas energías, cinética y potencial, va a representar la energía total del sistema,     E   {\displaystyle E}  , y se va a conservar, independientemente de la posición o posiciones que vaya ocupando el sistema en el tiempo.[4]  La noción de energía potencial se relaciona con el trabajo realizado por las fuerzas sobre el sistema físico para trasladarlo de una posición a otra del espacio. La función energía potencial dependerá de forma importante del tipo de campo de fuerzas o interacción que actúe sobre el sistema. Por ejemplo, la fuerza de gravitación, la electromagnética, responsable de las interacciones eléctrica y magnética, o la elástica (derivada de la electromagnética). Si el trabajo no depende del camino seguido, entonces a la fuerza       F →     {\displaystyle {\overrightarrow {F}}}   se le llama conservativa y el trabajo     W   {\displaystyle W}   expresa la diferencia de energía potencial      E   p  A     −  E   p  B       {\displaystyle E_{p_{A}}-E_{p_{B}}}   del sistema entre la posición de partida (A) y la posición de llegada (B).[5]  También se utiliza la función potencial     V   {\displaystyle V}   en lugar de la energía potencial      E  p     {\displaystyle E_{p}}   para representar el trabajo realizado por la unidad básica de la interacción. Si, por ejemplo, la interacción es la gravitatoria, sería la unidad de masa y en el caso de la interacción eléctrica, la unidad de carga.  La función energía potencial y, en especial, la función potencial, tienen gran interés en la física no solo cuando se aplican a las interacciones que son importantes a nuestra escala, como son la gravitatoria, la electromagnética y la elástica (derivada de la electromagnética), sino también cuando se estudia cualquier tipo de fuerza o interacción, incluso en la física cuántica al tratar de resolver la dinámica de un sistema físico mediante la ecuación de Schrödinger.[6] Se aplica, por ejemplo, a la física atómica en la obtención de los estados electrónicos del átomo o en la física molecular, para la obtención de los estados electrónicos, de vibración, de vibración-rotación y de rotación de la molécula, así como en la física del estado sólido. También se aplica en la física nuclear.[7]  En otras formulaciones más generales de la física, la función potencial juega, así mismo, un papel importante. Entre ellas, las formulaciones lagrangiana y hamiltoniana de la mecánica.[8]  Energía potencial gravitatoria La energía potencial gravitatoria se define como la energía que poseen los cuerpos por el hecho de poseer masa y estar situados a una determinada distancia mutua. Entre las masas de grandes magnitudes se ejercen fuerzas de atracción, de mayor intensidad cuanto mayores son estas. Aplicado, por ejemplo, al movimiento planetario, la masa mayor es la del sol que crea un campo de fuerzas gravitatorio que actúa sobre las masas menores de los planetas. A su vez, cada planeta crea un campo de fuerzas gravitatorio que actúa sobre las masas menores que estén próximas al planeta, los satélites.[9]  El trabajo realizado para llevar una masa de prueba m en presencia de otra masa M, fuente del campo gravitatorio, desde un punto A a otro B, es la diferencia de la energía potencial de la masa m en el punto de partida A menos la energía potencial en el punto de llegada B. El citado trabajo no depende del camino seguido sino tan solo de los puntos inicial y final. Al gozar de esta propiedad la fuerza gravitatoria y el campo gravitatorio (la fuerza gravitatoria sobre la unidad de masa), al campo se le llama campo conservativo y tiene pleno sentido obtener el potencial gravitatorio, derivado del campo creado por la masa M, así como la energía potencial gravitatoria derivada de la fuerza gravitatoria entre las masas m y M.  Si se considera una masa M en el origen del sistema de coordenadas como fuente del campo gravitatorio y se elige como referencia el infinito, punto en el que cualquier masa m tiene una energía potencial nula, la energía potencial es el trabajo necesario para llevar la masa m desde el infinito hasta un determinado punto A definido por la coordenada     r   {\displaystyle r}   (la distancia del punto A al origen de coordenadas).[9]      W =  E   p  A     −  E   p  ∞     =   {\displaystyle W=E_{p_{A}}-E_{p_{\infty }}=}       Δ  E  p   = −  ∫  ∞   A    F  g   ⋅  d r = −  ∫  ∞   A      G ⋅ M ⋅ m   r  2     ⋅  d r = G ⋅ M ⋅ m ⋅   [   1 r   ]   ∞   A   =    G ⋅ M ⋅ m   r  A     =  E   p  A       [ 1 ]   {\displaystyle \Delta E_{p}=-\int _{\infty }^{A}F_{g}\cdot \,dr=-\int _{\infty }^{A}{\frac {G\cdot M\cdot m}{r^{2}}}\cdot \,dr=G\cdot M\cdot m\cdot \left\lbrack {\frac {1}{r}}\right\rbrack _{\infty }^{A}={\frac {G\cdot M\cdot m}{r_{A}}}=E_{p_{A}}\qquad \qquad [1]}    Donde:      E  p     {\displaystyle E_{p}}   es la energía potencial gravitatoria de la masa     m   {\displaystyle m}  , cuyo valor depende de la distancia     r   {\displaystyle r}   entre la masa de prueba     m   {\displaystyle m}   y la masa     M   {\displaystyle M}   que genera el campo gravitatorio, y se mide en julios (    J   {\displaystyle J}  ). Por otro lado,       F  g   =    G ⋅ M ⋅ m   r  2       {\displaystyle F_{g}={\frac {G\cdot M\cdot m}{r^{2}}}}   es la fuerza gravitatoria sobre la masa de prueba     m   {\displaystyle m}   situada a una distancia     r   {\displaystyle r}   de la masa     M   {\displaystyle M}   que crea el campo gravitatorio y se mide en newtons (    N   {\displaystyle N}  ). Además,     G   {\displaystyle G}   es la constante de gravitación universal, cuyo valor es     G = 6.673 ×  10  − 11     (          N  ⋅   m   2              kg   2        )    {\displaystyle G=6.673\times 10^{-11}\;\left({\cfrac {{\text{N}}\cdot {\text{m}}^{2}}{{\text{kg}}^{2}}}\right)}  . Finalmente,     M   {\displaystyle M}   y     m   {\displaystyle m}   se miden en kilogramos (    k g   {\displaystyle kg}  )      R   {\displaystyle R}   es la distancia que separa las dos masas, medida en metros (    m   {\displaystyle m}  )   La ecuación [1] que representa la energía potencial      E   p  A       {\displaystyle E_{p_{A}}}   de las masas m y M cuando están separadas una distancia      r  A     {\displaystyle r_{A}}  , es aplicable tanto a masas puntuales como a masas con simetría esférica, siendo la distancia entre ellas, la que hay entre los centros de dichas esferas.  La energía potencial cerca de la superficie de la Tierra La energía potencial que posee una masa     m   {\displaystyle m}   situada a una altura     h   {\displaystyle h}   sobre la superficie terrestre vale:  Esta expresión es un caso particular de la ecuación anterior [1]. Dicho caso se presenta cuando la masa se encuentra a una altura pequeña sobre la superficie de la tierra. Para demostrarlo, basta con aplicar la expresión [1] y considerar la variación de energía potencial entre las alturas sobre la superficie de la tierra,       h  1     {\displaystyle h_{1}}   y      h  2   , (  h  1   ≪  R  t   ,  h  2   ≪  R  t     {\displaystyle h_{2},(h_{1}\ll R_{t},h_{2}\ll R_{t}}   y      h  2   >  h  1   )   {\displaystyle h_{2}>h_{1})}   siendo      R  t     {\displaystyle R_{t}}    el radio de la tierra.[10]  En este caso, los productos      R  t   ⋅  h  2   ,  R  t   ⋅  h  1     {\displaystyle R_{t}\cdot h_{2},R_{t}\cdot h_{1}}   y      h  1   ⋅  h  2     {\displaystyle h_{1}\cdot h_{2}}   son muy pequeños comparados con      R  t   2     {\displaystyle R_{t}^{2}}   y, por lo tanto, se pueden despreciar en la ecuación [3].  Llamando     g = G ⋅ M ⋅   1  R  t   2       {\displaystyle g=G\cdot M\cdot {\frac {1}{R_{t}^{2}}}}    Si se toma      h  1     {\displaystyle h_{1}}   como el origen de energías potenciales, por ejemplo, al nivel del mar y llamando      h  2   = h   {\displaystyle h_{2}=h}  :      Δ  E  p   = m ⋅ g ⋅ h   {\displaystyle \Delta E_{p}=m\cdot g\cdot h}    Del desarrollo anterior se deduce que para      h ≪  R  t     {\displaystyle h\ll R_{t}}     la aproximación última es adecuada.  Velocidad de escape La velocidad de escape es la velocidad mínima necesaria para que un cuerpo de masa     m   {\displaystyle m}   salga fuera de la atracción gravitatoria.[11]   La fuerza de gravitación es conservativa. La energía potencial      E  p   ( r )   Ejemplos de la energía potencial gravitatoria El dibujo de una montaña rusa en un plano se puede inter "
fisica,"El efecto Josephson es un efecto físico que se manifiesta por la aparición de una corriente eléctrica por efecto túnel entre dos superconductores separados. El físico británico Brian David Josephson predijo tal efecto en 1962[1] Un año más tarde, las uniones Josephson fueron construidas por primera vez por Anderson y Rowell.[2] Estos trabajos le valieron a Josephson el premio Nobel de física en 1973 (junto con Leo Esaki e Ivar Giaever).  Descripción Según la teoría BCS, la corriente eléctrica en los superconductores no la transportan electrones simples como sería el caso normal, sino pares de electrones, los llamados pares de Cooper.  Cuando los dos superconductores están separados por una capa de un medio aislante o un metal no superconductor de unos pocos nanómetros, los pares de Cooper pueden atravesar la barrera por efecto túnel, un efecto característico de la mecánica cuántica. Aunque los pares de Cooper no pueden existir en un aislante o un metal no superconductor, cuando la capa que separa los dos superconductores es lo suficientemente estrecha, estos la pueden atravesar y guardar su coherencia de fase. Es la persistencia de esta coherencia de fase lo que da lugar al efecto Josephson.  Las ecuaciones básicas[3] que gobiernan la dinámica del efecto Josephson son:      U ( t ) =   ℏ  2 e       ∂ ϕ   ∂ t      {\displaystyle U(t)={\frac {\hbar }{2e}}{\frac {\partial \phi }{\partial t}}}     (ecuación de la evolución de fase superconductora)            I ( t ) =  I  c   sin ⁡ ( ϕ ( t ) )   {\displaystyle {\frac {}{}}I(t)=I_{c}\sin(\phi (t))}     (relación de Josephson o de enlace débil corriente-fase)  donde     U ( t   {\displaystyle U(t}  ) e     I ( t )   {\displaystyle I(t)}   son los voltajes y la corriente a través de la unión de Josephson,     ϕ ( t )   {\displaystyle \phi (t)}   es la diferencia de fase entre las funciones de onda en los dos superconductores que forman la unión, e      I  c     {\displaystyle I_{c}}   es una constante, la corriente crítica de la unión. La corriente crítica es un parámetro experimental importante del dispositivo que puede alterarse tanto por la temperatura como por un campo magnético aplicado. La constante física,           h  2 e          {\displaystyle {\begin{matrix}{\frac {h}{2e}}\end{matrix}}}   es el cuanto de flujo magnético, la inversa del cual es la constante de Josephson.  Se distinguen dos tipos de efecto Josephson, el efecto Josephson continuo (D.C. Josephson effect en inglés) y el efecto Josephson alterno (A.C. Josephson effect en inglés).  Efecto Josephson alterno Con un voltaje fijo      U  D C     {\displaystyle U_{DC}}   entre las uniones, la fase variará linealmente con el tiempo y la corriente será una corriente alterna con una amplitud de      I  c     {\displaystyle I_{c}}   y una frecuencia de            2 e  h         {\displaystyle {\begin{matrix}{\frac {2e}{h}}\end{matrix}}}       U  D C     {\displaystyle U_{DC}}  . Esto significa que la unión de Josephson puede funcionar como un convertidor voltaje-frecuencias perfecto.  La corriente a través de la barrera separando los superconductores es:       I  s   =  I  c   sin ⁡ (  ϕ  1   −  ϕ  2   )   {\displaystyle I_{s}=I_{c}\sin(\phi _{1}-\phi _{2})}    donde      I  c     {\displaystyle I_{c}}   es una corriente característica de la unión y      ϕ  1 , 2     {\displaystyle \phi _{1,2}}   son las fases superconductoras de los dos superconductores.  Por otra parte, la fase superconductora está conjugada canónicamente con el número de partículas, y obedece a la ecuación de movimiento:      ℏ    d (  ϕ  1   −  ϕ  2   )   d t    = 2 e (  V  1   −  V  2   )   {\displaystyle \hbar {\frac {d(\phi _{1}-\phi _{2})}{dt}}=2e(V_{1}-V_{2})}    donde     e   {\displaystyle e}   es la carga del electrón, y      V  1   −  V  2     {\displaystyle V_{1}-V_{2}}   es la diferencia de potencial existente entre los dos superconductores.  Por ello resulta que:      I ( t ) =  I  c   sin ⁡  (     2 e  ℏ   (  V  1   −  V  2   ) t +  φ  0    )    {\displaystyle I(t)=I_{c}\sin \left({\frac {2e}{\hbar }}(V_{1}-V_{2})t+\varphi _{0}\right)}    O dicho de otra forma, la aplicación de una diferencia de potencial conlleva las oscilaciones de la corriente superconductora a una frecuencia de        2 e  h   (  V  1   −  V  2   )   {\displaystyle {\frac {2e}{h}}(V_{1}-V_{2})}  . El efecto Josephson alterno es una forma de medir la relación     e  /  h   {\displaystyle e/h}  .  Efecto Josephson continuo El efecto Josephson continuo se refiere al fenómeno de una corriente continua que atraviesa el aislante en ausencia de un campo electromagnético externo. Se obtiene al aplicar un campo magnético a una unión de Josephson. El campo magnético produce un desfase entre los pares de Cooper que atraviesan la unión de forma análoga al efecto Aharonov-Bohm. Este desfase puede producir interferencias destructivas entre los pares de Cooper, lo que constituye una reducción de la corriente máxima que puede atravesar la unión. Si     Φ   {\displaystyle \Phi }   es el flujo magnético a través de la unión, se tiene la relación:       I  s   m a x   =  I  c      "
fisica,"Un campo magnético es una descripción matemática de la influencia magnética de las corrientes eléctricas y de los materiales magnéticos.[1] El campo magnético en cualquier punto está especificado por dos valores, la dirección y la magnitud; de tal forma que es un campo vectorial. Específicamente, el campo magnético es un vector axial, como lo son los momentos mecánicos y los campos rotacionales. El campo magnético es más comúnmente definido en términos de la fuerza de Lorentz ejercida en cargas eléctricas.[2]: ch1 [3]   El término se usa para dos campos distintos pero estrechamente relacionados, indicados por los símbolos B y H, donde, en el Sistema Internacional de Unidades, H se mide en unidades de amperios por metro y B se mide en teslas o newtons entre metro por amperio. En un vacío, H y B son lo mismo aparte de las unidades; pero en un material con magnetización (denotado por el símbolo M), B es solenoidal (no tiene divergencia en su dependencia espacial) mientras que H es no rotacional (libre de ondulaciones).   Los campos magnéticos se producen por cualquier carga eléctrica producida por los electrones en movimiento y el momento magnético intrínseco de las partículas elementales asociadas con una propiedad cuántica fundamental, su espín. En la relatividad especial, campos eléctricos y magnéticos son dos aspectos interrelacionados de un objeto, llamado el tensor electromagnético. Las fuerzas magnéticas dan información sobre la carga que lleva un material a través del efecto Hall. La interacción de los campos magnéticos en dispositivos eléctricos tales como transformadores es estudiada en la disciplina de circuitos magnéticos.  Los campos magnéticos se utilizan en toda la tecnología moderna, especialmente en ingeniería eléctrica y electromecánica. Los campos magnéticos giratorios se utilizan tanto en los motores eléctricos como en los generadores. La interacción de los campos magnéticos en dispositivos eléctricos como los transformadores se conceptualiza e investiga como circuito magnético. Las fuerzas magnéticas dan información sobre los portadores de carga en un material a través del efecto Hall. La Tierra produce su propio campo magnético, que protege la capa de ozono de la Tierra del viento solar y es importante en la navegación mediante una brújula.  Fuerza de Lorentz Entre las definiciones de campo magnético se encuentra la dada por la fuerza de Lorentz. Esto sería el efecto generado por una corriente eléctrica o un imán, sobre una región del espacio en la que una carga eléctrica puntual de valor (q), que se desplaza a una velocidad      ( v )    {\displaystyle \mathbf {(v)} }  , experimenta los efectos de una fuerza que es secante y proporcional tanto a la velocidad (v) como al campo (B). Así, dicha carga percibirá una fuerza descrita con la siguiente ecuación:       F  = q (  v  ×  B  )   {\displaystyle \mathbf {F} =q(\mathbf {v} \times \mathbf {B} )}    Donde F es la fuerza magnética, v es la velocidad y B el campo magnético, también llamado inducción magnética y densidad de flujo magnético. (Nótese que tanto F como v y B son magnitudes vectoriales y el producto vectorial tiene como resultante un vector perpendicular tanto a v como a B). El módulo de la fuerza resultante será:       |   F   |  =  |  q  |   |   v   |   |   B   |  ⋅  sen  ⁡ ( θ )   {\displaystyle |\mathbf {F} |=|q||\mathbf {v} ||\mathbf {B} |\cdot \mathop {\operatorname {sen} } (\theta )}    La existencia de un campo magnético se pone de manifiesto gracias a la propiedad de orientar un magnetómetro (laminilla de acero imantado que puede girar libremente). La aguja de una brújula, que evidencia la existencia del campo magnético terrestre, puede ser considerada un magnetómetro. La ley de Lorentz establece que una partícula cargada q que circula a una velocidad v→ por un punto en el que existe una intensidad de campo magnético B→, sufrirá la acción de una fuerza F→ denominada fuerza de Lorentz cuyo valor es proporcional al valor de q, B→ y v→  se obtiene por medio de la siguiente expresión:  Historia Si bien algunos materiales magnéticos han sido conocidos desde la antigüedad, como por ejemplo el poder de atracción que la magnetita ejerce sobre el hierro, no fue sino hasta el siglo XIX cuando la relación entre la electricidad y el magnetismo quedó plasmada, pasando ambos campos de ser diferenciados a formar el cuerpo de lo que se conoce como electromagnetismo.  Antes de 1820, el único magnetismo conocido era el del hierro. Esto cambió con un profesor de ciencias poco conocido de la Universidad de Copenhague, Dinamarca, Hans Christian Oersted. En 1820 Oersted preparó en su casa una demostración científica a sus amigos y estudiantes. Planeó demostrar el calentamiento de un hilo por una corriente eléctrica y también llevar a cabo demostraciones sobre el magnetismo, para lo cual dispuso de una aguja de brújula montada sobre una peana de madera.  Mientras llevaba a cabo su demostración eléctrica, Oersted notó para su sorpresa que cada vez que se conectaba la corriente eléctrica, se movía la aguja de la brújula. Se calló y finalizó las demostraciones, pero en los meses sucesivos trabajó duro intentando explicarse el nuevo fenómeno.¡Pero no pudo! La aguja no era ni atraída ni repelida por la corriente. En vez de eso tendía a quedarse en ángulo recto. Hoy sabemos que esto es una prueba fehaciente de la relación intrínseca entre el campo magnético y el campo eléctrico plasmada en las ecuaciones de Maxwell.  Como ejemplo para ver la naturaleza un poco distinta del campo magnético basta considerar el intento de separar el polo de un imán. Aunque rompamos un imán por la mitad este ""reproduce"" sus dos polos. Si ahora volvemos a partir otra vez en dos, nuevamente tendremos cada trozo con los polos norte y sur diferenciados. En magnetismo no se han observado los monopolos magnéticos.  Nombre La fuerza sobre una carga eléctrica depende de su ubicación, velocidad y dirección; se utilizan dos campos vectoriales para describir esta fuerza.[2]: ch1  El primero es el campo eléctrico, que describe la fuerza que actúa sobre una carga estacionaria y da la componente de la fuerza que es independiente del movimiento. El campo magnético, en cambio, describe la componente de la fuerza que es proporcional tanto a la velocidad como a la dirección de las partículas cargadas.[2]: ch13  El campo se define por la ley de Lorentz y es, en cada instante, perpendicular tanto al movimiento de la carga como a la fuerza que experimenta.  El nombre de campo magnético o intensidad del campo magnético se aplica a dos magnitudes:  Desde un punto de vista físico, ambos son equivalentes en el vacío, salvo en una constante de proporcionalidad (permeabilidad) que depende del sistema de unidades: 1 en el sistema de Gauss,      μ  0   = 4 π ⋅  10  − 7     N       A    − 2      {\displaystyle \mu _{0}=4\pi \cdot 10^{-7}{\mbox{N}}{{\mbox{A}}^{-2}}}   en el SI. Solo se diferencian en medios materiales con el fenómeno de la magnetización.  Uso El campo H se ha considerado tradicionalmente el campo principal o intensidad de campo magnético, ya que se puede relacionar con unas cargas, masas o polos magnéticos por medio de una ley similar a la de Coulomb para la electricidad. Maxwell, por ejemplo, utilizó este enfoque, aunque aclarando que esas cargas eran ficticias. Con ello, no solo se parte de leyes similares en los campos eléctricos y magnéticos (incluyendo la posibilidad de definir un potencial escalar magnético), sino que en medios materiales, con la equiparación matemática de H con E, por un lado, y de B con D, por otro, se pueden establecer paralelismos útiles en las condiciones de contorno y las relaciones termodinámicas; las fórmulas correspondientes en el sistema electromagnético de Gauss son:           B  = μ  H        H  =  B  − 4 π  M       D  = ϵ  E      E  =  D  − 4 π  P        {\displaystyle {\begin{array}{lll}\mathbf {B} =\mu \mathbf {H} &\qquad &\mathbf {H} =\mathbf {B} -4\pi \mathbf {M} \\\mathbf {D} =\epsilon \mathbf {E} &&\mathbf {E} =\mathbf {D} -4\pi \mathbf {P} \end{array}}}    En electrotecnia no es raro que se conserve este punto de vista porque resulta práctico.  Con la llegada de las teorías del electrón de Lorentz y Poincaré, y de la relatividad de Einstein, quedó claro que estos paralelismos no se corresponden con la realidad física de los fenómenos, por lo que hoy es frecuente, sobre todo en física, que el nombre de campo magnético se aplique a B (por ejemplo, en los textos de Alonso-Finn y de Feynman).[4] En la formulación relativista del electromagnetismo, E no se agrupa con H para el tensor de intensidades, sino con B.  En 1944, F. Rasetti preparó un experimento para dilucidar cuál de los dos campos era el fundamental, es decir, aquel que actúa sobre una carga en movimiento, y el resultado fue que el campo magnético real era B y no H.[5]  Para caracterizar H y B se ha recurrido a varias distinciones. Así, H describe cuan intenso es el campo magnético en la región que afecta, mientras que B es la cantidad de flujo magnético por unidad de área que aparece en esa misma región. Otra distinción que se hace en ocasiones es que H se refiere al campo en función de sus fuentes (las corrientes eléctricas) y B al campo en función de sus efectos (fuerzas sobre las cargas).  Relación entre H y B Las fórmulas derivadas para el campo magnético anteriores son correctas cuando se trata de la corriente completa. Sin embargo, un material magnético colocado dentro de un campo magnético genera su propia magnetización, que puede ser un reto para calcular. (Esta corriente ligada se debe a la suma de los bucles de corriente de tamaño atómico y al espín de las partículas subatómicas, como los electrones, que componen el material). El campo H, tal y como se ha definido anteriormente, ayuda a factorizar esta corriente ligada; pero para ver cómo, ayuda a introducir primero el concepto de magnetización.  Magnetización El campo vectorial de magnetización M representa la fuerza con la que una región de material está magnetizada. Se define como el momento dipolar magnético neto por unidad de volumen de esa región. La magnetización de un imán uniforme es por tanto una constante del material, igual al momento magnético m del imán dividido por su volumen. Como la unidad SI del momento magnético es A⋅m2, la unidad SI de la magnetización M es el amperio por metro, idéntica a la del campo H.  El campo de magnetización M de una región apunta en la dirección del momento dipolar magnético medio en esa región. Las líneas del campo de magnetización, por lo tanto, comienzan cerca del polo sur magnético y terminan cerca del polo norte magnético. (La magnetización no existe fuera del imán).  En el modelo de bucles amperianos, la magnetización se debe a la combinación de muchos bucles amperianos diminutos para formar una corriente resultante llamada corriente ligada. Esta corriente ligada, entonces, es la fuente del campo magnético B debido al imán. (Ver Dipolos magnéticos más adelante y polos magnéticos vs. corrientes atómicas para más información). Dada la definición de dipolo magnético, el campo de magnetización sigue una ley similar a la de la ley de Ampere:[6]  donde la integral es una integral de línea sobre cualquier bucle cerrado y Ib es la corriente límite encerrada por ese bucle cerrado.  En el modelo de los polos magnéticos, la magnetización comienza y termina en los polos magnéticos. Por lo tanto, si una región determinada tiene una ""fuerza de polo magnético"" neta positiva (correspondiente a un polo norte), entonces tiene más líneas de campo de magnetización que entran en ella que las que salen. Matemáticamente esto equivale a:  donde la integral es una integral de superficie cerrada sobre la superficie cerrada S y qM es la ""carga magnética"" (en unidades de flujo magnético) encerrada por S. (Una superficie cerrada rodea completamente una región sin agujeros que dejen escapar las líneas de campo). El signo negativo se produce porque el campo de magnetización se desplaza de sur a norte "
quimica,"En química, una molécula (del nuevo latín molecula, que es un diminutivo de la palabra moles, 'masa') es un grupo eléctricamente neutro y suficientemente estable de al menos dos átomos en una configuración definida, unidos por enlaces químicos fuertes covalentes.[4][5][6][7][8][9]  En este estricto sentido, las moléculas se diferencian de los iones poliatómicos. En la química orgánica y la bioquímica, el término ""molécula"" se utiliza de manera menos estricta y se aplica también a los compuestos orgánicos (moléculas orgánicas) y en las biomoléculas.  Antes, se definía la molécula de forma menos general y precisa, como la más pequeña parte de una sustancia que podía tener existencia independiente y estable conservando aún sus propiedades fisicoquímicas. De acuerdo con esta definición, podían existir moléculas monoatómicas. En la teoría cinética de los gases, el término molécula se aplica a cualquier partícula gaseosa con independencia de su composición. De acuerdo con esta definición, los átomos de un gas noble se considerarían moléculas aunque se componen de átomos no enlazados.[10]  Una molécula puede consistir en varios átomos de un único elemento químico, como en el caso del oxígeno diatómico (O2),[11] o de diferentes elementos, como en el caso del agua (H2O).[12] Los átomos y complejos unidos por enlaces no covalentes como los enlaces de hidrógeno o los enlaces iónicos no se suelen considerar como moléculas individuales.  Las moléculas como componentes de la materia son comunes en las sustancias orgánicas (y por tanto en la bioquímica). También conforman la mayor parte de los océanos y de la atmósfera. Sin embargo, un gran número de sustancias sólidas familiares, que incluyen la mayor parte de los minerales que componen la corteza, el manto y el núcleo de la Tierra, contienen muchos enlaces químicos, pero no están formados por moléculas. Además, ninguna molécula típica puede ser definida en los cristales iónicos (sales) o en cristales covalentes, aunque estén compuestos por celdas unitarias que se repiten, ya sea en un plano (como en el grafito) o en tres dimensiones (como en el diamante o el cloruro de sodio). Este sistema de repetir una estructura unitaria varias veces también es válida para la mayoría de las fases condensadas de la materia con enlaces metálicos, lo que significa que los metales sólidos tampoco están compuestos por moléculas. En el vidrio (sólidos que presentan un estado vítreo desordenado), los átomos también pueden estar unidos por enlaces químicos sin que se pueda identificar ningún tipo de molécula, pero tampoco existe la regularidad de la repetición de unidades que caracteriza a los cristales.  Casi toda la química orgánica y buena parte de la química inorgánica se ocupan de la síntesis y reactividad de moléculas y compuestos moleculares. La química física y, especialmente, la química cuántica también estudian, cuantitativamente, en su caso, las propiedades y reactividad de las moléculas. La bioquímica está íntimamente relacionada con la biología molecular, ya que ambas estudian a los seres vivos a nivel molecular. El estudio de las interacciones específicas entre moléculas, incluyendo el reconocimiento molecular es el campo de estudio de la química supramolecular. Estas fuerzas explican las propiedades físicas como la solubilidad o el punto de ebullición de un compuesto molecular.[13]  Las moléculas rara vez se encuentran sin interacción entre ellas, salvo en gases enrarecidos y en los gases nobles. Así, pueden encontrarse en redes cristalinas, como el caso de las moléculas de H2O en el hielo o con interacciones intensas, pero que cambian rápidamente de direccionalidad, como en el agua líquida. En orden creciente de intensidad, las fuerzas intermoleculares más relevantes son: las fuerzas de Van der Waals y los puentes de hidrógeno.  La dinámica molecular es un método de simulación por computadora que utiliza estas fuerzas para tratar de explicar las propiedades de las moléculas.  No se puede definir una molécula típica para  sales ni para cristales covalentes, aunque estos a menudo se componen de células unitarias repetidas que se extienden en un plano, por ejemplo, el grafeno ; o tridimensionalmente, por ejemplo, el diamante, el cuarzo, o el cloruro de sodio. El tema de la estructura celular unitaria repetida también se aplica a la mayoría de los metales que son fases condensadas con enlaces metálicos. Por tanto, los metales sólidos no están hechos de moléculas.  En los vidrios, que son sólidos que existen en un estado vítreo desordenado, los átomos se mantienen unidos por enlaces químicos sin presencia de ninguna molécula definible, ni ninguna de la regularidad de la estructura celular unitaria repetida que caracteriza a las sales, cristales covalentes y rieles.  Ciencia molecular La ciencia de las moléculas se denomina química molecular o física molecular, dependiendo de si se centra en la química o en la física. La química molecular se ocupa de las leyes que rigen la interacción entre las moléculas que da lugar a la formación y ruptura de enlaces químicos, mientras que la física molecular se ocupa de las leyes que rigen su estructura y propiedades. En la práctica, sin embargo, esta distinción es imprecisa. En las ciencias moleculares, una molécula consiste en un sistema estable (estado ligado) compuesto por dos o más átomos. Los iones poliatómicos pueden considerarse a veces como moléculas cargadas eléctricamente. El término molécula inestable se utiliza para especies muy reactivas, es decir, conjuntos de corta duración (resonancias) de electrones y núcleos, como radicales, iones moleculares, moléculas de Rydberg, estados de transición, complejos de van der Waals, o sistemas de átomos en colisión como en el condensado de Bose-Einstein.  Historia y etimología Según la Real Academia Española el vocablo «molécula» deriva del latín moles 'mole' o 'masa' y el sufijo diminutivo -ula 'masa pequeña'.[14]  La definición de molécula ha ido evolucionando a medida que ha aumentado el conocimiento de la estructura de las moléculas. Las definiciones anteriores eran menos precisas, y definían las moléculas como las partículas más pequeñas de sustancia químicas puras que aún conservan su composición y sus propiedades químicas.[17] Esta definición a menudo se rompe ya que muchas sustancias en la experiencia ordinaria, como rocas, sales, y metales, se componen de grandes redes cristalinas de átomos de enlace químico o iones, pero no están hechas de moléculas discretas.  Definición y sus límites De manera menos general y precisa, se ha definido molécula como la parte más pequeña de una sustancia química que conserva sus propiedades químicas, y a partir de la cual se puede reconstituir la sustancia sin reacciones químicas. De acuerdo con esta definición, que resulta razonablemente útil para aquellas sustancias puras constituidas por moléculas, podrían existir las ""moléculas monoatómicas"" de gases nobles, mientras que las redes cristalinas, sales, metales y la mayoría de vidrios quedarían en una situación confusa.  Las moléculas lábiles pueden perder su consistencia en tiempos relativamente cortos, pero si el tiempo de vida medio es del orden de unas pocas vibraciones moleculares, estamos ante un estado de transición que no se puede considerar molécula. Actualmente, es posible el uso de láser pulsado para el estudio de la química de estos sistemas.  Las entidades que comparten la definición de las moléculas, pero tienen carga eléctrica se denominan iones poliatómicos, iones moleculares o moléculas ion. Las sales compuestas por iones poliatómicos se clasifican habitualmente dentro de los materiales de base molecular o materiales moleculares.  Las moléculas están formadas por partículas. Una molécula viene a ser la porción de materia más pequeña que aún conserva las propiedades de la materia original. Las moléculas se encuentran fuertemente enlazadas con la finalidad de formar materia. Las moléculas están formadas por átomos unidos por medio de enlaces químicos.  Una molécula es una unidad de sustancia que puede ser monoatómica o poliatómica. La unidad de todas las sustancias gaseosas es la molécula.[18]  Tipos de moléculas Las moléculas se pueden clasificar en:  Molécula de dinitrógeno, el gas que es el componente mayoritario del aire  Molécula de fullereno, tercera forma estable del carbono tras el diamante y el grafito  Molécula de agua, «disolvente universal», de importancia fundamental en innumerables procesos bioquímicos e industriales  Representación poliédrica del anión de Keggin, un polianión molecular  Representación de un fragmento de ADN, un polímero de importancia fundamental en la genética  Enlace peptídico que une los péptidos para formar proteínas  Representación de un fragmento lineal de polietileno, el plástico más usado  Primera generación de un dendrímero, un tipo especial de polímero que crece de forma fractal  Enlaces Los átomos que forman las moléculas se mantienen juntos mediante enlaces covalentes o enlaces iónicos. Varios tipos de elementos no metálicos existen solo como moléculas en el medio ambiente. Por ejemplo, el hidrógeno solo existe como molécula de hidrógeno. Una molécula de un compuesto está formada por dos o más elementos.[19] Una molécula homonuclear está formada por dos o más átomos de un solo elemento.  Mientras que algunas personas dicen que un cristal metálico puede considerarse una sola molécula gigante unida por enlaces metálicos,[20] otros señalan que los metales actúan de manera muy diferente a las moléculas. [21]  Covalente Un enlace covalente es un enlace químico que implica el intercambio de pares de electrones entre átomos. Estos pares de electrones se denominan pares compartidos o pares de enlace, y el equilibrio estable de fuerzas atractivas y repulsivas entre átomos, cuando comparten electrones, se denomina enlace covalente.[22]  Iónico El enlace iónico es un tipo de enlace químico que implica la atracción electrostática entre iones con carga eléctrica opuesta y es la interacción principal que se produce en los compuestos iónicos. Los iones son átomos que han perdido uno o más electrones (denominados cationes) y átomos que han ganado uno o más electrones (denominados aniones).[23] Esta transferencia de electrones se denomina electrovalencia en contraste con la covalencia. En el caso más simple, el catión es un átomo de metal y el anión es un átomo no  metálico, pero estos iones pueden ser de naturaleza más complicada, por ejemplo, iones moleculares como NH4+ o SO4 2−.  A temperaturas y presiones normales, la unión iónica crea principalmente sólidos (u ocasionalmente líquidos) sin moléculas identificables separadas, pero la vaporización/sublimación de tales materiales produce pequeñas moléculas separadas donde los electrones aún se transfieren lo suficiente como para que los enlaces se consideren iónicos en lugar de covalentes.  Descripción La estructura molecular puede ser descrita de diferentes formas. La fórmula molecular es útil para moléculas sencillas, como H2O para el agua o NH3 para el amoniaco. Contiene los símbolos de los elementos presentes en la molécula, así como su proporción indicada por los subíndices.  Para moléculas más complejas, como las que se encuentran comúnmente en química orgánica, la fórmula química no es suficiente, y vale la pena usar una fórmula estructural o una fórmula esqueletal, las que indican gráficamente la disposición espacial de los distintos grupos funcionales.  Cuando se quieren mostrar variadas propiedades moleculares, o se trata de sistemas muy complejos como proteínas, ADN o polímeros, se utilizan representaciones especiales, como los modelos tridimensionales (físicos o representados por ordenador). "
quimica,"En química, un enlace es el proceso químico generado por las interacciones atractivas entre átomos y moléculas,[1][2] y que confiere estabilidad a los compuestos químicos diatómicos y poliatómicos. La explicación de tales fuerzas atractivas es un área compleja que está descrita por las leyes de la química cuántica.  Es la fuerza existente entre los átomos una vez que se ha formado un sistema  estable.[3]  Las moléculas, cristales, metales y gases diatómicos (que forman la mayor parte del ambiente físico que nos rodea) están unidos por enlaces químicos, que determinan las propiedades físicas y químicas de la materia.  Las cargas opuestas se atraen porque al estar unidas adquieren una situación más estable que cuando estaban separadas. Esta situación de mayor estabilidad suele darse cuando el número de electrones que poseen los átomos en su último nivel es igual a ocho, estructura que coincide con la de los gases nobles ya que los electrones que orbitan el núcleo están cargados negativamente, y que los protones en el núcleo lo están positivamente, la configuración más estable del núcleo y los electrones es una en la que los electrones pasan la mayor parte del tiempo «entre» los núcleos, que en otro lugar del espacio. Estos electrones hacen que los núcleos se atraigan mutuamente.  Teoría del enlace químico En la visión simplificada del denominado enlace covalente, uno o más electrones (frecuentemente un par de electrones) son llevados al espacio entre los dos núcleos atómicos. Ahí, los electrones negativamente cargados son atraídos a las cargas positivas de ambos núcleos, en vez de solo su propio núcleo. Esto vence a la repulsión entre los dos núcleos positivamente cargados de los dos átomos, y esta atracción tan grande mantiene a los dos núcleos en una configuración de equilibrio relativamente fija, aunque aún vibrarán en la posición de equilibrio. En resumen, el enlace covalente involucra la compartición de electrones en los que los núcleos positivamente cargados de dos o más átomos atraen simultáneamente a los electrones negativamente cargados que están siendo compartidos. En un enlace covalente polar, uno o más electrones son compartidos inequitativamente entre dos núcleos.  En una visión simplificada de un enlace iónico, el electrón de enlace no es compartido, sino que es transferido. En este tipo de enlace, el orbital atómico más externo de un átomo tiene un lugar libre que permite la adición de uno o más electrones. Estos electrones recientemente agregados ocupan potencialmente un estado de menor energía (más cerca al núcleo debido a la alta carga nuclear efectiva) de lo que experimentan en un tipo diferente de átomo. En consecuencia, un núcleo ofrece una posición de más fuerte unión a un electrón de lo que lo hace el otro núcleo. Esta transferencia ocasiona que un átomo asuma una carga neta positiva, y que el otro asuma una carga neta negativa. Entonces, el enlace resulta de la atracción electrostática entre los átomos, y los átomos se constituyen en ((iones)) de carga positiva o negativa.  Todos los enlaces pueden ser explicados por la teoría cuántica, pero, en la práctica, algunas reglas de simplificación les permiten a los químicos predecir la fuerza de enlace, direccionalidad y polaridad de los enlaces. La regla del octeto y la (TREPEV) teoría de repulsión de pares de electrones de la capa de valencia son dos ejemplos.  Existen teorías más sofisticadas, como la teoría del enlace de valencia, que incluye la hibridación de orbitales y la resonancia, y el método de combinación lineal de orbitales atómicos dentro de la teoría de los orbitales moleculares, que incluye a la teoría del campo de los ligantes. La electrostática es usada para describir polaridades de enlace y los efectos que ejerce en las sustancias químicas.  Historia del concepto de «enlace» Las primeras especulaciones respecto a la naturaleza del enlace químico son tan tempranas como en el siglo XII. Se suponía que ciertos tipos de especies químicas estaban unidas entre sí por un tipo de afinidad química.   En 1704, Isaac Newton esbozó su teoría de enlace atómico, en ""Query 31"" de su Opticks, donde los átomos se unen unos a otros por alguna «fuerza». Específicamente, después de investigar varias teorías populares, en boga en aquel tiempo, de cómo los átomos se podía unir unos a otros, por ejemplo, «átomos enganchados», «átomos pegados unos a otros por reposo», o «unidos por movimientos conspirantes», Newton señaló lo que inferiría posteriormente a partir de su cohesión que:  En 1819, a raíz de la invención de la pila voltaica, Jöns Jakob Berzelius desarrolló una teoría de combinación química, introduciendo indirectamente el carácter electropositivo y electronegativo de los átomos combinantes. A mediados del siglo XIX, Edward Frankland, F. A. Kekule, A. S. Couper, A. M. Butlerov y Hermann Kolbe, ampliando la teoría de radicales, desarrollaron la teoría de valencia, originalmente llamado «poder combinante» en que los compuestos se mantenía unidos debido a la atracción entre polos positivo y negativo. En 1916, el químico Gilbert N. Lewis desarrolló el concepto de enlace de par de electrones, en el que dos átomos pueden compartir uno y seis electrones, formando el enlace de un solo electrón, enlace simple, enlace doble, o enlace triple:  En las propias palabras de Lewis:  El mismo año, Walther Kossel lanzó una teoría similar a la de Lewis, con la diferencia de que su modelo asumía una transferencia completa de electrones entre los átomos, con lo que era un modelo de enlace iónico. Tanto Lewis y Kossel estructuraron sus modelos de enlace a partir de la regla de Abegg (1904).  En 1927, el físico danés Oyvind Burrau derivó la primera descripción cuántica matemáticamente completa de un enlace químico simple, el producido por un electrón en el ion de hidrógeno molecular (dihidrogenilio), H2+.[4] Este trabajo mostró que la aproximación cuántica a los enlaces químicos podrían ser correctas fundamental y cualitativamente, pero los métodos matemáticos usados no podrían extenderse a moléculas que contuvieran más de un electrón. Una aproximación más práctica, aunque menos cuantitativa, fue publicada en el mismo año por Walter Heitler y Fritz London. El método de Heitler-London forma la base de lo que ahora se denomina teoría del enlace de valencia. En 1929, sir John Lennard-Jones introdujo el método de combinación lineal de orbitales atómicos (CLOA o dentro de la teoría de orbitales moleculares, sugiriendo también métodos para derivar las estructuras electrónicas de moléculas de F2 (flúor) y las moléculas de O2 (oxígeno), a partir de principios cuánticos básicos. Esta teoría de orbital molecular representó un enlace covalente como un orbital formado por combinación de los orbitales atómicos de la mecánica cuántica de Schrödinger que habían sido hipotetizados por los electrones en átomos solitarios. Las ecuaciones para los electrones de enlace en átomos multielectrónicos no podrían ser resueltos con perfección matemática (esto es, analíticamente), pero las aproximaciones para ellos aún producen muchas predicciones y resultados cualitativos buenos. Muchos cálculos cuantitativos en química cuántica moderna usan tanto las teorías de orbitales moleculares o de enlace de valencia como punto de partida, aunque una tercera aproximación, la teoría del funcional de la densidad, se ha estado haciendo más popular en años recientes.  En 1935, H. H. James y A. S. Coolidge llevaron a cabo un cálculo sobre la molécula de dihidrógeno que, a diferencia de todos los cálculos previos que usaban funciones solo de la distancia de los electrones a partir del núcleo atómico, usó funciones que sólo adicionaban explícitamente la distancia entre los dos electrones.[5] Con 13 parámetros ajustables, ellos obtienen el resultado muy cercano al resultado experimental para la energía de disociación de enlace. Posteriores extensiones usaron hasta 54 parámetros y producen gran concordancia con los experimentos. Este cálculo convenció a la comunidad científica que la teoría cuántica podría concordar con los experimentos. Sin embargo, esta aproximación no tiene relación física con la teoría de enlace de valencia y orbitales moleculares y es difícil de extender a moléculas más grandes.  Teoría de enlace de valencia En el año 1927, la teoría de enlace de valencia fue formulada, argumentando esencialmente que el enlace químico se forma cuando dos electrones de valencia, en sus respectivos orbitales atómicos, trabajan o funcionan para mantener los dos núcleos juntos, en virtud a los efectos de disminución de energía del sistema. En 1939, a partir de esta teoría, el químico Linus Pauling publicó lo que algunos consideran uno de las más importantes publicaciones en la historia de la química: ""Sobre la naturaleza del enlace químico"". En este documento, tomando en cuenta los trabajos de Lewis, la teoría del enlace de valencia (TEV) de Heitler y London, así como su propio trabajo preliminar, presentó seis reglas para el enlace de electrones compartidos, aunque las tres primeras ya eran conocidas genéricamente:   Sus tres últimas reglas eran nuevas:  A partir de este artículo, Pauling publicaría en 1939 un libro de texto, Sobre la Naturaleza del Enlace Químico', que vendría a ser llamado por algunos como la «biblia» de la química moderna. Este libro ayudó a los químicos experimentales a entender el impacto de la teoría cuántica sobre la química. Sin embargo, la edición posterior de 1939 falló en explicar adecuadamente los problemas que parecían ser más cognoscibles por la teoría de orbitales moleculares. El impacto de la teoría del enlace de valencia declinó durante la década de 1960 y 1970 a la par con el crecimiento en popularidad de la teoría de orbitales moleculares, que estaba siendo implementada en muchos programas de grandes ordenadores. A partir de la década de 1960, los problemas más difíciles de la implementación de la teoría del enlace de valencia en programas de computadoras habían sido mayormente resueltos y la teoría del enlace de valencia vio un resurgimiento.  Teoría de los orbitales moleculares La teoría de los orbitales moleculares (TOM) usa una combinación lineal de orbitales atómicos para formar orbitales moleculares, que abarcan la molécula entera. Estos orbitales son divididos frecuentemente en orbitales enlazantes, orbitales antienlazantes, y orbitales de no enlace. Un orbital molecular es simplemente un orbital de Schrödinger que incluye varios, pero frecuentemente solo dos, núcleos. Si este orbital es del tipo en que los electrones tienen una mayor probabilidad de estar entre los núcleos que en cualquier otro lugar, el orbital será un orbital enlazante, y tenderá a mantener los núcleos cerca. Si los electrones tienden a estar presentes en un orbital molecular en que pasan la mayor parte del tiempo en cualquier lugar excepto entre los núcleos, el orbital funcionará como un orbital antienlazante, y realmente debilitará el enlace. Los electrones en orbitales no enlazantes tienden a estar en orbitales profundos (cerca a los orbitales atómicos) asociados casi enteramente o con un núcleo o con otro y entonces pasarán igual tiempo entre los núcleos y no en ese espacio. Estos electrones no contribuyen ni detractan la fuerza del enlace.  El modelo de enlace A pesar de que todos los electrones de un átomo giran alrededor de su núcleo, solo los electrones de valencia giran más lejos de él, mientras más alejados del núcleo se encuentren, más posibilidades tendrá ese átomo de interactuar con electrones de otro.  Los electrones de valencia interaccionan de distintas formas, ya que dependen de las características del otro átomo con el que pueda conjuntarse. Algunos átomos ceden sus electrones a otro para lograr su equilibrio, otros los ganan y a veces también los diferenciarlos de los electrones de valencia del hidrógeno.  La mayoría de los átomos se unen compartiendo electrones mediante uno, dos o hasta tres pares. Para no colocar tantos puntos, Heitler-London, produce una aproximación más cercana a la energía de enlace, y provee una representación más exacta del comportamiento de los electrones al formarse y romperse los enlaces químicos. En contraste, la teoría de orbitales moleculares simple predice que la molécula de hidrógeno se disocia en una superposición lineal de átomos de hidrógeno, e iones positivos y negativos de hidrógeno, un resultado completamente contrario a la evidencia física. Esto explica en parte por qué la curva de energía total versus la distancia interatómica del método "
quimica,"Un proceso químico es un conjunto de operaciones  químicas y/o físicas encaminadas a la transformación de unas sustancias iniciales en productos finales diferentes.[1] Un producto es diferente de otro cuando tenga distinta composición, esté en un estado distinto o hayan cambiado sus condiciones. Los procesos químicos como la destilación y cristalización se remontan a la alquimia en Alejandría, Egipto.  En la descripción general de cualquier proceso químico existen diferentes operaciones involucradas. Unas llevan inherentes diversas reacciones químicas. En cambio otros pasos son meramente físicos, es decir, sin reacciones químicas presentes. Podemos decir que cualquier proceso químico que se pueda diseñar consta de una serie de operaciones físicas y químicas. Cada una de estas operaciones es una operación unitaria dentro del proceso global.  En un sentido científico, un proceso químico es un método que genera el cambio de uno o más productos químicos o compuestos químicos. Un proceso químico puede producirse por sí mismo o ser causado por una fuerza externa, y consiste en una reacción química de algún tipo. En un sentido ingenieril, un proceso químico es un método destinado a ser utilizado en la fabricación o en escala industrial para cambiar la composición del producto químico(s) o material(es), por lo general utilizando una tecnología similar o relacionada con la utilizada en plantas químicas o la industria química.  Ninguna de estas definiciones es exacta en el sentido de que siempre se puede decir definitivamente lo que es un proceso químico y lo que no lo es; son definiciones prácticas. También hay un solapamiento significativo en estas dos variaciones de definición. Debido a la inexactitud de la definición, los químicos y otros científicos utilizan el término «proceso químico» solo en un sentido general o en el sentido de la ingeniería.   Aunque en un proceso químico a veces puede implicar solo un paso, a menudo múltiples etapas son requeridas de manera previa, a ellas se las conoce como operaciones unitarias. En una planta, cada una de las operaciones unitarias son continuos e interrelacionadas en cada sección de la misma. A menudo, una o más reacciones químicas están involucradas, pero otras formas de cambiar (material o) la composición química pueden ser utilizadas, tales como la mezcla o procesos de separación. Los pasos del proceso pueden ser secuenciales en el tiempo o secuencial en el espacio a lo largo de una corriente o el material en movimiento. Para una cantidad dada de un material de alimentación (entrada) hay producto terminado (salida), una cantidad esperada de un material se puede determinar en los pasos clave en el proceso, a partir de datos empíricos y los cálculos de balance de materiales. Estas cantidades se pueden extrapolar hacia arriba o hacia abajo para adaptarse a la capacidad deseada o el funcionamiento de una planta química en particular construida para tal proceso.   Los procesos químicos pueden ilustrarse generalmente como diagramas de flujo de bloque o en más detalle como diagramas de flujo de proceso. diagramas de flujo de bloques muestran las unidades como bloques y las corrientes que fluyen entre ellos como la conexión de líneas con puntas de flecha para mostrar la dirección del flujo.  "
quimica,"Una composición química especifica: la identidad, la disposición y la proporción de los elementos que conforman un compuesto.  Las fórmulas químicas se pueden usar para describir las cantidades relativas de elementos presentes en un compuesto. Se utilizan diferentes tipos de fórmulas químicas para transmitir información sobre la composición, como una fórmula empírica o molecular. Por ejemplo, la fórmula química del agua es H2O: esto significa que cada molécula de agua está constituida por 2 átomos de hidrógeno (H) y 1 átomo de oxígeno (O). La composición química del agua puede interpretarse como una proporción de 2:1 de átomos de hidrógeno a átomos de oxígeno.   La nomenclatura se puede usar para expresar los elementos presentes en un compuesto y su disposición dentro de las moléculas del compuesto. De esta forma, los compuestos tendrán nombres únicos que pueden describir su composición elemental. [1]  Composición de una mezcla La composición química de una mezcla se puede definir como la distribución de las sustancias individuales que constituyen la mezcla, denominadas ""componentes"". Equivale a cuantificar la concentración de cada componente. Debido a que existen diferentes formas de definir la concentración de un componente, también existen diferentes formas de definir la composición de una mezcla. Puede expresarse como fracción molar, fracción de volumen, fracción de masa, molalidad, molaridad o normalidad o relación de mezcla.  La composición química de una mezcla se puede representar gráficamente en gráficos como gráfico ternario y gráfico cuaternario.  "
quimica,"El átomo es la parte más pequeña de la materia que tiene propiedades de un elemento químico.[1] Cada sólido, líquido, gas y plasma se compone de átomos neutros o ionizados. Los átomos son microscópicos; los tamaños típicos son alrededor de 100 pm (diez mil millonésima parte de un metro).[2] No obstante, los átomos no tienen límites bien definidos y hay diferentes formas de definir su tamaño que dan valores diferentes pero cercanos. Los átomos son lo suficientemente pequeños para que la física clásica dé resultados notablemente incorrectos. A través del desarrollo de la física, los modelos atómicos han incorporado principios cuánticos para explicar y predecir mejor su comportamiento. El término proviene del latín atŏmus, calco del griego ἄτομον (átomon) ἄτομος, unión de α (a, que significa «sin»), y τόμος (tómos, «sección»), que literalmente es «que no se puede cortar, indivisible»,[3] y fue el nombre que se dice les dio Demócrito de Abdera, discípulo de Leucipo de Mileto, a las partículas que él concebía como las de menor tamaño posible.[4]  Cada átomo se compone de un núcleo y uno o más electrones unidos al núcleo. El núcleo está compuesto de uno o más protones y típicamente un número similar de neutrones.[5] Los protones y los neutrones son llamados nucleones. Más del 99,94 % de la masa del átomo está en el núcleo. Los protones tienen una carga eléctrica positiva, los electrones tienen una carga eléctrica negativa y los neutrones no tienen carga eléctrica. Si el número de protones y electrones son iguales, ese átomo es eléctricamente neutro. Si un átomo tiene más o menos electrones que protones, entonces tiene una carga global negativa o positiva, respectivamente, y se denomina ion (anión si es negativa y catión si es positiva).  Los electrones de un átomo son atraídos por los protones en un núcleo atómico por la fuerza electromagnética. Los protones y los neutrones en el núcleo son atraídos el uno al otro por una fuerza diferente, la fuerza nuclear, que es generalmente más fuerte que la fuerza electromagnética que repele los protones cargados positivamente entre sí. Bajo ciertas circunstancias, más acentuado cuanto mayor número de protones tenga el átomo, la fuerza electromagnética repelente se vuelve más fuerte que la fuerza nuclear y los nucleones pueden ser expulsados o desechados del núcleo, dejando tras de sí un elemento diferente: desintegración nuclear que resulta en transmutación nuclear.  El número de protones en el núcleo define a qué elemento químico pertenece el átomo:[6] por ejemplo, todos los átomos de cobre contienen 29 protones. El número de neutrones define el isótopo del elemento.[7] El número de electrones influye en las propiedades magnéticas de un átomo. Los átomos pueden unirse a otro u otros átomos por enlaces químicos (en los cuales intervienen los electrones de dichos átomos) para formar compuestos químicos tales como moléculas y redes cristalinas. La capacidad de los átomos de asociarse y disociarse es responsable de la mayor parte de los cambios físicos observados en la naturaleza y es el tema de la disciplina de la química.  Existe la antimateria, la cual está compuesta también por átomos pero con las cargas invertidas;[8] los protones tienen carga negativa y se denominan antiprotones, y los electrones tienen una carga positiva y se denominan positrones. Es muchísimo menos frecuente en la naturaleza. Al entrar en contacto con la respectiva partícula (como los protones con los antiprotones y los electrones con los positrones) ambas se aniquilan generando un estallido de energía de rayos gamma y otras partículas.  No toda la materia del universo está compuesta de átomos; de hecho, solo el 5% o menos del universo está compuesto por estos. La materia oscura, que constituye según algunas estimaciones más del 20% del universo, no se compone de átomos, sino de partículas de un tipo actualmente desconocido. También cabe destacar la energía oscura, la cual es un componente que está distribuido por todo el universo, ocupando aproximadamente más del 70% de este.  Introducción El concepto de átomo como bloque básico e indivisible que compone la materia del universo fue postulado por la escuela atomista en la Antigua Grecia, en el siglo V a. C., siendo Demócrito (Abdera, Tracia, c. 460 a. C.-c. 370 a. C.) uno de sus exponentes.   Aristóteles, posteriormente, postula que la materia estaba formada por cuatro elementos, pero niega la idea de átomo. La teoría atomista fue sin embargo mantenida por diversas escuelas filosóficas, entre ellas la epicúrea. Para Epicuro, los átomos son unidades indivisibles que poseen tres propiedades: forma, tamaño y peso. Se encuentran permanentemente en movimiento y se unen unos a otros en virtud de sus formas. Su número es infinito y la cantidad de sus formas también es muy grande (aunque no necesariamente infinita). Las propiedades de los cuerpos derivan de las propiedades atómicas.  Tras la Revolución científica, la escuela atomista griega fue reconsiderada por las nuevas generaciones de científicos de mediados del siglo XIX, cuando sus conceptos fueron introducidos para explicar las leyes químicas. Con el desarrollo de la física nuclear en el siglo XX se comprobó que el átomo puede subdividirse en partículas más pequeñas.[9][10]  Los átomos son objetos muy pequeños con masas igualmente minúsculas: su diámetro y masa son del orden de la diez mil millonésima parte de un metro y cuatrillonésima parte de un gramo. Solo pueden ser observados mediante instrumentos especiales tales como un microscopio de efecto túnel. Más de un 99,94 % de la masa del átomo está concentrada en su núcleo, en general repartida de manera aproximadamente equitativa entre protones y neutrones. El núcleo de un átomo puede ser inestable y sufrir una transmutación mediante desintegración radioactiva. Los electrones en la nube del átomo están repartidos en distintos niveles de energía u orbitales, y determinan las propiedades químicas del mismo. Las transiciones entre los distintos niveles dan lugar a la emisión o absorción de radiación electromagnética en forma de fotones, y son la base de la espectroscopia.  Estructura atómica Partículas subatómicas A pesar de que átomo significa ‘indivisible’, en realidad está formado por varias partículas subatómicas. El átomo contiene protones, neutrones y electrones, con la excepción del átomo de hidrógeno-1, que no contiene neutrones, y del catión hidrógeno o hidrón, que no contiene electrones. Los protones y neutrones del átomo se denominan nucleones, por formar parte del núcleo atómico.  El electrón es la partícula más ligera de las que componen el átomo, con una masa de 9,11 · 10−31 kg. Tiene una carga eléctrica negativa, cuya magnitud se define como la carga eléctrica elemental, y se ignora si posee subestructura, por lo que se considera una partícula elemental. Los protones tienen una masa de 1,672 · 10−27 kg, 1836 veces la del electrón, y una carga positiva opuesta a la de este. Los neutrones tienen una masa de 1,69 · 10−27 kg, 1839 veces la del electrón, y no poseen carga eléctrica. Las masas de ambos nucleones son ligeramente inferiores dentro del núcleo, debido a la energía potencial del mismo, y sus tamaños son similares, con un radio del orden de 8 · 10−16 m o 0,8 femtómetros (fm).[11]  Ni el protón, ni el neutrón son partículas elementales, sino que constituyen un estado ligado de quarks u y d, partículas fundamentales recogidas en el modelo estándar de la física de partículas, con cargas eléctricas iguales a +2/3 y −1/3 respectivamente, respecto de la carga elemental. Un protón contiene dos quarks u y un quark d, mientras que el neutrón contiene dos d y un u, en consonancia con la carga de ambos. Los quarks se mantienen unidos mediante la fuerza nuclear fuerte, mediada por gluones —del mismo modo que la fuerza electromagnética está mediada por fotones—. Además de estas, existen otras partículas subatómicas en el modelo estándar: más tipos de quarks, leptones cargados (similares al electrón), etc.  El núcleo atómico Los protones y neutrones de un átomo se encuentran ligados en el núcleo atómico, en la parte central del mismo. El volumen del núcleo es aproximadamente proporcional al número total de nucleones, el número másico A,[12] lo cual es mucho menor que el tamaño del átomo, cuyo radio es del orden de 105 fm o 1 ángstrom (Å). Los nucleones se mantienen unidos mediante la fuerza nuclear, que es mucho más intensa que la fuerza electromagnética a distancias cortas, lo cual permite vencer la repulsión eléctrica entre los protones.[13]  Los átomos de un mismo elemento tienen el mismo número de protones, que se denomina número atómico y se representa por Z. Los átomos de un elemento dado pueden tener distinto número de neutrones: se dice entonces que son isótopos. Ambos números conjuntamente determinan el nucleido.  El núcleo atómico puede verse alterado por procesos muy energéticos en comparación con las reacciones químicas. Los núcleos inestables sufren desintegraciones que pueden cambiar su número de protones y neutrones emitiendo radiación. Un núcleo pesado puede fisionarse en otros más ligeros en una reacción nuclear o espontáneamente. Mediante una cantidad suficiente de energía, dos o más núcleos pueden fusionarse en otro más pesado.  En átomos con número atómico bajo, los núcleos con una cantidad distinta de protones y neutrones tienden a desintegrarse en núcleos con proporciones más parejas, más estables. Sin embargo, para valores mayores del número atómico, la repulsión mutua de los protones requiere una proporción mayor de neutrones para estabilizar el núcleo.[14]  Nube de electrones Los electrones en el átomo son atraídos por los protones a través de la atracción electromagnética. Esta fuerza los atrapa en un pozo de potencial electrostático alrededor del núcleo, lo que hace necesaria una fuente de energía externa para liberarlos. Cuanto más cerca está un electrón del núcleo, mayor es la fuerza atractiva, y mayor por tanto la energía necesaria para que escape.  Los electrones, como otras partículas, presentan simultáneamente propiedades de partícula puntual y de onda, y tienden a formar un cierto tipo de onda estacionaria alrededor del núcleo, en reposo respecto de este. Cada una de estas ondas está caracterizada por un orbital atómico, una función matemática que describe la probabilidad de encontrar al electrón en cada punto del espacio. El conjunto de estos orbitales es discreto, es decir, puede enumerarse, como es propio en todo sistema cuántico. La nube de electrones es la región ocupada por estas ondas, visualizada como una densidad de carga negativa alrededor del núcleo.  Cada orbital corresponde a un posible valor de energía para los electrones, que se reparten entre ellos. El principio de exclusión de Pauli prohíbe que más de dos electrones se encuentren en el mismo orbital. Pueden ocurrir transiciones entre los distintos niveles de energía: si un electrón absorbe un fotón con energía suficiente, puede saltar a un nivel superior; también desde un nivel más alto puede acabar en un nivel inferior, radiando el resto de la energía en un fotón. Las energías dadas por las diferencias entre los valores de estos niveles son las que se observan en las líneas espectrales del átomo.  Propiedades del átomo Masa La mayor parte de la masa del átomo viene de los nucleones, los protones y neutrones del núcleo. También contribuyen en una pequeña parte la masa de los electrones, y la energía de ligadura de los nucleones, en virtud de la equivalencia entre masa y energía. La unidad de masa que se utiliza habitualmente para expresarla es la unidad de masa atómica (u). Esta se define como la doceava parte de la masa de un átomo neutro de carbono-12 libre, cuyo núcleo contiene 6 protones y 6 neutrones, y equivale a 1,66 · 10−27 kg aproximadamente. En comparación el protón y el neutrón libres tienen una masa de 1,007 y 1,009 u. La masa de un átomo es entonces aproximadamente igual al número de nucleones en su núcleo —el número másico— multiplicado por la unidad de masa atómica. El átomo estable más pesado es el plomo-208, con una masa de 207,98  "
quimica,"La tabla periódica de los elementos es una disposición de los elementos químicos en forma de tabla, ordenados por su número atómico (número de protones),[2] por su configuración de electrones y sus propiedades químicas. Este ordenamiento muestra tendencias periódicas como elementos con comportamiento similar en la misma columna.  En palabras de Theodor Benfey, la tabla y la ley periódica «son el corazón de la química —comparables a la teoría de la evolución en biología (que sucedió al concepto de la scala naturae), y a los principios de termodinámica en la física clásica—».[3]  Las filas de la tabla se denominan períodos y las columnas grupos.[4] Algunos grupos tienen nombres, así por ejemplo el grupo 17 es el de los halógenos y el grupo 18 el de los gases nobles.[5] La tabla también se divide en cuatro bloques con algunas propiedades químicas similares.[6] Debido a que las posiciones están ordenadas, se puede utilizar la tabla para obtener relaciones entre las propiedades de los elementos, o pronosticar propiedades de elementos nuevos todavía no descubiertos o sintetizados. La tabla periódica proporciona un marco útil para analizar el comportamiento químico y es ampliamente utilizada en química y otras ciencias.  Dmitri Mendeléyev publicó en 1869 la primera versión de tabla periódica que fue ampliamente reconocida, la desarrolló para ilustrar tendencias periódicas en las propiedades de los elementos entonces conocidos, al ordenar los elementos basándose en sus propiedades químicas,[7] si bien Julius Lothar Meyer, trabajando por separado, llevó a cabo un ordenamiento a partir de las propiedades físicas de los átomos.[8] Mendeléyev también pronosticó algunas propiedades de elementos entonces desconocidos que anticipó que ocuparían los lugares vacíos en su tabla. Posteriormente se demostró que la mayoría de sus predicciones eran correctas cuando se descubrieron los elementos en cuestión.   La tabla periódica de Mendeléyev ha sido desde entonces ampliada y mejorada con el descubrimiento o síntesis de elementos nuevos y el desarrollo de modelos teóricos nuevos para explicar el comportamiento químico. La estructura actual fue diseñada por Alfred Werner a partir de la versión de Mendeléyev. Existen además otros arreglos periódicos de acuerdo a diferentes propiedades y según el uso que se le quiera dar (en didáctica, geología, etc.).[9] Para celebrar el 150 aniversario de su creación, la UNESCO declaró 2019 como el Año Internacional de la Tabla Periódica de los Elementos Químicos.  Se han descubierto o sintetizado todos los elementos de número atómico del 1 (hidrógeno) al 118 (oganesón); la IUPAC confirmó los elementos 113, 115, 117 y 118 el 30 de diciembre de 2015,[10] y sus nombres y símbolos oficiales se hicieron públicos el 28 de noviembre de 2016.[1] Los primeros 94 existen naturalmente, aunque algunos solo se han encontrado en cantidades pequeñas y fueron sintetizados en laboratorio antes de ser encontrados en la naturaleza.[n. 1] Los elementos con números atómicos del 95 al 118 solo han sido sintetizados en laboratorios. Allí también se produjeron numerosos radioisótopos sintéticos de elementos presentes en la naturaleza. Los elementos del 95 a 100 existieron en la naturaleza en tiempos pasados, pero actualmente no.[11] La investigación para encontrar por síntesis nuevos elementos de números atómicos más altos continúa.      Para una versión más detallada de la tabla periódica con hipertexto, consúltese Anexo:Tabla periódica.  Historia La historia de la tabla periódica está muy relacionada con varios aspectos del desarrollo de la química y física:   Descubrimiento de los elementos Aunque algunos elementos como el oro (Au), plata (Ag), cobre (Cu), plomo (Pb) y mercurio (Hg) ya eran conocidos desde la antigüedad, el primer descubrimiento científico de un elemento ocurrió en el siglo XVII d. C., cuando el alquimista Hennig Brand descubrió el fósforo (P).[13] En el siglo XVIII d. C. se conocieron numerosos nuevos elementos, los más importantes de los cuales fueron los gases, con el desarrollo de la química neumática: oxígeno (O), hidrógeno (H) y nitrógeno (N). También se consolidó en esos años la nueva concepción de elemento, que condujo a Antoine Lavoisier a escribir su famosa lista de sustancias simples, donde aparecían 33 elementos. A principios del siglo XIX d. C., la aplicación de la pila eléctrica al estudio de fenómenos químicos condujo al descubrimiento de nuevos elementos, como los metales alcalinos y alcalino-térreos, sobre todo gracias a los trabajos de Humphry Davy. En 1830 ya se conocían 55 elementos. Posteriormente, a mediados del siglo XIX d. C., con la invención del espectroscopio, se descubrieron nuevos elementos, muchos de ellos nombrados por el color de sus líneas espectrales características: cesio (Cs, del latín caesĭus, azul), talio (Tl, de tallo, por su color verde), rubidio (Rb, rojo), etc. Durante el siglo XX d. C., la investigación en los procesos radioactivos llevó al descubrimiento en cascada de una serie de elementos pesados (casi siempre sustancias artificiales sintetizadas en laboratorio, con periodos de vida estable muy cortos), hasta alcanzar la cifra de 118 elementos con denominación oficialmente aceptados por la IUPAC en noviembre de 2016.[1]  Noción de elemento y propiedades periódicas Lógicamente, un requisito previo necesario a la construcción de la tabla periódica era el descubrimiento de un número suficiente de elementos individuales, que hiciera posible encontrar alguna pauta en comportamiento químico y sus propiedades. Durante los siguientes dos siglos se fue adquiriendo un mayor conocimiento sobre estas propiedades, así como descubriendo muchos elementos nuevos.  La palabra «elemento» procede de la ciencia griega, pero su noción moderna apareció a lo largo del siglo XVII d. C., aunque no existe un consenso claro respecto al proceso que condujo a su consolidación y uso generalizado. Algunos autores citan como precedente la frase de Robert Boyle en su famosa obra El químico escéptico, donde denomina elementos «ciertos cuerpos primitivos y simples que no están formados por otros cuerpos, ni unos de otros, y que son los ingredientes de que se componen inmediatamente y en que se resuelven en último término todos los cuerpos perfectamente mixtos». En realidad, esa frase aparece en el contexto de la crítica de Robert Boyle a los cuatro elementos aristotélicos.   A lo largo del siglo XVIII d. C., las tablas de afinidad recogieron un nuevo modo de entender la composición química, que aparece claramente expuesto por Lavoisier en su obra Tratado elemental de química. Todo ello condujo a diferenciar en primer lugar qué sustancias de las conocidas hasta ese momento eran elementos químicos, cuáles eran sus propiedades y cómo aislarlas.  El descubrimiento de gran cantidad de elementos nuevos, así como el estudio de sus propiedades, pusieron de manifiesto algunas semejanzas entre ellos, lo que aumentó el interés de los químicos por buscar algún tipo de clasificación.  Los pesos atómicos A principios del siglo XIX d. C., John Dalton (1766-1844) desarrolló una concepción nueva del atomismo, a la que llegó gracias a sus estudios meteorológicos y de los gases de la atmósfera. Su principal aportación consistió en la formulación de un «atomismo químico» que permitía integrar la nueva definición de elemento realizada por Antoine Lavoisier (1743-1794) y las leyes ponderales de la química (proporciones definidas, proporciones múltiples, proporciones recíprocas).   Dalton empleó los conocimientos sobre proporciones en las que reaccionaban las sustancias de su época y realizó algunas suposiciones sobre el modo como se combinaban los átomos de las mismas. Estableció como unidad de referencia la masa de un átomo de hidrógeno (aunque se sugirieron otros en esos años) y refirió el resto de los valores a esta unidad, por lo que pudo construir un sistema de masas atómicas relativas. Por ejemplo, en el caso del oxígeno, Dalton partió de la suposición de que el agua era un compuesto binario, formado por un átomo de hidrógeno y otro de oxígeno. No tenía ningún modo de comprobar este punto, por lo que tuvo que aceptar esta posibilidad como una hipótesis a priori.   Dalton sabía que una parte de hidrógeno se combinaba con siete partes (ocho, afirmaríamos en la actualidad) de oxígeno para producir agua. Por lo tanto, si la combinación se producía átomo a átomo, es decir, un átomo de hidrógeno se combinaba con un átomo de oxígeno, la relación entre las masas de estos átomos debía ser 1:7 (o 1:8 se calcularía en la actualidad). El resultado fue la primera tabla de masas atómicas relativas (o pesos atómicos, como los llamaba Dalton), que fue modificada y desarrollada en años posteriores. Las inexactitudes antes mencionadas dieron lugar a toda una serie de polémicas y disparidades respecto a las fórmulas y los pesos atómicos, que solo comenzarían a superarse, aunque no totalmente, en el congreso de Karlsruhe en 1860.  Primeros intentos de sistematización En 1789 Antoine Lavoisier publicó una lista de 33 elementos químicos, agrupándolos en gases, metales, no metales y tierras.[14][15] Aunque muy práctica y todavía funcional en la tabla periódica moderna, fue rechazada debido a que había muchas diferencias tanto en las propiedades físicas como en las químicas.[cita requerida]  Los químicos pasaron el siglo siguiente buscando un esquema de clasificación más preciso. Uno de los primeros intentos para agrupar los elementos de propiedades análogas y relacionarlos con los pesos atómicos se debe al químico alemán Johann Wolfgang Döbereiner (1780-1849) quien en 1817 puso de manifiesto el notable parecido que existía entre las propiedades de ciertos grupos de tres elementos, con una variación gradual del primero al último. Posteriormente (1827) señaló la existencia de otros grupos en los que se daba la misma relación —cloro, bromo y yodo; azufre, selenio y telurio; litio, sodio y potasio—.  A estos grupos de tres elementos se los denominó tríadas. Al clasificarlas, Döbereiner explicaba que el peso atómico promedio de los pesos de los elementos extremos, es parecido al del elemento en medio.[16] Esto se conoció como la ley de Tríadas.[17] Por ejemplo, para la tríada cloro-bromo-yodo, los pesos atómicos son respectivamente 36, 80 y 127; el promedio es 81, que es aproximadamente 80; el elemento con el peso atómico aproximado a 80 es el bromo, lo cual hace que concuerde con el aparente ordenamiento de tríadas.   El químico alemán Leopold Gmelin trabajó con este sistema, y en 1843 había identificado diez tríadas, tres grupos de cuatro, y un grupo de cinco. Jean-Baptiste Dumas publicó el trabajo en 1857 que describe las relaciones entre los diversos grupos de metales. Aunque los diversos químicos fueron capaces de identificar las relaciones entre pequeños grupos de elementos, aún tenían que construir un esquema que los abarcara a todos.[16]  En 1857 el químico alemán August Kekulé observó que el carbono está a menudo unido a otros cuatro átomos. El metano, por ejemplo, tiene un átomo de carbono y cuatro átomos de hidrógeno.[18] Este concepto finalmente se conocería como «valencia».[19]  En 1862 de Chancourtois, geólogo francés, publicó una primera forma de tabla periódica que llamó la «hélice telúrica» o «tornillo». Fue la primera persona en notar la periodicidad de los elementos. Al disponerlos en espiral sobre un cilindro por orden creciente de peso atómico, de Chancourtois mostró que los elementos con propiedades similares parecían ocurrir a intervalos regulares. Su tabla incluye además algunos iones y compuestos. También utiliza términos geológicos en lugar de químicos y no incluye un diagrama; como resultado, recibió poca atención hasta el trabajo de Dmitri Mendeléyev.[20]  En 1864 Julius Lothar Meyer, un químico alemán, publicó una tabla con 44 elementos dispuestos por valencia. La misma mostró que los elementos con propiedades similares a menudo compartían la misma valencia.[21] Al mismo tiempo, William Odling —un químico inglés— publicó un arreglo de 57 elementos ordenados en función de sus pesos atómicos. Con algunas irregularidades y vacíos, se dio cuenta de lo que parecía ser una periodicidad de pesos atómicos entre los elementos y que esto estaba de acuerdo con «las agrupaciones que generalmente recibían».[22] Odling alude a la idea de una ley periódica, pero no siguió la misma.[23] En 1870 propuso una clasificación basada en la valencia de los elementos.[24]  Ley de las octavas de Newlands El químico inglés John Newlands produjo una serie de documentos de 1863 a 1866 y señaló que cuando los elementos se enumeran en orden "
quimica,"Los elementos químicos son un tipo de materia formada por átomos de la misma clase.[1] Los átomos que lo constituyen, poseen un número determinado de protones en su núcleo haciéndolo pertenecer a una categoría única clasificada por su número atómico, aún cuando este pueda desplegar distintas masas atómicas.   Un átomo es aquella sustancia que no puede ser descompuesta mediante una reacción química, en otras más simples. Pueden existir dos átomos de un mismo elemento con características distintas y, en el caso de que estos posean número másico distinto, pertenecen al mismo elemento pero en lo que se conoce como uno de sus isótopos. También es importante diferenciar entre los «elementos químicos» de una sustancia simple. Los elementos se encuentran en la tabla periódica de los elementos.  El ozono (O3) y el dioxígeno (O2) son dos sustancias simples, cada una de ellas con propiedades diferentes. Y el elemento químico que forma estas dos sustancias simples es el oxígeno (O).  Algunos elementos se han encontrado en la naturaleza y otros obtenidos de manera artificial, formando parte de sustancias simples o de compuestos químicos. Otros han sido creados artificialmente en los aceleradores de partículas o en reactores atómicos. Estos últimos suelen ser inestables y solo existen durante milésimas de segundo. A lo largo de la historia del universo se han ido generando la variedad de elementos químicos a partir de nucleosíntesis en varios procesos, fundamentalmente debidos a estrellas.  Los nombres de los elementos químicos son nombres comunes y como tales deben escribirse sin mayúscula inicial, salvo que otra regla ortográfica lo imponga.  Elementos químicos de la tabla periódica Los elementos químicos se encuentran clasificados en la tabla periódica de los elementos. A continuación se detallan los elementos conocidos, ordenados por su número atómico.  Elemento 118 El descubrimiento del elemento 118 por un equipo del Lawrence Berkeley National Laboratory entre los años 2009 y 2010 fue más tarde revocado porque no fue posible repetir tal experimento. Sin embargo científicos rusos en el año 2006 publicaron su síntesis y este resultado no ha sido cuestionado por otros científicos.[7]  [8]  Procedencia de los nombres de los elementos químicos Los nombres de los elementos proceden de sus nombres en griego, latín, inglés o llevan el nombre de su descubridor o ciudad en que se descubrieron.  Relación entre los elementos y la tabla periódica La relación que tienen los elementos con la tabla periódica es que la tabla periódica contiene los elementos químicos en una forma ordenada de acuerdo a su número atómico, estableciendo más de 118 elementos conocidos. Algunos se han encontrado en la naturaleza, formando parte de sustancias simples o compuestos químicos. Otros han sido creados artificialmente en los aceleradores de partículas o en reactores atómicos. Estos últimos son inestables y solo existen durante milésimas de segundo.  Conceptos básicos El descubrimiento de los elementos Clave de colores:       Antes del 1500 (13 elementos): Antigüedad y Edad Media.      1500-1800 (+21 elementos): casi todos en el Siglo de las Luces.      1800-1849 (+24 elementos): revolución científica y revolución industrial.      1850-1899 (+26 elementos): gracias a la espectroscopia.      1900-1949 (+13 elementos): gracias a la teoría cuántica antigua y la mecánica cuántica.      1950-2000 (+17 elementos): elementos ""postnucleares"" (del nº at. 98 en adelante) por técnicas de bombardeo.      2001-presente (+4 elementos): por fusión nuclear.   Metales, no metales y metaloides La primera clasificación de elementos conocida fue propuesta por Antoine Lavoisier, quien propuso que los elementos se clasificaran en metales, no metales y metaloides o metales de transición. Aunque muy práctico y todavía funcional en la tabla periódica moderna, fue rechazada debido a que había muchas diferencias en las propiedades físicas como químicas.  Metales La mayor parte de los elementos metálicos exhibe el lustre brillante que asociamos a los metales. Los metales conducen el calor y la electricidad, son maleables (se pueden golpear para formar láminas delgadas) y dúctiles (se pueden estirar para formar alambres). Todos son sólidos a temperatura ambiente con excepción del mercurio (punto de fusión =–39 °C), que es un líquido. Dos metales se funden ligeramente por encima de la temperatura ambiente: el cesio a 28.4 °C y el galio a 29.8 °C. En el otro extremo, muchos metales se funden a temperaturas muy altas. Por ejemplo, el cromo se funde a 1900 °C.  Los metales tienden a tener energías de ionización bajas y por tanto se oxidan (pierden electrones) cuando sufren reacciones químicas. Los metales comunes tienen una relativa facilidad de oxidación. Muchos metales se oxidan con diversas sustancias comunes, incluidos O2 y los ácidos.  Se utilizan con fines estructurales, fabricación de recipientes, conducción del calor y la electricidad. Muchos de los iones metálicos cumplen funciones biológicas importantes: hierro, calcio, magnesio, sodio, potasio, cobre, manganeso, zinc, cobalto, molibdeno, cromo, estaño y vanadio.  Hay muchos metales como:  Hierro (Fe). Llamado también fierro, es uno de los metales más abundantes de la corteza terrestre, que compone el corazón mismo del planeta, en donde se halla en estado líquido. Su propiedad más llamativa, aparte de su dureza y fragilidad, es su gran capacidad ferromagnética. A través de alearlo con carbono es posible obtener el acero.  Magnesio (Mg). Tercer elemento más abundante de la tierra, tanto en su corteza como disuelto en los mares, jamás se presenta en la naturaleza en estado puro, sino como iones en sales. Es indispensable para la vida, aprovechable para aleaciones y altamente inflamable.  Oro (Au). Un metal precioso de color amarillo, blando, brillante, que no reacciona con la mayoría de las sustancias químicas excepto con el cianuro, el mercurio, el cloro y la lejía. A lo largo de la historia jugó un papel vital en la cultura económica humana, como símbolo de la riqueza y respaldo de las monedas.  Plata (Ag). Otro de los metales preciosos, es blanco, brillante, dúctil y maleable, se halla en la naturaleza como parte de diversos minerales o como pencas puras del elemento, ya que es muy común en la corteza terrestre. Es el mejor conductor de calor y electricidad que se conoce.  Aluminio (Al). Metal muy ligero, no ferromagnético, el tercero más abundante de la corteza terrestre. Es muy valorado en los oficios industriales y siderúrgicos, ya que a través de aleaciones puede obtenerse variantes de mayor resistencia pero que conserven su versatilidad. Posee una baja densidad y muy buena resistencia a la corrosión.  Níquel (Ni). Metal blanco muy dúctil y muy maleable, buen conductor de electricidad y calor, además de ser ferromagnético. Es uno de los metales densos, junto con el iridio, osmio y el hierro. Es vital para la vida, pues forma parte de numerosas enzimas y proteínas.  Zinc (Zn). Se trata de un metal de transición parecido al cadmio y al magnesio, empleado a menudo en procesos de galvanización, es decir, recubrimiento protector de otros metales. Es muy resistente a la deformación plástica en frío, por lo que se le trabaja por encima de los 100 °C.  Plomo (Pb). El único elemento capaz de detener la radiactividad es el plomo. Es un elemento muy particular, dada su flexibilidad molecular única, facilidad de fundición y resistencia relativa a ácidos fuertes como el sulfúrico o el clorhídrico.  Estaño (Sn). Metal pesado y de fácil oxidación, empleado en muchas aleaciones para brindar resistencia a la corrosión. Cuando se lo dobla, produce un sonido muy característico que se ha bautizado como el “grito del estaño”.  Sodio (Na). El sodio es un metal alcalino blando, plateado, presente en la sal marina y en el mineral llamado halita. Es sumamente reactivo, oxidable y posee una reacción exotérmica violenta cuando se lo mezcla con agua. Es uno de los componentes vitales de los organismos vivos conocidos.  No metales Los no metales varían mucho en su apariencia, no son lustrosos y por lo general son malos conductores del calor y la electricidad. Sus puntos de fusión son más bajos que los de los metales (aunque el diamante, una forma de carbono, se funde a 700 °C en condiciones normales de presión y temperatura). Varios no metales existen en condiciones ordinarias como moléculas diatómicas. En esta lista están incluidos cinco gases (H2, N2, O2, F2 y Cl2), un líquido (Br2) y un sólido volátil (I2). El resto de los no metales son sólidos que pueden ser duros como el diamante o blandos como el azufre. Al contrario de los metales, son muy frágiles y no pueden estirarse en hilos ni en láminas. Se encuentran en los tres estados de la materia a temperatura ambiente: son gases (como el oxígeno), líquidos (bromo) y sólidos (como el carbono). No tienen brillo metálico y no reflejan la luz. Muchos no metales se encuentran en todos los seres vivos: carbono, hidrógeno, oxígeno, nitrógeno, fósforo y azufre en cantidades importantes. Otros son oligoelementos: flúor, silicio, arsénico, yodo, cloro.  Comparación de los metales y no metales Metales  No Metales  Localización en la tabla periódica Metales  Corresponde a los elementos situados a la izquierda y centro de la Tabla Periódica (Grupos 1 (excepto hidrógeno) al 12, y en los siguientes se sigue una línea quebrada que, aproximadamente, pasa por encima de Aluminio (Grupo 13), Germanio (Grupo 14), Antimonio (Grupo 15) y Polonio (Grupo 16) de forma que al descender aumenta en estos grupos el carácter metálico).  No Metales  Elementos químicos y número atómico Un elemento químico es cada una de las formas fundamentales de la materia, es decir es una sustancia pura (o especie química definida). Se presenta siempre como átomos de un mismo y único tipo, y que por lo tanto no pueden ser descompuestas en sustancias más simples todavía, formada por átomos que tienen el mismo número atómico, es decir, el mismo número de protones; lo que se distinguen de los demás en su naturaleza y sus propiedades fundamentales. Por ejemplo: el elemento oro tiene unas propiedades que son diferentes a las del elemento hierro o el elemento oxígeno. Los elementos químicos se expresan usualmente mediante símbolos distintos para cada uno.  Símbolo químico Los símbolos químicos son abreviaciones o signos que se utilizan para identificar los elementos y compuestos químicos. Algunos elementos de uso frecuente y sus símbolos son: carbono, C; oxígeno, O; nitrógeno, N; hidrógeno, H; cloro, Cl; azufre, S; magnesio, Mg; aluminio, Al; cobre, Cu; argón, Ar; oro, Au; hierro, Fe; plata, Ag; platino, Pt. Fueron propuestos en 1814 por Agustín en reemplazo de los símbolos alquímicos y los utilizados por Dalton en 1808 para explicar su teoría atómica.  La mayoría de los símbolos químicos se derivan de las letras del nombre del elemento, principalmente en latín, pero a veces en inglés, alemán, francés o ruso. La primera letra del símbolo se escribe con mayúscula, y la segunda (si la hay) con minúscula. Los símbolos de algunos elementos conocidos desde la antigüedad, proceden normalmente de sus nombres en latín. Por ejemplo, Cu de cuprum (cobre), Ag de argentum (plata), Au de aurum (oro) y Fe de ferrum (hierro). Este conjunto de símbolos que denomina a los elementos químicos es universal. Los símbolos de los elementos pueden ser utilizados como abreviaciones para nombrar al elemento, pero también se utilizan en fórmulas y ecuaciones para indicar una cantidad relativa fija del mismo. El símbolo suele representar un átomo del elemento en una molécula u otra especie química. Sin embargo, los átomos tienen unas masas fijas, "
quimica,"La masa atómica  es la masa de un átomo, más frecuentemente expresada en unidades de masa atómica unificada.[1] La masa atómica es usada a veces incorrectamente como un sinónimo de masa atómica relativa, masa atómica media y peso atómico; estos últimos difieren sutilmente de la masa atómica. *La masa atómica está definida como la masa de un átomo, que solo puede ser de un isótopo a la vez, y no es un promedio ponderado en las abundancias de los isótopos.* En el caso de muchos elementos que tienen un isótopo dominante, la similitud/diferencia numérica real entre la masa atómica del isótopo más común y la masa atómica relativa o peso atómico estándar puede ser muy pequeña, tal que no afecta a muchos cálculos bastos, pero tal error puede ser crítico cuando se consideran átomos individuales.   El peso atómico estándar se refiere a la media de las masas atómicas relativas de un elemento en el medio local de la corteza terrestre y la atmósfera terrestre, como está determinado por la Commission on Atomic Weights and Isotopic Abundances (Comisión de Pesos Atómicos y Abundancias Isotópicas) de la IUPAC.[2] Estos valores son los que están incluidos en una tabla periódica estándar, y es lo que es más usado para los cálculos ordinarios. Se incluye una incertidumbre en paréntesis que frecuentemente refleja la variabilidad natural en la distribución isotópica, en vez de la incertidumbre en la medida.[3] Para los elementos sintéticos, el isótopo formado depende de los medios de síntesis, por lo que el concepto de abundancia isotópica natural no tiene sentido. En consecuencia, para elementos sintéticos, el conteo total de nucleones del isótopo más estable (esto es, el isótopo con la vida media más larga) está listado en paréntesis en el lugar del peso atómico estándar. El litio representa un caso único, donde la abundancia natural de los isótopos ha sido perturbada por las actividades humanas al punto de afectar la incertidumbre en su peso atómico estándar, incluso en muestras obtenidas de fuentes naturales, como los ríos.  La masa atómica relativa es un sinónimo para peso atómico y está cercanamente relacionado con la masa atómica promedio (pero no es un sinónimo de masa atómica), la media ponderada de las masas atómicas de todos los átomos de un elemento químico encontrados en una muestra particular, ponderados por abundancia isotópica.[4] Esto es usado frecuentemente como sinónimo para peso atómico relativo, y este uso no es incorrecto, dado que los pesos atómicos estándar son masas atómicas relativas, aunque es menos específico. La masa atómica relativa también se refiere a ambientes no terrestres y ambientes terrestres altamente específicos que se desvían de la media o tienen diferentes certidumbres (número de cifras significativas) que los pesos atómicos estándar.  La masa isotópica relativa es la masa relativa de un isótopo dado (más específica, cualquier núclido solo), escalado con el carbono-12 como exactamente 12. No hay otros núclidos distintos al carbono-12 que tengan exactamente un número entero de masas en esta escala. Esto es debido a dos factores:  Los protones y neutrones del núcleo representan casi toda la masa total de los átomos, con los electronesy la energía de enlace nuclear haciendo contribuciones menores.[5] Por lo tanto, el valor numérico de la masa atómica cuando se expresa en daltons tiene casi el mismo valor que el número de masa . La conversión entre masa en kilogramos y masa en daltons se puede hacer usando la constante de masa atómica      m   u    =    m (      12   C   )   12    = 1     D a     {\displaystyle m_{\rm {u}}={{m({\rm {^{12}C}})} \over {12}}=1\ {\rm {Da}}}  .  La fórmula usada para la conversión es:[6][7]  donde      M   u      {\displaystyle M_{\rm {u}}}   es la constante de masa molar,      N   A      {\displaystyle N_{\rm {A}}}   es la constante de Avogadro[8] y     M  (  12    C  )   {\displaystyle M(^{12}\mathrm {C} )}   es la masa molar del carbono-12 determinada experimentalmente.[9]  Masa isotópica relativa La masa isotópica relativa (una propiedad de un solo átomo) no debe confundirse con la cantidad promedio peso atómico (véase arriba), que es un promedio de valores para muchos átomos en una muestra dada de un elemento químico.  Mientras que la masa atómica es una masa absoluta, la masa isotópica relativa es un número adimensional sin unidades. Esta pérdida de unidades resulta del uso de una relación de escala con respecto a un estándar de carbono-12, y la palabra ""relativa"" en el término ""masa isotópica relativa"" se refiere a esta escala ""relativa"" al carbono-12.  La masa isotópica relativa, entonces, es la masa de un isótopo dado (específicamente, cualquier núclido), cuando este valor se escala por la masa de carbono-12, donde este último tiene que ser determinado experimentalmente. De manera equivalente, la masa isotópica relativa de un isótopo o nucleido es la masa del isótopo relativa a 1/12 de la masa de un átomo de carbono-12.  Por ejemplo, la masa isotópica relativa de un átomo de carbono-12 es exactamente 12. En comparación, la masa atómica de un átomo de carbono-12 es exactamente 12 daltons. Alternativamente, la masa atómica de un átomo de carbono-12 puede expresarse en cualquier otra unidad de masa: por ejemplo, la masa atómica de un átomo de carbono-12 es 1,99264687992(60)×10−26 kg.  Como es el caso de la ""masa atómica"" relacionada cuando se expresa en daltons, los números de masa isotópicos relativos de nucleidos distintos del carbono-12 no son números enteros, pero siempre están cerca de los números enteros. Esto se discute completamente a continuación.  Defectos de masa en masas atómicas La cantidad que las masas atómicas se desvían de su número de masa es como sigue: la desviación empieza, positiva en el hidrógeno-1, disminuyendo hasta alcanzar un mínimo en el hierro-56, hierro-58 y níquel-62, luego aumenta a valores positivos en los isótopos más pesados, conforme aumenta el número atómico. Esto corresponde a lo siguiente: la fisión nuclear en un elemento más pesado que el hierro produce energía, y la fisión de cualquier elemento más ligero que el hierro requiere energía. Lo opuesto es verdadero para las reacciones de fusión nuclear: la fusión en los elementos más ligeros que el hierro produce energía, y la fusión en los elementos más pesados que el hierro requiere energía. La fusión de dos átomos de 4He producir berilio-8 requeriría energía, y el berilio se desmoronaría rápidamente de nuevo. 4He puede fusionarse con tritio (3H) o with 3He; estos procesos ocurrieron durante la nucleosíntesis del Big Bang. La formación de elementos con más de siete nucleones requiere la fusión de tres átomos de 4He en el proceso triple alfa, omitiendo litio, berilio y boro para producir carbono-12.  Aquí hay algunos valores de la relación entre la masa atómica y el número de masa:[10]  Medición de las masas atómicas El proceso que se siguió históricamente para determinar las masas reales de los átomos de los diferentes elementos fue similar al seguido en el modelo clips, trabajando inicialmente con gases y comparando las masas de gases situados en recipientes con las mismas condiciones de presión, volumen y temperatura: como las masas eran distintas, pero había el mismo número de partículas (de acuerdo con el modelo de materia y el principio de Avogadro), se debía a que las partículas tenían masas reales diferentes.Actualmente la comparación directa y medición de las masas de los átomos se logra con la utilización de un espectrómetro de masas.  Factor de conversión entre unidad de masa atómica y gramos La unidad científica estándar para manejar átomos en cantidades macroscópicas es el mol, que está definido arbitrariamente como la cantidad de sustancia que tiene tantos átomos u otra unidad como átomos hay en 12 gramos de carbono del isótopo C-12. El número de átomos en un mol es denominado número de Avogadro, cuyo valor es aproximadamente 6,022 x 1023 mol−1. Un mol de una sustancia siempre contiene exactamente la masa atómica relativa o masa molar de dicha sustancia, expresado en gramos; sin embargo, esto no es cierto para la masa atómica. Por ejemplo, el peso atómico estándar del hierro es 55,847 g/mol, y en consecuencia un mol de hierro como se suele encontrar en la Tierra tiene una masa de 55,847 gramos. La masa atómica del isótopo 56Fe es 55,935 u, y un mol de 56Fe pesará, en teoría, 55,935 g, pero no se ha encontrado tales cantidades puras de isótopo 56Fe en la Tierra.  La fórmula para la conversión entre unidad de masa atómica y la masa SI en gramos para un solo átomo es:  donde      M  u     {\displaystyle M_{u}}   es la  constante de masa molar y      N  A     {\displaystyle N_{A}}   es el número de Avogadro.  Relación entre masa atómica y masa molecular Se aplican definiciones similares a las moléculas. Se puede calcular la masa molecular de un compuesto por adición de las masas atómicas-moleculares de sus átomos constituyentes (núclidos). También se puede calcular la masa molar indefinida por la adición de las masas atómicas relativas de los elementos dados en la fórmula molecular. En ambos casos, la multiplicidad de los átomos (el número de veces que está presente) debe ser tomado en cuenta, generalmente multiplicando cada masa única por su multiplicidad inversa.  Historia En la historia de la química, los primeros científicos en determinar los pesos atómicos fueron John Dalton entre 1803 y 1808, y Jöns Jakob Berzelius entre 1808 y 1826. Los pesos atómicos fueron definidos originalmente en relación con el elemento hidrógeno, el más ligero, tomándolo como 1, y en 1820, la hipótesis de Prout indicaba que las masas atómicas de todos los elementos deberían ser un múltiplo entero del peso del hidrógeno. Sin embargo, Berzelius pronto probó que esta hipótesis no siempre se sostenía, y en algunos casos, como el cloro, el peso atómico caía casi exactamente entre dos múltiplos del peso del hidrógeno. Posteriormente, se mostró que esto se debía a un efecto causado por los isótopos, y que la masa atómica de los isótopos puros, o núclidos, era múltiplo de la masa del hidrógeno, en un margen de diferencia del 0,96%.  En la década de 1860, Stanislao Cannizzaro refinó los pesos atómicos aplicando la ley de Avogadro (en el Congreso de Karlsruhe de 1860). Formuló una ley para determinar los pesos atómicos de los elementos: las distintas cantidades del mismo elemento contenido en distintas moléculas son todas múltiplos enteros del peso atómico, y determinó los pesos atómicos y pesos moleculares comparando la densidad de vapor de un conjunto de gases con moléculas conteniendo uno o más del elemento químico en cuestión.[11]  A principios del siglo XX, hasta la década de 1960, los químicos y físicos utilizaban dos escalas de masa atómicas. Los químicos usaban una escala tal que la mezcla natural de isótopos de oxígeno tenía una masa atómica de 16, mientras que los físicos asignaron el mismo número 16 a la masa atómica del isótopo de oxígeno más común (que contiene ocho protones y ocho neutrones). Sin embargo, debido a que también están presentes en el oxígeno natural, tanto el oxígeno-17 como el oxígeno-18, esto conducía a 2 tablas diferentes de masas atómicas.[cita requerida] La escala unificada, basada en el carbono-12, 12C, cumplía el requerimiento de los físicos de basar la escala en un isótopo puro, a la vez que se hacía numéricamente cercana a la escala de los químicos. This was adopted as the 'unified atomic mass unit'. La recomendación principal actual del Sistema Internacional de Unidades (SI) para el nombre de esta unidad es el dalton y el símbolo 'Da'. El nombre 'unidad de masa atómica unificada' y el símbolo 'u' son nombres y símbolos reconocidos para la misma unidad.[12]  El término ""peso atómico"" se está eliminando gradualmente y se está reemplazando por ""masa atómica relativa"", en la mayoría de los usos actuales. Este cambio en la nomenclatura se remonta a la década de 1960 y ha sido fuente de mucho debate en la comunidad científica, que fue desencadenado por la adopción de la unidad de masa atómica unificada y la comprensión de que el peso era en cierto modo un término inapropiado. El argumento para mantener el término ""peso atómico"" fue principalmente que era un término bien entendido por aquellos "
quimica,"Se denomina isótopos  a los átomos de un mismo elemento, cuyos núcleos tienen una cantidad diferente de neutrones, y por lo tanto, difieren en número másico.[1]   La palabra isótopo (del griego: ἴσος isos 'igual, mismo'; τόπος tópos 'lugar', ""en mismo sitio"") se usa para indicar que todos los tipos de átomos de un mismo elemento químico (isótopos) se encuentran en el mismo sitio de la tabla periódica. Los átomos que son isótopos entre sí son los que tienen igual número atómico (número de protones en el núcleo), pero diferente número másico (suma del número de neutrones y el de protones en el núcleo). Los distintos isótopos de un elemento difieren, pues, en el número de neutrones.[1]  La mayoría de los elementos químicos tienen más de un isótopo. Solamente 8 elementos (por ejemplo berilio o sodio) poseen un solo isótopo natural. En contraste, el estaño es el elemento con más isótopos estables, 10.   Otros elementos tienen isótopos naturales, pero inestables, como el uranio, cuyos isótopos pueden transformarse o decaer en otros isótopos más estables, emitiendo en el proceso radiación, por lo que se dice que son radiactivos.[2]   Los isótopos inestables son útiles para estimar la edad de una gran variedad de muestras naturales, como rocas y materia orgánica. Esto es posible, siempre y cuando se conozca el ritmo promedio de desintegración de determinado isótopo, en relación con los que ya han decaído.[3] Gracias a este método de datación, se puede estimar la edad de la Tierra.[4]  Tipos de isótopos Todos los isótopos de un  elemento tienen el mismo número atómico pero difieren en lo que actualmente se conoce como número másico.  Si la relación entre el número de protones y de neutrones no es la apropiada para obtener la estabilidad nuclear, el isótopo es radiactivo.  Por ejemplo, en la naturaleza el carbono se presenta como una mezcla de tres isótopos con números másicos 12, 13 y 14: 12C, 13C y 14C. Sus abundancias respecto a la cantidad global de carbono son respectivamente 98,89 %, 1,11 % y trazas.  Los isótopos se subdividen en isótopos estables (existen menos de 300) y no estables o isótopos radiactivos (existen alrededor de 1200). El concepto de estabilidad no es exacto, ya que existen isótopos casi estables. Su estabilidad se debe al hecho de que, aunque son radiactivos, tienen un periodo de semidesintegración extremadamente largo comparado con la edad de la Tierra.   Notación Inicialmente los nombres de los isótopos de cada elemento que se iban descubriendo recibieron nombres propios diferentes al del elemento al que pertenecían. Así cuando se descubrieron tres isótopos del hidrógeno, recibieron los nombres de protio, deuterio y tritio. El núcleo del protio consta de un protón, el del deuterio de un protón y un neutrón, y el del tritio de un protón y dos neutrones.  Cuando se siguieron descubriendo isótopos de casi todos los elementos se vio que serían necesarios cientos o miles de nombres y se cambió el sistema de nomenclatura. Actualmente cada isótopo se representa con el símbolo del elemento al que pertenece, colocando como subíndice a la izquierda su número atómico (número de protones en el núcleo), y como superíndice a la izquierda su número másico (suma del número de protones y de neutrones). Así los isótopos del hidrógeno protio, deuterio y tritio se denotan 11H, 21H y 31H, respectivamente.  Como todos los isótopos de un mismo elemento tienen el mismo número atómico, que es el orden en la tabla periódica, y el mismo símbolo, habitualmente se omite el número atómico. Así para los isótopos del hidrógeno escribiremos 1H, 2H y 3H. Esto se hace porque todos los isótopos de un elemento particular se comportan de la misma manera en cualquier reacción química. Por ejemplo, un átomo del escaso isótopo de oxígeno que tiene número másico 18, se combinará exactamente igual con dos átomos de hidrógeno para formar agua que si se tratara del abundante átomo de oxígeno de número másico 16. Sin embargo cuando se están describiendo reacciones nucleares es útil tener el número atómico como referencia.   En el caso de textos no científicos, como textos periodísticos, esta notación con subíndices y superíndices es incómoda, por lo que también se usa una notación consistente en el nombre del elemento unido por un guion al número másico del isótopo de que se trate. De esta forma los isótopos del hidrógeno 11H, 21H y 31H, también se pueden nombrar como hidrógeno-1, hidrógeno-2 e hidrógeno-3 respectivamente.  Estas son las reglas de nomenclatura científicamente aceptadas, correspondientes a la Nomenclatura de Química Inorgánica. Recomendaciones de 2005 (Libro Rojo de la IUPAC), tal y como se pueden encontrar en su sección IR-3.3.  Hay que recordar que los nombres de los elementos químicos son nombres comunes y como tales deben escribirse sin mayúscula inicial, salvo que otra regla ortográfica lo imponga.  Radioisótopos Los radioisótopos son isótopos radiactivos ya que tienen un núcleo atómico inestable y emiten energía y partículas cuando se transforman en un isótopo diferente más estable. La desintegración puede detectarse con un contador Geiger o con una película fotográfica.   La principal razón de la inestabilidad está en el exceso de protones o neutrones. La fuerza nuclear fuerte, que une protones y neutrones entre sí, requiere que la cantidad de neutrones y protones esté cerca de cierta relación. Cuando el número de neutrones es superior al que requiere esta relación el átomo puede presentar decaimiento beta negativo. Cuando el átomo tiene un exceso de protones (defecto de neutrones) suele presentar decaimiento beta positivo.   Esto sucede porque la fuerza nuclear fuerte residual depende de la proporción de neutrones y protones. Si la relación está muy sesgada hacia uno de los extremos la fuerza nuclear débil responsable del decaimiento beta puede producir esporádicamente la pérdida de algún nucleón. Para números atómicos elevados (Z > 80) también se vuelve frecuente la desintegración alfa (que casi es mucho más frecuente cuando además hay exceso de protones).  Cada radioisótopo tiene un periodo de semidesintegración o semivida característico. La energía puede ser liberada principalmente en forma de radiación alfa (partículas constituidas por núcleos de helio), beta (partículas formadas por electrones o positrones) o gamma (energía en forma de radiación electromagnética).  Varios isótopos  radiactivos inestables y artificiales tienen usos en técnicas de radioterapia en medicina. Por ejemplo, un isótopo del tecnecio (99mTc, la ""m"" indica que es un isómero nuclear metaestable) puede usarse para identificar vasos sanguíneos bloqueados.   Varios isótopos radiactivos naturales se usan en datación radiométrica para determinar cronologías, por ejemplo, arqueológicas.  Aplicaciones de los isótopos Las siguientes son varias de las aplicaciones de diferentes isótopos en diversas áreas, como la medicina:  Utilización de las propiedades químicas En el marcado isotópico, se usan isótopos inusuales como marcadores de reacciones químicas. Los isótopos añadidos reaccionan químicamente igual que los que están presentes en la reacción, pero después se pueden identificar por espectrometría de masas o espectroscopia infrarroja. Si se usan radioisótopos, se pueden detectar también gracias a las radiaciones que emiten. Los procesos de separación isotópica o enriquecimiento isotópico representan un desafío"
quimica,"En química, el número másico o número de masa es la suma del número de protones y el número de neutrones del núcleo de un átomo. Se simboliza con la letra A (el uso de esta letra proviene de alemán Atomgewicht, que quiere decir peso atómico, aunque sean conceptos distintos que no deben confundirse). Suele ser mayor que el número atómico, dado que los neutrones del núcleo proporcionan a este la cohesión necesaria para superar la repulsión entre los protones.  El número másico es además el indicativo de los distintos isótopos de un elemento químico. Dado que el número de protones es idéntico para todos los átomos del elemento, solo el número másico, que lleva implícito el número de neutrones en el núcleo, indica de qué isótopo del elemento se trata. El número másico se indica con un superíndice situado a la izquierda de su símbolo, sobre el número atómico. Por ejemplo, el 1H es el isótopo de hidrógeno conocido como protio. El 2H es el deuterio y el 3H es el tritio. Dado que todos ellos son hidrógeno, el número atómico tiene que ser 1.  Para todo átomo o ion:  Número másico (A) = número atómico (Z) + número de neutrones(N)  A = Z + N  Para calcular la cantidad de neutrones que posee un átomo debe hacerse: ""A – Z"" (número másico menos número atómico), consultando antes en la tabla periódica las cantidades correspondientes."
quimica,"El peso atómico es una magnitud física adimensional, definida como la razón del promedio de las masas de los átomos de un elemento (de una muestra dada o fuente) con respecto a la doceava parte de la masa de un átomo de carbono-12 (conocida como una unidad de masa atómica unificada).  El concepto se utiliza generalmente sin mayor calificación para referirse al peso atómico estándar, que a intervalos regulares publica la Comisión sobre Abundancia de Isótopos y Pesos Atómicos de la IUPAC.[1][2] Se pretende que sean aplicables a materiales de laboratorios normales.  Consideraciones generales No debe olvidarse que la mayoría de los elementos presentan distintas formas isotópicas (con masas atómicas diferentes), por lo que a efectos prácticos de cuantificar con la precisión necesaria los componentes de una gran variedad de reacciones químicas, es necesario determinar con exactitud el peso atómico de cada elemento teniendo en cuenta la proporción presente de los citados isótopos. Esto explica que se trate de cifras decimales (al contrario que los números atómicos, que son cifras enteras), y que en ocasiones se faciliten en forma de intervalos (valor mínimo y valor máximo), dado que en función de la procedencia del elemento, su composición isotópica puede variar sensiblemente.  Los valores de estos pesos atómicos estándares están reimpresos en una amplia variedad de libros de texto, catálogos comerciales, pósteres, etc. Para describir esta cantidad física se puede usar también la expresión «masa atómica relativa». Desde por lo menos 1860[3] y hasta la década de 1960, el uso continuado de la locución ha suscitado una controversia considerable[4] (véase más adelante).  Cálculo:  A diferencia de las masas atómicas (las masas de los átomos individuales), los pesos atómicos no son constantes físicas. Varían de una muestra a otra. Sin embargo, en muestras «normales» son suficientemente constantes para ser de importancia fundamental en química. No se debe confundir al peso atómico con la masa molecular.  Definición La definición IUPAC[5] del peso atómico es:  En la definición, enfáticamente se especifica «un peso atómico…», puesto que, según sea la fuente, un elemento tiene diferentes pesos atómicos. Por ejemplo, debido a su diferente composición isotópica, el boro de Turquía tiene un peso atómico menor que el boro de California.[6][7] Sin embargo, dados el costo y las dificultades del análisis isotópico, es usual el uso de valores tabulados de pesos atómicos estándares, que son ubicuos en los laboratorios químicos.  En 1960 se introdujo una unidad denominada unidad de masa atómica, definida como 1/12 de la masa del carbono 12. Se representa con el símbolo u; de este modo, 12C = 12u. La tabla de los pesos atómicos relativos se basa ahora en la masa atómica de 12C = 12.  Así mismo, la masa molar de una molécula es la masa de un mol de esas moléculas (sus unidades en química son g/mol), se obtiene multiplicando la masa atómica relativa por la constante de masa molar. Por definición un mol es el número de átomos que están contenidos en exactamente 12 gramos de carbono de masa isotópica 12 (12C). A este número se le denomina número de Avogadro. El valor más exacto que se conoce hasta ahora de él es 6,0221367 × 1023.  Controversia en el nombre Entre los científicos el uso del nombre «peso atómico» ha generado gran controversia.[4] Comúnmente, quienes lo objetan prefieren la expresión «masa atómica relativa» (no confundir con masa atómica). La objeción básica es que el peso atómico no es propiamente peso, que es la fuerza ejercida en un objeto en un campo gravitatorio, medido en unidades de fuerza tales como el newton.  En respuesta, los que apoyan el concepto «peso atómico» opinan (entre otros argumentos)[4] que el nombre ha estado en uso continuo para la misma cantidad desde que el concepto fue establecido por primera vez por John Dalton en 1808.[8]  Podría agregarse que el peso atómico suele no ser verdaderamente «atómico» siquiera, puesto que no corresponde a un átomo individual. El mismo argumento puede plantearse contra el término «masa atómica relativa» cuando se usa en este sentido.  Determinación de los pesos atómicos Los pesos atómicos modernos se calculan a partir de valores medidos de masa atómica, de cada nucleido, según su composición isotópica. Hay disponibilidad de datos sumamente precisos de masas atómicas[9][10] de virtualmente todos los núclidos no radioactivos. Las composiciones isotópicas son más difíciles de medir a un alto grado de precisión, ya que están sujetas a variaciones entre muestras.[11][12]  Por esta razón los pesos atómicos de los veintidós elementos mononucleicos se conocen con una precisión especialmente alta, con incertidumbre de solo una parte en 38 millones en el caso del flúor: precisión mayor que el mejor valor actual de la constante de Avogadro: una parte en 20 millones.  Se ejemplifica el cálculo del silicio, cuyo peso atómico es especialmente importante en metrología. En la naturaleza, de este elemento existe una mezcla de tres isótopos: 28Si, 29Si y 30Si.  Las masas atómicas de estos núclidos se conocen con una precisión de una parte en 14 000 000 000 (catorce mil millones) para el 28Si; de los restantes, una parte por 1 000 000 000 (mil millones). Sin embargo, el intervalo de abundancia natural de los isótopos es tal que la abundancia estándar está determinada hasta aproximadamente ±0,001 % (véase tabla adjunta).  El cálculo es:  La estimación de la incertidumbre es complicada,[13] especialmente dado que la distribución de la muestra no es necesariamente simétrica: los pesos atómicos estándares de la IUPAC están indicados con incertidumbres simétricas estimadas,[14] y el valor referente al silicio es 28,0855 (referencia 3). La incertidumbre estándar relativa en este valor es 1×10−5 o 10 ppm (partes por millón).  Relación de elementos por orden alfabético con sus pesos atómicos Como se ha indicado anteriormente, el peso atómico de cada elemento se calcula en función de la proporción en la que sus isótopos estables aparecen en la corteza terrestre (es decir, para cada isótopo se agrega el peso de sus protones y el peso de sus neutrones, y se calcula la media ponderada en función de la abundancia en la que aparece cada isótopo). Así, elementos con un solo isótopo estable, o con un determinado isótopo en una proporción muy preponderante, tienen su correspondiente peso atómico único. En cambio, no es posible establecer el peso atómico de aquellos elementos que no poseen isótopos estables (por ser radiactivos), que en la tabla siguiente aparecen con el signo «(-)».  Desde el año 2010, la IUPAC decidió atribuir un intervalo de pesos atómicos a 10 elementos (hidrógeno, litio, boro, carbono, nitrógeno, oxígeno, silicio, azufre, cloro y talio),[15] (posteriormente se ha ampliado esta lista hasta 12, añadiendo el bromo y el magnesio) reflejando de forma más precisa cómo estos elementos se hallan en la naturaleza. Por ejemplo, para el azufre se utilizaba anteriormente el peso atómico 32,065; con la nueva tabla, se indica que este valor puede estar entre [32,05 y 32,08], en función de la procedencia de la muestra con la que se esté trabajando.  Tabla periódica con pesos atómicos En correspondencia con el listado anterior, se incluye a continuación la tabla periódica (IUPAC; año 2016) con la relación de elementos químicos (del 1 al 118) y sus correspondientes pesos atómicos[16] expresados en Unidades de Masa Atómica:    En esta tabla, sustituida por la anterior, cada elemento aparece con un único peso atómico en UMA (entonces no se habían introducido los intervalos de peso)..  "
quimica,"La masa molar (símbolo M) de sustancia dada es una propiedad física definida como su masa por unidad de cantidad de sustancia.[1] Su unidad de medida en el SI es kilogramo por mol (kg/mol o kg·mol−1). Sin embargo, por razones históricas, la masa molar es expresada casi siempre en gramos por mol (g/mol).  Elementos La masa molar de los átomos de un elemento está dado por el peso atómico de cada elemento[2] multiplicado por la constante de masa molar, Mu = 1×10−3 kg/mol = 1 g/mol.[3] Su valor numérico coincide con el de la masa molecular, pero expresado en gramos/mol en lugar de unidades de masa atómica (u), y se diferencia de ella en que mientras la masa molecular alude una sola molécula, la masa molar corresponde a un mol (6,022×1023) de moléculas. Ejemplos:  La multiplicación por la constante de masa molar asegura que el cálculo es dimensionalmente correcto: los pesos atómicos son cantidades adimensionales (i. e. números puros, sin unidades) mientras que las masas molares tienen asociada una unidad asociada a una magnitud física (en este caso, g/mol).  Usualmente algunos elementos son encontrados en forma molecular, como el hidrógeno (H2), azufre (S8), cloro (Cl2), etc. La masa molar de las moléculas homonucleares es el número de átomos en cada molécula multiplicado por el peso atómico del elemento constante, multiplicado por la constante de masa molar (Mu). Ejemplos:  Compuestos La masa molar de un compuesto está dada por la suma de los pesos atómicos estándar de los átomos que forman el compuesto, multiplicado por la constante de masa molar (Mu). Ejemplo:  Se puede definir una masa molar promedio para mezclas de compuestos.[1] Esto es particularmente importante en la ciencia de polímeros, donde moléculas de un polímero pueden tener distinto número de monómeros (polímeros no uniformes).[4][5]  Mezclas La masa molar promedio de mezclas        M ¯      {\displaystyle {\bar {M}}}   pueden ser calculados mediante las fracciones molares (xi) de los compuestos y sus masas molares (Mi) como sigue:  También puede ser calculado a partir de la fracción de masa (wi) de los compuestos:  Por ejemplo, la masa molar promedio del aire seco es 28,97 g/mol.  Cantidades relacionadas La masa molar está fuertemente relacionada con la masa molar relativa (Mr) de un compuesto y con las masas atómicas estándar de los elementos constituyentes. Sin embargo, debe ser distinguida de la masa molecular, la cual es la masa de una molécula de una composición isotópica particular, y no está directamente relacionada con la masa atómica, que es la masa de un átomo de cierto isótopo. El dalton (Da), es usado a veces como unidad de la masa molar, especialmente en bioquímica, con la definición 1 Da = 1 g/mol, a pesar del hecho de que estrictamente es una constante de masa (1 Da = 1 u = 1,660 538 921(73)×10−27 kg).[6][3]  El peso molecular (P.M.) es un antiguo término para lo que ahora se llama, más correctamente, masa molar relativa (Mr).[7] Es una cantidad adimensional igual a la masa molar dividida por la constante de masa molar. La definición técnica es que la masa molar relativa es una masa molar medida en una escala donde la masa molar de un átomo no enlazado de carbono-12, en reposo y en su estado fundamental, es 12. La primera definición es equivalente a la completa, debido a la manera en que está definida la constante de masa molar.  Masa molecular La masa molecular (m) es la masa de determinada molécula: se mide en daltons (Da) o unidad de masa atómica unificada (u).[6] Moléculas diferentes de un mismo compuesto pueden tener masas moleculares distintas debido a que este puede contener diferentes isótopos de un mismo elemento. La masa molar es una medida del promedio de la masa molecular de todas las moléculas de una muestra, y generalmente es la medida más apropiada para trabajar con cantidades macroscópicas (que pueden ser pesadas) de una sustancia.  La masas moleculares se calculan a partir de las masas atómicas relativas[8] de cada nucleido, mientras que las masas molares se calculan a partir del peso atómico de cada elemento. El peso atómico considera la distribución isotópica de cada elemento en una muestra dada (habitualmente se asume que es ""normal""). Por ejemplo, el agua tiene una masa molar de 18,015 3(3) g/mol; sin embargo, moléculas individuales de agua tienen masas entre 18,010 564 686 3(15) u y 22,027 736 4(9) u, pertenecientes a las composiciones isotópicas 1H216O y ²H218O, respectivamente.  La distinción entre masa molar y masa molecular es importante debido a que las masas moleculares relativas pueden medirse directamente mediante espectometría, a menudo con una precisión de pocas partes por millón. Esta precisión es suficiente para determinar directamente la fórmula química de una molécula.[9]  Usos en la síntesis de ADN El término peso atómico tiene un significado específico cuando se utiliza en el contexto de la síntesis de ADN:   El término peso fórmula (F.W.) tiene un significado específico cuando se utiliza en el contexto de la síntesis del ADN: mientras que una nucleobase fosforamidita individual que ha de añadirse a un polímero de ADN cuenta con grupos protectores y tiene su peso molecular estimado que incluye estos grupos, la cantidad de peso molecular añadida finalmente por esta nucleobase a un polímero de ADN se denomina peso fórmula, es decir, el peso molecular de esta nucleobase dentro del polímero de ADN menos los grupos protectores.  Precisión e incertidumbres La precisión con que se conoce cada masa molar depende de la precisión de los pesos atómicos con los que es calculada. Se conoce la mayoría de los pesos atómicos a una precisión de al menos una parte en 10 000, siendo esta a menudo mucho mejor.[2] Sin embargo, el peso atómico del litio es una notable y seria excepción.[10] La precisión es adecuada para casi todos los usos químicos normales: es más preciso que la mayoría de los análisis químicos, y supera la pureza de la mayoría de los reactivos de laboratorio.  La precisión de los pesos atómicos, y por ende de las masas molares, está limitado por el conocimiento de la distribución isotópica del elemento. Si se requiere de un valor más preciso, es necesario determinar la distribución isotópica de la muestra investigada, la cual puede ser diferente de la distribución estándar usada para calcular el peso atómico estándar. Las distribuciones isotópicas de elementos diferentes de una muestra no son necesariamente independientes entre sí: por ejemplo, una muestra que ha sido destilada estará enriquecida en el isótopo más liviano de todos los elementos presentes. Esto complica el cálculo de la incertidumbre estándar de la masa molar.  Una útil convención para el trabajo común de laboratorio es poner entre paréntesis dos posiciones decimales de las masas molares para todos los cálculos. Esto es más riguroso de lo que usualmente es requerido, pero evita caer en errores de redondeo mediante los cálculos. Cuando la masa molar es mayor a 1000 g/mol, raramente es apropiado usar más de una posición decimal. Estas convenciones son seguidas en la mayoría de los valores tabulados de masas molares.[11]  Medición Las masas molares casi nunca se miden directamente. En vez de eso, pueden ser calculadas a partir de las masas atómicas estándar, las cuales son listadas con frecuencia en catálogos de química y fichas de datos de seguridad (FDS). Las masas molares típicamente varían entre:  Si bien las masas molares son casi siempre, en la práctica, calculadas a partir de los pesos atómicos, también pueden ser medidas en ciertos casos. Tales mediciones son mucho menos precisas que las mediciones modernas de espectromía de masas utilizadas para medir los pesos atómicos, y prácticamente solo son de interés histórico. Todos los procedimientos confían en las propiedades coligativas, y cualquier disociación del compuesto debe ser tomada en cuenta.  Densidad de vapor La medición de la masa molar por densidad de vapor confía en el principio, enunciado originalmente por Amedeo Avogadro, que iguales volúmenes de gases, bajo idénticas condiciones, contienen igual cantidad de partículas. Este principio se incluye en la ley de los gases ideales:  donde n es la cantidad de sustancia. La densidad de vapor (ρ) está dada en términos de:  Combinando estas dos ecuaciones se obtiene la expresión para la masa molar en términos de la densidad de vapor para condiciones conocidas de presión y temperatura:  Descenso crioscópico El punto de congelación de una disolución es inferior que el del solvente puro, y el descenso crioscópico (ΔT) es directamente proporcional a la molaridad de la disolución. Cuando la composición está expresada como molalidad, la constante proporcional es conocida como la constante crioscópica (Kf) y es característica para cada solvente. Si w representa el concentración porcentual en peso de un soluto en disolución, y suponiendo que el soluto no está disuelto, la masa molar está dada por:  Aumento ebulloscópico El punto de ebullición de una disolución de un soluto no volátil es mayor que el de un solvente puro, y el aumento ebulloscópico (ΔT) es directamente proporcional a la molaridad de las disoluciones. Cuando la concentración está expresada en molalidad, la constante de proporcionalidad es conocida como constante ebulloscópica (Kb) y es característica para cada solvente. Si w representa la concentración porcentual en peso de una disolución, y suponiendo que soluto no está disuelto, "
quimica,"Un compuesto químico es una sustancia formada por la combinación química de dos o más elementos de la tabla periódica.[1] Los compuestos son representados por una fórmula química. Por ejemplo, el agua (H2O) está constituida por dos átomos de hidrógeno y uno de oxígeno. Los elementos de un compuesto no se pueden dividir ni separar por procesos físicos (decantación, filtración, destilación), sino solo mediante procesos químicos.   Los compuestos están formados por moléculas o iones con enlaces estables que no obedece a una selección humana arbitraria. Por lo tanto, no son mezclas  o aleaciones como el bronce o el chocolate.[2][3] Un elemento químico unido a un elemento químico idéntico no es un compuesto químico, ya que solo está involucrado un elemento, no dos elementos diferentes.  Hay cuatro tipos de compuestos, dependiendo de cómo se mantienen unidos los átomos constituyentes:  Muchos compuestos químicos tienen un identificador numérico único asignado por el Chemical Abstracts Service (CAS): su número CAS.  Fórmula En química inorgánica los compuestos se representan mediante fórmulas químicas.[4]  Una fórmula química es una forma de expresar información sobre las proporciones de los átomos que constituyen un compuesto químico en particular, utilizando las abreviaturas normalizadas de los elementos químicos y subíndices para indicar el número de átomos involucrados. Por ejemplo, el agua se compone de dos átomos de hidrógeno unidos a uno de oxígeno átomo: la fórmula química es H2O. En el caso de compuestos no estequiométricos, las proporciones pueden ser reproducibles con respecto a su preparación y dar proporciones fijas de sus elementos componentes, pero proporciones que no son integrales [por ejemplo, para el hidruro de paladio, PdH x (0.02 <x <0.58 )].[5]  El orden de los elementos en la fórmula de los compuestos inorgánicos comienza por la izquierda con el elemento menos electronegativo, hasta la derecha con el más electronegativo. Por ejemplo en el NaCl, el cloro que es más electronegativo que el sodio va en la parte derecha.[6] Para los compuestos orgánicos existen otras varias reglas y se utilizan fórmulas esqueletales o semidesarrolladas para su representación.[7]  Definiciones Cualquier sustancia que consista en dos o más tipos diferentes de átomos (elementos químicos) en una proporción estequiométrica fija puede denominarse compuesto químico. El concepto se entiende mejor cuando se consideran sustancias químicas puras.[8][9][10]  De la composición de proporciones fijas de dos o más tipos de átomos se desprende que los compuestos químicos se pueden convertir, mediante una reacción química, en compuestos o sustancias, cada uno con menos átomos.  Los compuestos químicos tienen una estructura química única y definida que se mantiene unida en una disposición espacial definida por enlaces químicos. Los compuestos químicos pueden ser compuestos moleculares, mantenidos juntos por enlaces covalentes, salesmantenidas entre sí por enlaces iónicos, compuestos intermetálicos mantenidos juntos por enlaces metálicos, o el subconjunto de complejos químicos que se mantienen unidos por enlaces covalentes coordinados .[11] Los elementos químicos puros generalmente no se consideran compuestos químicos, ya que no cumplen con el requisito de dos o más átomos, aunque a menudo consisten en moléculas compuestas de múltiples átomos (como en la molécula diatómica H2, o la molécula poliatómica S8, etc.)[11] Muchos compuestos químicos tienen un identificador numérico único asignado por el Chemical Abstracts Service (CAS): su número CAS.[12]   Hay nomenclatura variable y a veces inconsistente para diferenciar sustancias, que incluyen ejemplos verdaderamente no estequiométricos de los compuestos químicos, que requieren que las proporciones sean fijas. Muchas sustancias químicas sólidas, por ejemplo muchos minerales de silicato, no tienen fórmulas simples que reflejen el enlace químico de los elementos entre sí en proporciones fijas; aun así, estas sustancias cristalinas a menudo se denominan ""compuestos no estequiométricos"". Se puede argumentar que están relacionados con dichos productos, en lugar de ser compuestos químicos propiamente dichos, en la medida en que la variabilidad en sus composiciones a menudo se debe a la presencia de elementos extraños atrapados dentro de la estructura cristalina de un compuesto químico verdadero, o debido a perturbaciones en su estructura en relación con el compuesto conocido que surge debido a un exceso o déficit de los elementos constituyentes en lugares de su estructura; tales sustancias no estequiométricas forman la mayor parte de la corteza y el manto de la Tierra. Otros compuestos considerados químicamente idénticos pueden tener cantidades variables de isótopos pesados o ligeros de los elementos constituyentes, lo que cambia ligeramente la proporción en masa de los elementos.  Clasificación Se pueden clasificar de acuerdo al tipo de enlace químico o a su composición. Atendiendo al tipo de enlace químico, se pueden dividir en:  Por su composición, se pueden dividir en dos grandes grupos:[13]  Moléculas Una molécula es un grupo eléctricamente neutro de dos o más átomos unidos por enlaces químicos.[17][18][19][20][21] Una molécula puede ser homonuclear, es decir, estar formada por átomos de un mismo elemento químico, como ocurre con dos átomos en la molécula de oxígeno (O2); o puede ser heteronuclear, es decir, un compuesto químico compuesto por más de un elemento, como el agua (dos átomos de hidrógeno y un átomo de oxígeno; H2O).[22] Los átomos y complejos unidos por enlaces no covalentes como los enlaces de hidrógeno o los enlaces iónicos no se suelen considerar como moléculas individuales.  Compuestos iónicos Un compuesto iónico es un compuesto químico compuesto de anion que se mantienen unidos por fuerzas electrostáticas denominadas enlace iónico. El compuesto es neutro en general, pero consta de iones cargados positivamente llamados cationes y iones cargados negativamente llamados aniones. Estos pueden ser iones simples como el sodio(Na+) y el cloruro (Cl−) en el cloruro de sodio, o especies poliatómicas como el amonio (NH+4) y carbonato (CO2−3) en el carbonato de amonio.[23] Los iones individuales dentro de un compuesto iónico generalmente tienen múltiples vecinos más cercanos, por lo que no se consideran parte de moléculas, sino parte de una red tridimensional continua, generalmente en una estructura cristalina.[24]  Los compuestos iónicos que contienen iones básicos hidróxido (OH−) u óxido(O2−) se clasifican como bases. Los compuestos iónicos sin estos iones también se conocen como sales y pueden formarse mediante reacciones ácido-base.[25] Los compuestos iónicos también se pueden producir a partir de sus iones constituyentes por evaporación de su disolvente, precipitación, congelación, una reacción en estado sólido o la reacción de transferencia de electrones de metales reactivos con no metales reactivos, como los gases halógenos.  Los compuestos iónicos suelen tener altos puntos de fusión y ebullición, y son duros y quebradizos. Como sólidos, casi siempre son eléctricamente aislantes, pero cuando se funden o disuelven se vuelven altamente conductores, porque se movilizan los iones.[26]  Compuestos intermetálicos Un compuesto intermetálico es un tipo de aleación metálica que forma un compuesto de estado sólido ordenado entre dos o más elementos metálicos. Los intermetálicos son generalmente duros y quebradizos, con buenas propiedades mecánicas a altas temperaturas.[27][28][29] Se pueden clasificar como compuestos intermetálicos estequiométricos o no estequiométricos.[27]  Complejos químicos Un complejo de coordinación consiste en un átomo o ion central, que generalmente es metálico y se llama centro de coordinación, y una matriz circundante de moléculas o iones unidos, que a su vez se conocen como ligandos o agentes complejantes.[30][31][32] Muchos compuestos que contienen metales, especialmente los de metales de transición, son complejos de coordinación.[33] Un complejo de coordinación cuyo centro es un átomo metálico se denomina complejo metálico o elemento de bloque d.[34]  Enlaces y fuerzas Los compuestos se mantienen unidos por medio de diferentes tipos de enlaces y fuerzas. Las diferencias entre los tipos de enlaces de los compuestos dependen del tipo de elemento presente en el compuesto.  Las fuerzas de dispersión de London son las fuerzas más débiles entre las fuerzas intermoleculares. Son fuerzas de atracción temporales que se forman cuando los electrones en dos átomos adyacentes se colocan de manera que crean un dipolo temporal. Además, estas fuerzas son responsables de la condensación de sustancias no polares en líquidos y posterior congelación a un estado sólido dependiendo de la temperatura del ambiente.[35]  Un enlace covalente, también conocido como enlace molecular, implica el intercambio de electrones entre dos átomos. Principalmente, este tipo de enlace se produce entre elementos que aparecen uno cerca del otro en la tabla periódica de elementos, aunque se observa entre algunos metales y no metales. Esto se debe al mecanismo de este tipo de enlace. Los elementos cercanos en la tabla periódica tienden a tener electronegatividades similares, lo que significa que tienen una afinidad similar por los electrones. Como ninguno de los elementos tiene una afinidad más fuerte para donar o ganar electrones, hace que los elementos compartan electrones de manera que ambos elementos tengan un octeto más estable.  El enlace iónico se produce cuando los electrones de valencia se transfieren completamente entre los elementos. Al contrario que el covalente, este enlace químico crea dos iones de carga opuesta. Los metales en enlaces iónicos generalmente pierden sus electrones de valencia, convirtiéndose en cationes, cargados positivamente. El no metal ganará los electrones del metal, haciendo que el no metal sea un anión, es decir, cargado negativamente. Es decir, los enlaces iónicos se producen entre un donador de electrones, generalmente un metal, y un aceptor de electrones, que tiende a ser un no metal.[36]  El enlace de hidrógeno se produce cuando un átomo de hidrógeno unido a un átomo electronegativo forma una conexión electrostática con otro átomo electronegativo a través de dipolos o cargas que interactúan.[37][38][39]  Reacciones Un compuesto se puede convertir en una composición química diferente (productos) mediante la interacción con un segundo compuesto químico (reactivos) a través de una reacción química. En este proceso, los enlaces entre los átomos se rompen en ambos compuestos que interactúan, y luego los enlaces se reforman para  obtener nuevas asociaciones entre los mismos átomos. Esquemáticamente, esta reacción podría describirse como AB + CD → AD + CB, donde A, B, C y D son cada uno átomos únicos; y AB, AD, CD y CB son cada uno compuestos ùnicos "
quimica,"Una sustancia o substancia química[1]​ es una clase particular de materia homogénea cuya composición es fija[2]​ y químicamente definida, por lo que los átomos que la forman solo pueden aparecer en proporciones fijas.[3]​ Se compone por las siguientes entidades: moléculas, unidades formulares y átomos.[4]​  A veces, la palabra sustancia se emplea con un sentido más amplio, para referirse a la clase de materia de la que están formados los cuerpos, aunque por lo general, en química el empleo de la palabra sustancia está restringido al sentido dado por la primera definición.  Las sustancias se pueden diferenciar una de otra por su estado a la misma temperatura y presión, es decir, pueden ser sólidas, líquidas o gaseosas. También se pueden caracterizar por sus propiedades físicas, como la densidad, el punto de fusión, el punto de ebullición y solubilidad en diferentes disolventes.[5]​ Además estas distintas propiedades son específicas, fijas y reproducibles a una temperatura y presión dada.[6]​[7]​[8]​  Una sustancia no puede separarse en otras por ningún medio físico.[9]​ Estas sustancias pueden clasificarse en dos grupos: sustancias simples y sustancias compuestas o compuestos. Las sustancias simples están formadas por átomos de un mismo tipo, es decir de un mismo elemento, y los compuestos están formados por dos o más tipos de átomos diferentes.[10]​  Toda sustancia puede sufrir tres tipos de cambios: físicos, fisicoquímicos y químicos. En los cambios físicos no hay ninguna transformación química de las sustancias, solo de su forma, por ejemplo, comprimir un gas o romper un sólido. En los cambios fisicoquímicos tampoco hay una transformación química, sino solo cambios de agregación, por ejemplo, fundir un metal o disolver sal en agua. Por último, cuando se lleva a cabo un cambio químico, una sustancia se transforma en otra totalmente diferente, como por ejemplo oxidar un alambre metálico, o cuando reacciona un ácido con un álcali.[5]​  Definición Una sustancia química bien puede definirse como ""cualquier material con una composición química definida"" en un libro de texto de química general introductoria.[11]​ Según esta definición, una sustancia química puede ser un elemento químico puro o un compuesto químico puro. Pero hay excepciones a esta definición; una sustancia pura también puede definirse como una forma de materia que tiene una composición definida y propiedades distintas.[12]​ El índice de sustancias químicas publicado por CAS también incluye varias aleaciones de composición incierta.[13]​ Los compuestos no estequiométricos son un caso especial (en química inorgánica) que viola la ley de composición constante, y para ellos a veces es difícil trazar la línea divisoria entre una mezcla y un compuesto, como en el caso del hidruro de paladio. Se pueden encontrar definiciones más amplias de los productos químicos o de las sustancias químicas, por ejemplo ""el término 'sustancia química' significa cualquier sustancia orgánica o inorgánica de una identidad molecular particular, incluyendo - (i) cualquier combinación de dichas sustancias que se produzca total o parcialmente como resultado de una reacción química o que se encuentre en la naturaleza"".[14]​  En geología, las sustancias de composición uniforme se denominan minerales, mientras que las mezclas físicas (agregados) de varios minerales (sustancias diferentes) se definen como rocas. Muchos minerales, sin embargo, se disuelven mutuamente en solución sólida, de modo que una sola roca es una sustancia uniforme a pesar de ser una mezcla en términos estequiométricos. Los feldespatoss son un ejemplo común: La anortoclasa es un silicato de aluminio alcalino, en el que el metal alcalino es indistintamente sodio o potasio.  En Derecho, las ""sustancias químicas"" pueden incluir tanto sustancias puras como mezclas con una composición o un proceso de fabricación definidos. Por ejemplo, el reglamento REACH de la UE define las ""sustancias monoconstituyentes"", las ""sustancias multiconstituyentes"" y las ""sustancias de composición desconocida o variable"". Las dos últimas consisten en múltiples sustancias químicas; sin embargo, su identidad puede establecerse mediante un análisis químico directo o por referencia a un único proceso de fabricación. Por ejemplo, el carbón vegetal es una mezcla extremadamente compleja, parcialmente polimérica, que puede definirse por su proceso de fabricación. Por lo tanto, aunque se desconoce la identidad química exacta, la identificación puede realizarse con una precisión suficiente. El índice CAS también incluye mezclas.  Los polímeross aparecen casi siempre como mezclas de moléculas de múltiples masas molares, cada una de las cuales podría considerarse una sustancia química independiente. Sin embargo, el polímero puede definirse por un precursor o reacción(es) conocidos y la distribución de masa molar. Por ejemplo, el polietileno es una mezcla de cadenas muy largas de unidades repetitivas -CH2-, y se vende generalmente en varias distribuciones de masa molar, LDPE, MDPE, HDPE y Polietileno de ultra alto peso molecular.  Clasificación Las sustancias se pueden clasificar en sustancia simple y sustancia compuesta.   Se nombra sustancia simple a aquella sustancia formada por átomos y moléculas de un solo elemento químico. Por ejemplo el oxígeno diatómico       O  2         {\displaystyle {\ce {O2}}}   formado solamente por el elemento oxígeno o el Fullereno       C  60         {\displaystyle {\ce {C60}}}   formado solo por el elemento carbono.    Se llama sustancia compuesto en el caso de que la sustancia este formado por elementos distintos. Ejemplo de ello puede ser el agua       H  2      O    {\displaystyle {\ce {H2O}}}   porque está formado por elementos como el hidrógeno y oxígeno, otro ejemplo es la glucosa       C  6       H  12       O  6         {\displaystyle {\ce {C6H12O6}}}   formada por carbono, hidrógeno y oxígeno.     En ningún caso se debe confundir sustancia con mezcla, ya que la mezcla es una porción de materia que contiene dos a más sustancias, y puede ser heterogénea u homogénea.  Historia Una de las primeras fue el modelo aristotélico-escolástico introducido por los filósofos griegos, en este estudio se discutían los constituyentes básicos de la materia propuestos por Empédocles: agua, aire, tierra y fuego, así como las cuatro cualidades que tenía la materia en relación con estos elementos: frío, caliente, mojado y seco.[15]​  Después Paracelso en el siglo XVI, generó el término pureza, este concepto estaba relacionado con la destilación y la distinción entre el caput mortuum o sustancias mezcladas, con los espíritus destilados refiriéndose a las sustancias sin impurezas.[16]​  La primera ocasión en la que el término sustancia fue presentado ante una audiencia científica ocurrió en 1718 cuando Étienne François Geoffroy  presentó la tabla de afinidades de las sustancias que sirvió como faro de la química durante todo el siglo XVIII.[17]​  El concepto de sustancia química se estableció a finales del siglo XVIII con los trabajos del químico Joseph Proust sobre la composición de algunos compuestos químicos puros tales como el carbonato cúprico.[18]​ Proust dedujo que:  Esto se conoce como la ley de las proporciones definidas, y es una de las bases de la química moderna.[19]​  A principios del siglo XIX se dispuso de una representación microscópica general de las sustancias, en cualquier estado, y los cambios sustanciales en las reacciones químicas, tarea a la que contribuyó especialmente John Dalton.[17]​  Más tarde, con el avance de los métodos de síntesis química, especialmente en el ámbito de la química orgánica; el descubrimiento de muchos más elementos químicos y las nuevas técnicas en el ámbito de la química analítica utilizadas para el aislamiento y la purificación de elementos y compuestos de sustancias químicas que condujeron al establecimiento de la química moderna, el concepto se definió como se encuentra en la mayoría de los libros de texto de química. Sin embargo, existen algunas controversias con respecto a esta definición, principalmente porque es necesario indexar el gran número de sustancias químicas que aparecen en la literatura química.  El  isomerismo causó mucha consternación a los primeros investigadores, ya que los isómeros tienen exactamente la misma composición, pero difieren en la configuración (disposición) de los átomos. Por ejemplo, se especuló mucho sobre la identidad química del benceno, hasta que Friedrich August Kekulé describió la estructura correcta. Asimismo, la idea del  estereoisomerismo -que los átomos tienen una estructura tridimensional rígida y, por tanto, pueden formar isómeros que difieren sólo en su disposición tridimensional- fue otro paso crucial para comprender el concepto de sustancias químicas distintas. Por ejemplo, el ácido tartárico tiene tres isómeros distintos, un par de  diastereómeros con un diastereómero formando dos  enantiómeros.  Elementos químicos Un elemento es una sustancia química formada por un tipo particular de átomo y, por tanto, no puede descomponerse ni transformarse en otro elemento mediante una reacción química, aunque sí puede transmutarse en otro elemento mediante una reacción nuclear. Esto se debe a que todos los átomos de una muestra de un elemento tienen el mismo número de protoness, aunque pueden ser diferentes isótoposs, con distinto número de neutroness.  A fecha de 2019, se conocen 118 elementos, de los cuales unos 80 son estables, es decir, no cambian por desintegración radiactiva en otros elementos. Algunos elementos pueden presentarse como más de una sustancia química (alótropos). Por ejemplo, el oxígeno existe como oxígeno diatómico (O2) y como ozono (O3). La mayoría de los elementos se clasifican como metales. Se trata de elementos con un lustre característico, como el hierro, el cobre y el oro. Los metales suelen conducir bien la electricidad y el calor, y son malleables y dúctiles.[20]​ Alrededor de 14 a 21 elementos,[21]​ como el carbono, el nitrógeno y el oxígeno, se clasifican como no metales. Los no metales carecen de las propiedades metálicas descritas anteriormente, también tienen una alta electronegatividad y una tendencia a formar negativos. Ciertos elementos como el silicio a veces se parecen a los metales y a veces a los no metales, y se conocen como metaloides."
quimica,"La química orgánica es la rama de la química que estudia una clase numerosa de moléculas que en su gran mayoría contienen carbono formando enlaces covalentes: carbono-carbono o carbono-hidrógeno y otros heteroátomos, también conocidos como compuestos orgánicos.  Debido a la omnipresencia del carbono en los compuestos que esta rama de la química estudia, esta disciplina también es llamada química del carbono.[1]  Historia La química orgánica constituyó o se instituyó como disciplina en los años treinta. El desarrollo de nuevos métodos de análisis de las sustancias de origen animal y vegetal, basados en el empleo de disolventes, como el éter o el alcohol, permitió el aislamiento de un gran número de sustancias orgánicas que recibieron el nombre de ""principios inmediatos"". La aparición de la química orgánica se asocia a menudo al descubrimiento, en 1828, por el químico alemán Friedrich Wöhler, de que la sustancia inorgánica cianato de amonio podía convertirse en urea, una sustancia orgánica que se encuentra en la orina de muchos animales. Antes de este descubrimiento, los químicos creían que para sintetizar sustancias orgánicas, era necesaria la intervención de lo que llamaban ‘la fuerza vital’, es decir, los organismos vivos. El experimento de Wöhler[2] rompió la barrera entre sustancias orgánicas e inorgánicas. De esta manera, los químicos modernos consideran compuestos orgánicos a aquellos que contienen carbono e hidrógeno, y otros elementos (que pueden ser uno o más), siendo los más comunes: oxígeno, nitrógeno, azufre y los halógenos.  En 1856, sir William Henry Perkin, mientras trataba de estudiar la quinina, accidentalmente fabricó el primer colorante orgánico ahora conocido como malva de Perkin.[3]  La diferencia entre la química orgánica y la química biológica,es que en la segunda las moléculas de ADN tienen una historia y, por ende, en su estructura nos hablan de su historia, del pasado en el que se han constituido, mientras que una molécula orgánica, creada hoy, es solo testigo de su presente, sin pasado y sin evolución histórica.[4]  Cronología   Primeros compendios La tarea de presentar la química orgánica de manera sistemática y global se realizó mediante una publicación surgida en Alemania, fundada por el químico Friedrich Konrad Beilstein (1838-1906). Su Handbuch der organischen Chemie (Manual de la química orgánica) comenzó a publicarse en Hamburgo en 1880 y consistió en dos volúmenes que recogían información de unos quince mil compuestos orgánicos conocidos. Cuando la Deutsche chemische Gesellschaft (Sociedad Alemana de Química) trató de elaborar la cuarta reedición, en la segunda década del siglo XX, la cifra de compuestos orgánicos se había multiplicado por diez. Treinta y siete volúmenes fueron necesarios para la edición básica, que aparecieron entre 1916 y 1937. Un suplemento de 27 volúmenes se publicó en 1938, recogiendo información aparecida entre 1910 y 1919.  En la actualidad, se está editando el Fünftes Ergänzungswerk (quinta serie complementaria), que recoge la documentación publicada entre 1960 y 1979. Para ofrecer con más prontitud sus últimos trabajos, el Beilstein Institut ha creado el servicio Beilstein On line, que funciona desde 1988. Recientemente, se ha comenzado a editar periódicamente un CD-ROM, Beilstein Current Facts in Chemistry, que selecciona la información química procedente de importantes revistas. Actualmente, la citada información está disponible a través de internet.  El alma de la química orgánica: el carbono La gran cantidad de compuestos orgánicos que existen tiene su explicación en las características del átomo de carbono, que tiene cuatro electrones en su capa de valencia: según la regla del octeto necesita ocho para completarla, por lo que forma cuatro enlaces (valencia = 4) con otros átomos. Esta especial configuración electrónica da lugar a una variedad de posibilidades de hibridación orbital del átomo de carbono (hibridación química).  La molécula orgánica más sencilla que existe es el metano. En esta molécula, el carbono presenta hibridación sp3, con los átomos de hidrógeno formando un tetraedro.  El carbono forma enlaces covalentes con facilidad para alcanzar una configuración estable, estos enlaces los forma con facilidad con otros carbonos, lo que permite formar frecuentemente cadenas abiertas (lineales o ramificadas) y cerradas (anillos).  Clasificación de compuestos orgánicos La clasificación de los compuestos orgánicos puede realizarse de diversas maneras: atendiendo a su origen (natural o sintético), a su estructura (p. ej.: alifático o aromático), a su funcionalidad (p. ej.: alcoholes o cetonas), o a su peso molecular (p. ej.: monómeros o polímeros).  Clasificación según su origen La clasificación de los compuestos orgánicos según el origen es de dos tipos: naturales o sintéticos. A menudo, los de origen natural se entiende que son los presentes en los seres vivos, pero no siempre es así, ya que algunas moléculas orgánicas también se sintetizan ex-vivo, es decir en ambientes inertes, como por ejemplo el ácido fórmico en el cometa Halle-Bopp.  Natural Los compuestos orgánicos presentes en los seres vivos o ""biosintetizados"" constituyen una gran familia de compuestos orgánicos. Su estudio tiene interés en medicina, farmacia, perfumería, cocina y muchos otros campos más.  Los carbohidratos están compuestos fundamentalmente de carbono (C), oxígeno (O) e hidrógeno (H). Son a menudo llamados ""azúcares"", pero esta nomenclatura no es del todo correcta. Tienen una gran presencia en el reino vegetal (fructosa, celulosa, almidón, alginatos), pero también en el animal (glucógeno, glucosa). Se suelen clasificar según su grado de polimerización en:  Los lípidos son un conjunto de moléculas orgánicas, la mayoría biomoléculas, compuestas principalmente por carbono e hidrógeno y en menor medida oxígeno, aunque también pueden contener fósforo, azufre y nitrógeno. Tienen como característica principal el ser hidrófobas (insolubles en agua) y solubles en disolventes orgánicos como la bencina, el benceno y el cloroformo. En el uso coloquial, a los lípidos se les llama incorrectamente grasas, ya que las grasas son solo un tipo de lípidos procedentes de animales. Los lípidos cumplen funciones diversas en los organismos vivientes, entre ellas la de reserva energética (como los triglicéridos), la estructural (como los fosfolípidos de las bicapas) y la reguladora (como las hormonas esteroides).  Las proteínas son polipéptidos, es decir están formados por la polimerización de péptidos, y estos por la unión de aminoácidos. Pueden considerarse así ""poliamidas naturales"", ya que el enlace peptídico es análogo al enlace amida. Comprenden una familia muy importante de moléculas en los seres vivos, pero en especial en el reino animal. Por otra parte, son producto de la expresión de genes contenidos en el ADN. Algunos ejemplos de proteínas son el colágeno, las fibroínas, o la seda de araña.  Los ácidos nucleicos son polímeros formados por la repetición de monómeros denominados nucleótidos, unidos mediante enlaces fosfodiéster. Se forman, así, largas cadenas; algunas moléculas de ácidos nucleicos llegan a alcanzar pesos moleculares gigantescos, con millones de nucleótidos encadenados. Están formados por la moléculas de carbono, hidrógeno, oxígeno, nitrógeno y fosfato. Los ácidos nucleicos almacenan la información genética de los organismos vivos y son los responsables de la transmisión hereditaria. Existen dos tipos básicos, el ADN y el ARN.  Las moléculas pequeñas son compuestos orgánicos de peso molecular moderado (generalmente se consideran ""pequeñas"" aquellas con peso molecular menor a 1000 g/mol) y que aparecen en pequeñas cantidades en los seres vivos, pero no por ello su importancia es menor. A ellas pertenecen distintos grupos de hormonas como la testosterona, el estrógeno u otros grupos como los alcaloides. Las moléculas pequeñas tienen gran interés en la industria farmacéutica por su relevancia en el campo de la medicina.  Son compuestos orgánicos que han sido sintetizados sin la intervención de ningún ser vivo, en ambientes extracelulares y extravirales.  El petróleo es una sustancia clasificada como mineral en la cual se presentan una gran cantidad de compuestos orgánicos. Muchos de ellos, como el benceno, son empleados por el hombre tal cual, pero muchos otros son tratados o derivados para conseguir una gran cantidad de compuestos orgánicos, como por ejemplo los monómeros para la síntesis de materiales poliméricos o plásticos.  El sistema climático está constituido por la atmósfera, la hidrósfera, la biosfera, la geosfera y sus interacciones. Las variaciones en el equilibrio climático pueden generar diversos procesos como el calentamiento global, el efecto invernadero o la disminución de la capa de ozono.  En el año 2000 el ácido fórmico, un compuesto orgánico sencillo, también fue hallado en la cola del cometa Hale-Bopp.[5][6] Puesto que la síntesis orgánica de estas moléculas es inviable bajo las condiciones espaciales, este hallazgo parece sugerir que a la formación del sistema solar debió anteceder un periodo de calentamiento durante su colapso final.[6]  Sintético Desde la síntesis de Wöhler de la urea un altísimo número de compuestos orgánicos han sido sintetizados químicamente para beneficio humano. Estos incluyen fármacos, desodorantes, perfumes, detergentes, jabones, fibras textiles sintéticas, materiales plásticos, polímeros en general, o colorantes orgánicos.  Cadenas hidrocarbonadas sencillas Hidrocarburos El compuesto más simple es el metano, un átomo de carbono con cuatro de hidrógeno (valencia = 1), pero también puede darse la unión carbono-carbono, formando cadenas de distintos tipos, ya que pueden darse enlaces simples, dobles o triples. Cuando el resto de enlaces de estas cadenas son con hidrógeno, se habla de hidrocarburos, que pueden ser:  Radicales y ramificaciones de cadena Los radicales[7] son fragmentos de cadenas de carbonos que cuelgan de la cadena principal. Su nomenclatura se hace con la raíz correspondiente (en el caso de un carbono met-, dos carbonos et-, tres carbonos prop-, cuatro carbonos but-, cinco carbonos pent-, seis carbonos hex-, y así sucesivamente) y el sufijo -il. Además, se indica con un número, colocado delante, la posición que ocupan. El compuesto más simple que se puede hacer con radicales es el 2-metilbutano. En caso de que haya más de un radical, se nombrarán por orden alfabético de las raíces. Por ejemplo, el 2-etil, 5-metil, 8-butil, 10-docoseno.  Clasificación según los grupos funcionales Los compuestos orgánicos también pueden contener otros elementos, también otros grupos de átomos además del carbono e hidrógeno, llamados grupos funcionales. Un ejemplo es el grupo hidroxilo, que forma los alcoholes: un átomo de oxígeno enlazado a uno de hidrógeno (-OH), al que le queda una valencia libre. Asimismo también existen funciones alqueno (dobles enlaces), éteres, ésteres, aldehídos, cetonas, carboxílicos, carbamoilos,[8] azo, nitro o sulfóxido, entre otros.[9]  Alquino  Hidroxilo  Éter  Amina  Aldehído  Cetona  Carboxilo  Éster  Amida  Azo  Nitro  Sulfóxido  Oxigenados Son cadenas de carbonos con uno o varios átomos de oxígeno. Pueden ser:  El grupo –OH es muy polar y, lo que es más importante, es capaz de establecer puentes de hidrógeno: con sus moléculas compañeras o con otras moléculas neutras.  Dependiendo de la cantidad de grupos -OH que forman parte del alcohol, el mismo puede ser clasificado como monohidroxilado (presencia de un hidroxilo) o polihidroxilado (dos o más grupos hidroxilos en la molécula).  Nitrogenados Cíclicos Son compuestos que contienen un ciclo saturado. Un ejemplo de estos son los norbornanos, que en realidad son compuestos bicíclicos, los terpenos, u hormonas como el estrógeno, progesterona, testosterona u otras biomoléculas como el colesterol.  Aromáticos Los compuestos aromáticos tienen estructuras cíclicas insaturadas. El benceno es el claro ejemplo de un compuesto aromático, entre cuyos derivados están el tolueno, el fenol o el ácido benzoico. En general se define un compuesto aromático aquel que tiene anillos que cumplen la regla de Hückel, es decir que tienen 4n+2 electrones en orbitales π (n=0,1,2,...). A los compuestos orgánicos que tienen otro grupo distinto al carbono en sus cilos (normalmente N, O u S) se denominan compuestos aromáticos heterocíclicos. Así los compuestos aromáticos se suelen dividir en:  Isómeros Ya que el carbono puede enlazarse de diferentes maneras, una cadena puede tener diferentes configuraciones de enlace dando lugar a los llamados isómeros, moléculas tienen la misma fórmula química, pero distintas estructuras y propiedades. Existen distintos tipos de isomería: isomería de cadena, isomería de función, tautomería, estereoisomería, y estereoisomería configuracional.  El ejemplo mostrado a la izquierda es un caso de isometría de cadena en la que el compuesto con fórmula C6H12 puede ser un ciclo (ciclohexano) o un alqueno lineal, el 1-hexeno. Un ejemplo de isomería de función sería el caso del propanal y la acetona, ambos con fórmula C3H6O.  "
quimica,"El carbono (del latín, carbo, 'carbón') es un elemento químico con símbolo C, número atómico 6 y masa atómica 12,01. Es un no metal y tetravalente, disponiendo de cuatro electrones y 6 protones para formar enlaces químicos covalentes. Tres isótopos del carbono se producen de forma natural, los estables 12C y 13C y el isótopo radiactivo 14C, que decae con una vida media de unos 5730 años.[1] El carbono es uno de los pocos elementos conocidos desde la antigüedad,[2] y es el pilar básico de la química orgánica. Está presente en la Tierra en estado de cuerpo simple (carbón y diamantes), de compuestos inorgánicos (CO2 y CaCO3) y de compuestos orgánicos (biomasa, petróleo y gas natural). También se han sintetizado muchas nuevas estructuras basadas en el carbono: carbón activado, negro de humo, fibras, nanotubos, fullerenos y grafeno.  El carbono es el 15.º elemento más abundante en la corteza terrestre,[3] y el cuarto elemento más abundante en el universo en masa después del hidrógeno, el helio y el oxígeno. La abundancia del carbono, su diversidad única de compuestos orgánicos y su inusual capacidad para formar polímeros a las temperaturas comúnmente encontradas en la Tierra, permite que este elemento sirva como componente común de toda la vida conocida. Es el segundo elemento más abundante en el cuerpo humano en masa (aproximadamente el 18,5%) después del oxígeno.[4]  Los átomos de carbono pueden unirse de diferentes maneras, denominadas alótropos del carbono, reflejo de las condiciones de formación. Los más conocidos que ocurren naturalmente son el grafito, el diamante y el carbono amorfo.[5] Las propiedades físicas del carbono varían ampliamente con la forma alotrópica. Por ejemplo, el grafito es opaco y negro, mientras que el diamante es altamente transparente. El grafito es lo suficientemente blando como para formar una raya en el papel (de ahí su nombre, del verbo griego ""γράφειν"" que significa 'escribir'), mientras que el diamante es el material natural más duro conocido. El grafito es un buen conductor eléctrico mientras que el diamante tiene una baja conductividad eléctrica. En condiciones normales, el diamante, los nanotubos de carbono y el grafeno tienen las conductividades térmicas más altas de todos los materiales conocidos. Todos los alótropos del carbono son sólidos en condiciones normales, siendo el grafito la forma termodinámicamente estable. Son químicamente resistentes y requieren altas temperaturas para reaccionar incluso con oxígeno.  El estado de oxidación más común del carbono en los compuestos inorgánicos es +4, mientras que +2 se encuentra en el monóxido de carbono y en complejos carbonilos de metales de transición. Las mayores fuentes de carbono inorgánico son las calizas, dolomitas y dióxido de carbono, pero cantidades significativas se producen en depósitos orgánicos de carbón, turba, petróleo y clatratos de metano. El carbono forma un gran número de compuestos, más que cualquier otro elemento, con casi diez millones de compuestos descritos hasta la fecha[6] (con 500.000 compuestos nuevos por año), siendo sin embargo ese número solo una fracción del número de compuestos teóricamente posibles bajo condiciones estándar. Por esta razón, a menudo el carbono se ha descrito como el «rey de los elementos».[7]  La combustión del carbono en todas sus formas ha sido la base del desarrollo tecnológico desde tiempos prehistóricos. Los materiales basados en el carbono tienen aplicaciones en numerosas áreas de vanguardia tecnológica: materiales compuestos, baterías de iones de litio, descontaminación del aire y del agua, electrodos para hornos de arco, en la síntesis de aluminio, etc.  Características El carbono es un elemento notable por varias razones. Sus formas alotrópicas incluyen, una de las sustancias más blandas (el grafito) y una de las más duras (el diamante) y, desde el punto de vista económico, es de los materiales más baratos (carbón) y uno de los más caros (diamante). Más aún, presenta una gran afinidad para enlazarse químicamente con otros átomos pequeños, incluyendo otros átomos de carbono con los que puede formar largas cadenas, y su pequeño radio atómico le permite formar enlaces múltiples. Así, con el oxígeno forma el dióxido de carbono, vital para el crecimiento de las plantas (ver ciclo del carbono); con el hidrógeno forma numerosos compuestos denominados genéricamente hidrocarburos, esenciales para la industria y el transporte en la forma de combustibles fósiles; y combinado con oxígeno e hidrógeno forma gran variedad de compuestos como, por ejemplo, los ácidos grasos, esenciales para la vida, y los ésteres que dan sabor a las frutas; además es vector, a través del ciclo carbono-nitrógeno, de parte de la energía producida por el Sol.[8]  Estados alotrópicos Se conocen cinco formas alotrópicas del carbono, además del amorfo: grafito, diamante, fullereno, grafeno y carbino.[9]  Una de las formas en las cuales se encuentra el carbono es el grafito, caracterizado por tener sus átomos ""en los vértices de hexágonos que tapizan un plano"",[10] es de color negro, opaco y blando, y es el material del cual está hecha la parte interior de los lápices de madera. El grafito tiene exactamente los mismos átomos del diamante, pero por estar dispuestos en diferente forma tienen distintas propiedades físicas y químicas. Los diamantes naturales se forman en lugares donde el carbono ha sido sometido a grandes presiones y altas temperaturas. Su estructura es tetraédrica, que da como resultado una red tridimensional y a diferencia del grafito tiene un grado de dureza alto: 10 Mohs. Los diamantes se pueden crear artificialmente, sometiendo el grafito a temperaturas y presiones muy altas. El precio del grafito es menor al de los diamantes naturales, pero si se han elaborado adecuadamente tienen la misma dureza, color y transparencia.  La forma amorfa es esencialmente grafito, pero no llega a adoptar una estructura cristalina macroscópica. Esta es la forma presente en la mayoría de los carbones y en el hollín.  A presión normal, el carbono adopta la forma del grafito, en la que cada átomo está unido a otros tres en un plano compuesto de celdas hexagonales; este estado se puede describir como tres electrones de valencia en orbitales híbridos planos sp² y el cuarto en el orbital p.  Las dos formas de grafito conocidas alfa (hexagonal) y beta (romboédrica) tienen propiedades físicas idénticas. Los grafitos naturales contienen más del 30 % de la forma beta, mientras que el grafito sintético contiene únicamente la forma alfa. La forma alfa puede transformarse en beta mediante procedimientos mecánicos, y esta recristalizar en forma alfa al calentarse por encima de 1000 °C.  Debido a la deslocalización de los electrones del orbital pi, el grafito es conductor de la electricidad, propiedad que permite su uso en procesos de electroerosión. El material es blando y las diferentes capas, a menudo separadas por átomos intercalados, se encuentran unidas por enlaces de Van de Waals, siendo relativamente fácil que unas deslicen respecto de otras, lo que le da utilidad como lubricante.  A muy altas presiones, el carbono adopta la forma del diamante, en el cual cada átomo está unido a otros cuatro átomos de carbono, encontrándose los 4 electrones en orbitales sp³, como en los hidrocarburos. El diamante presenta la misma estructura cúbica que el silicio y el germanio y, gracias a la resistencia del enlace químico carbono-carbono, es, junto con el nitruro de boro, la sustancia más dura conocida. La transición a grafito a temperatura ambiente es tan lenta que es indetectable. Bajo ciertas condiciones, el carbono cristaliza como lonsdaleíta, una forma similar al diamante pero hexagonal.  El orbital híbrido sp1 que forma enlaces covalentes solo es de interés en química, manifestándose en algunos compuestos, como por ejemplo el acetileno.  Los fullerenos fueron descubiertos hace 15 años[10] tienen una estructura similar al grafito, pero el empaquetamiento hexagonal se combina con pentágonos (y en ciertos casos, heptágonos), lo que curva los planos y permite la aparición de estructuras de forma esférica, elipsoidal o cilíndrica. El constituido por 60 átomos de carbono, que presenta una estructura tridimensional y geometría similar a un balón de fútbol, es especialmente estable. Los fullerenos en general, y los derivados del C60 en particular, son objeto de intensa investigación en química desde su descubrimiento a mediados de los 1980.  A esta familia pertenecen también los nanotubos de carbono, que pueden describirse como capas de grafito enrolladas en forma cilíndrica y rematadas en sus extremos por hemiesferas (fulerenos), y que constituyen uno de los primeros productos industriales de la nanotecnología.  Aplicaciones El principal uso industrial del carbono es como un componente de hidrocarburos, especialmente los combustibles fósiles (petróleo y gas natural). Del primero se obtienen, por destilación en las refinerías, gasolinas, queroseno y aceites, siendo además la materia prima empleada en la obtención de plásticos. El segundo se está imponiendo como fuente de energía por su combustión más limpia. Otros usos son:  Historia El carbono fue descubierto en la prehistoria y ya era conocido en la antigüedad, a pesar de que en esta la manufacturaban mediante la combustión incompleta de materiales orgánicos. Los últimos alótropos conocidos, los fullerenos (C60), fueron descubiertos como subproducto en experimentos realizados con gases moleculares en la década de los 80. Se asemejan a un balón de fútbol, por lo que coloquialmente se les llama futbolenos.  Newton, en 1704, intuyó que el diamante podía ser combustible, pero no se consiguió quemar un diamante hasta 1772 en que Lavoisier demostró que en la reacción de combustión se producía CO2.  Tennant demostró que el diamante era carbono puro en 1797. El isótopo más común del carbono es el 12C; en 1961 este isótopo se eligió para reemplazar al isótopo oxígeno-16 como base de los pesos atómicos, y se le asignó un peso atómico de 12.  Los primeros compuestos de carbono se identificaron en la materia viva a principios del siglo XIX, y por ello el estudio de los compuestos de carbono se llamó química orgánica.  Abundancia y obtención El carbono no se creó durante el Big Bang porque hubiera necesitado la triple colisión de partículas alfa (núcleos atómicos de helio) y el Universo se expandió y enfrió demasiado rápido para que la probabilidad de que ello aconteciera fuera significativa. Donde sí ocurre este proceso es en el interior de las estrellas (en la fase RH (Rama horizontal)) donde este elemento es abundante, encontrándose además en otros cuerpos celestes como los cometas y en las atmósferas de los planetas. Algunos meteoritos contienen diamantes microscópicos que se formaron cuando el Sistema Solar era aún un disco protoplanetario.  En combinaciones con otros elementos, el carbono se encuentra en la atmósfera terrestre y disuelto en el agua, y acompañado de menores cantidades de calcio, magnesio y hierro forma enormes masas rocosas (caliza, dolomita, mármol, etc).  El grafito se encuentra en grandes cantidades en Rusia, Estados Unidos, México, Groenlandia y la India.  Los diamantes naturales se encuentran asociados a rocas volcánicas (kimberlita y lamproita). Los mayores depósitos de diamantes se encuentran en el África (Sudáfrica, Namibia, Botsuana, República del Congo y Sierra Leona).[11] Existen además depósitos importantes en Canadá, Rusia, Brasil y Australia.[cita requerida]  Compuestos inorgánicos El más importante óxido de carbono es el dióxido de carbono (CO2), un componente minoritario de la atmósfera terrestre (del orden del 0,04 % en peso) producido y usado por los seres vivos (ver ciclo del carbono). En el agua forma trazas de ácido carbónico (H2CO3) —las burbujas de muchos refrescos— pero, al igual que otros compuestos similares, es inestable, aunque a través de él pueden producirse iones carbonato estables por resonancia. Algunos minerales importantes, como la calcita, son carbonatos.  Los otros óxidos son el monóxido de carbono (CO) y el más raro subóxido de carbono (C3O2). El monóxido se forma durante la combustión incompleta de materias orgánicas y es incoloro e inodoro. Dado que la molécula de CO contiene un enlace triple, es muy polar, por lo que manifiesta una acusada tendencia a unirse a la hemoglobina, formando un nuevo compuesto muy peligroso denominado "
quimica,"Los no metales son elementos químicos que no son buenos conductores de la corriente eléctrica y el calor. Son muy débiles, por lo que no se pueden ni estirar ni convertir en una lámina.[1]   Las propiedades químicas de los no  metales, a diferencia de los metales, son muy diversas, a pesar de que representan un número muy reducido, la mayoría de ellos son esenciales para los sistemas biológicos (oxígeno, carbono, hidrógeno, nitrógeno, fósforo y azufre). En el grupo de los no metales se incluyen los halógenos[1] (flúor, cloro, bromo, yodo, astato y téneso), que tienen 7 electrones en su última capa de valencia y los gases nobles (helio, neón, argón, kriptón, xenón, radón), que tienen 8 electrones en su última capa (excepto el helio, que tiene 2). Por lo tanto, dicha capa está completa y son poco reactivos. El resto de los no metales pertenecen a diversos grupos y son hidrógeno, carbono, azufre, selenio, nitrógeno, oxígeno y fósforo. Las propiedades únicas del hidrógeno lo apartan del resto de los elementos en la Tabla Periódica de Elementos.  Los no metales son los elementos situados a la derecha en la Tabla, por encima de la línea quebrada de los grupos 14 a 17 (incluido el hidrógeno).[2] Colocados en orden creciente de número atómico, los elementos pueden clasificarse por similitud de propiedades en 18 familias o grupos (verticalmente por columnas).  Desde el punto de vista de la electrónica, los elementos de una familia poseen la misma configuración electrónica en la última capa, aunque difieren en el número de capas (períodos).[3] Los grupos o familias son 18 y se corresponden con las columnas de la Tabla Periódica de Elementos.  La mayoría de los no metales tienen aplicaciones biológicas, tecnológicas o domésticas. Los organismos vivos están compuestos casi en su totalidad por los no metales hidrógeno, oxígeno, carbono y nitrógeno. Casi todos los no metales tienen usos individuales en medicina, farmacias, iluminación, lasers y artículos domésticos.  Aunque el término ""no metálico"" se remonta a 1566, no existe una definición precisa de no metal ampliamente aceptada. Algunos elementos presentan una marcada mezcla de propiedades metálicas y no metálicas, y los casos límite que se consideran no metales varían en función de los criterios de clasificación. Catorce elementos se reconocen siempre como no metales y otros nueve se califican parcialmente como no metales.  Definición y elementos aplicables Un no metal es un elemento químico que se considera que carece de una preponderancia de propiedades metálicas como el brillo, la deformabilidad, una buena conductividad térmica y eléctrica y la capacidad de formar un óxido básico (en lugar de ácido).[4] Puesto que no existe una definición rigurosa de un no metal,[5][6][7] existe cierta variación entre las fuentes en cuanto a qué elementos se clasifican como tales. Las decisiones implicadas dependen de qué propiedad o propiedades se consideran más indicativas del carácter no metálico o metálico.[8]  Aunque Steudel,[9] en 2020, reconoció veintitrés elementos como no metales, cualquier lista de este tipo está abierta a cuestionamiento.[10] Catorce casi siempre reconocidos son hidrógeno, oxígeno, nitrógeno y azufre; los altamente reactivos halógenos flúor, cloro, bromo y yodo; y los gases nobles helio, neón, argón, criptón, xenón y radón (véase, por ejemplo, Larrañaga et al). [10] Los autores reconocieron el carbono, el fósforo y el selenio como no metales; Vernon[11] había informado anteriormente de que estos tres elementos a veces se consideraban metaloides. Los elementos comúnmente reconocidos como metaloides, a saber, boro; silicio y germanio; arsénico y antimonio; y telurio se cuentan a veces como una clase intermedia entre los metales y los no metales cuando los criterios utilizados para distinguir entre metales y no metales no son concluyentes.[12] En otras ocasiones se cuentan como no metales a la luz de su química no metálica.[13]  De los 118 elementos conocidos[14] no más del 20% se consideran no metales.[15] El estatus de unos pocos elementos es menos seguro. El astato, el quinto halógeno, a menudo se ignora debido a su rareza e intensa radioactividad;[16] la teoría y las pruebas experimentales sugieren que es un metal.[17] Los elementos superpesados copernicio (Z= 112), flerovio (114), y oganeso (118) pueden resultar no metales; su estatus no ha sido confirmado.[18]  Características Los no metales varían mucho en su apariencia, no son lustrosos y por lo general son malos conductores del calor y la electricidad. Sus puntos de fusión son más bajos que los de los metales (aunque el diamante, una forma de carbono, funde a 3570 °C).[19] Varios no metales existen en condiciones ordinarias como moléculas diatómicas.  En esta lista[20] están incluidos cinco gases (H2, N2, O2, F2 y Cl2), un líquido (Br2) y un sólido volátil (I2). El resto de los no metales son sólidos que pueden ser duros como el diamante o blandos como el azufre. Al contrario de los metales, son muy frágiles y no pueden estirarse ni en hilos ni en láminas.  Se encuentran en los tres estados de la materia a temperatura ambiente: son gases (como el oxígeno), líquidos (bromo) y sólidos (como el carbono). No tienen brillo metálico y no reflejan la luz. Muchos no metales se encuentran en todos los seres vivos: carbono, hidrógeno, oxígeno, nitrógeno, fósforo y azufre en cantidades importantes. Otros son oligoelementos: selenio, yodo, cloro.  Pueden ser sólidos, líquidos o gases, indistintamente. Sus puntos de fusión y ebullición dependen de sus propiedades químicas, que están relacionadas con su capacidad para ganar electrones (los de la última capa, o sea los de valencia).  No conducen bien la electricidad, muchos ante ella se descomponen o recombinan químicamente. Con el agua dan generalmente sustancias ácidas. Están ubicados a la derecha de la Tabla Periódica de Elementos, y al combinarse químicamente ganan electrones para adquirir la configuración electrónica del gas noble del mismo periodo.  Propiedades generales Físicas Aproximadamente la mitad de los elementos no metálicos son gases; la mayoría del resto son sólidos brillantes. El bromo, el único líquido, es tan volátil que suele estar cubierto por una capa de sus vapores; el azufre es el único no metal sólido coloreado. Los no metales fluidos tienen densidades, punto de fusión y punto de ebullición muy bajos, y son malos conductores de calor y electricidad.[21] Los elementos sólidos tienen densidades bajas, son quebradizos o desmenuzables con baja resistencia mecánica y estructural,[22] y de malos a buenos conductores.[n 1]  Las estructuras internas y las disposiciones de enlace de los no metales explican sus diferencias de forma. Los que existen como átomos discretos (por ejemplo, xenón) o moléculas (por ejemplo, oxígeno, azufre y bromo) tienen puntos de fusión y ebullición bajos, ya que se mantienen unidos por débiles fuerzas de dispersión de London que actúan entre sus átomos o moléculas.[26] Muchos son gases a temperatura ambiente. Los no metales que forman estructuras gigantes, como cadenas de hasta 1000 átomos (por ejemplo, el selenio),[27] láminas (por ejemplo, el carbono) o entramados tridimensionales (por ejemplo, el silicio), tienen puntos de fusión y ebullición más altos, ya que se necesita más energía para superar sus enlace covalentes más fuertes, por lo que todos son sólidos. Los que están más cerca del lado izquierdo de la tabla periódica, o más abajo en una columna, suelen tener algunas interacciones metálicas débiles entre sus moléculas, cadenas o capas, en consonancia con su proximidad a los metales; esto ocurre en el boro,[28] carbono,[29] fósforo,[30] arsénico,[31] selenio,[32] antimonio,[33] telurio[34] y yodo.[35]  Los elementos no metálicos son brillantes, coloreados o incoloros. En el caso del boro, el carbono grafítico, el silicio, el fósforo negro, el germanio, el arsénico, el selenio, el antimonio, el telurio y el yodo, sus estructuras presentan diversos grados de electrones deslocalizados que dispersan la luz visible entrante, dando lugar a un aspecto brillante. [36] Los no metales coloreados (azufre, flúor, cloro, bromo) absorben algunos colores (longitudes de onda) y transmiten los colores complementarios. En el caso del cloro, su ""familiar color amarillo verdoso... se debe a una amplia región de absorción en las regiones violeta y azul del espectro"".[37][n 2] En el caso de los no metales incoloros (hidrógeno, nitrógeno, oxígeno y los gases nobles), sus electrones se mantienen con suficiente fuerza como para que no se produzca absorción en la parte visible del espectro y se transmita toda la luz visible.[39]  Las conductividades eléctrica y térmica de los no metales y la naturaleza frágil de los sólidos están igualmente relacionadas con sus disposiciones internas. Mientras que una buena conductividad y plasticidad (maleabilidad, ductilidad) se asocian normalmente con la presencia de electrones en movimiento libre y uniformemente distribuidos en los metales[40] los electrones en los no metales carecen típicamente de tal movilidad.[41] Entre los elementos no metálicos, la buena conductividad eléctrica y térmica se da sólo en el carbono, el arsénico y el antimonio. [n 3] Por lo demás, la buena conductividad térmica sólo se da en el boro, el silicio, el fósforo y el germanio;[23] dicha conductividad se transmite a través de las vibraciones de las redes cristalinas de estos elementos.[42] El boro, el silicio, el fósforo, el germanio, el selenio, el telurio y el yodo presentan una conductividad eléctrica moderada.[n 4]}. La plasticidad se produce en circunstancias limitadas sólo en carbono, fósforo, azufre y selenio. [n 5].  Las diferencias físicas entre metales y no metales surgen de las fuerzas atómicas internas y externas. Internamente, la carga positiva que surge de los protones en el núcleo de un átomo actúa para mantener los electrones externos del átomo en su lugar. Externamente, los mismos electrones están sometidos a fuerzas de atracción de los protones de los átomos cercanos. Cuando las fuerzas externas son mayores o iguales que la fuerza interna, se espera que los electrones externos se muevan libremente entre los átomos, y se predicen propiedades metálicas. En caso contrario, se esperan propiedades no metálicas.[49]  Químicas Los no metales tienen valores de electronegatividad de moderados a altos[50] y tienden a formar compuestos ácidos. Por ejemplo, los no metales sólidos (incluidos los metaloides) reaccionan con ácido nítrico para formar o bien un ácido, o bien un óxido que tiene propiedades ácidas predominantes.[n 6].  Tienden a ganar o compartir electrones cuando reaccionan, a diferencia de los metales que tienden a donar electrones. Dada la estabilidad de las  configuraciones electrónicas de los gases nobles, que tienen cáscara externa completa, los no metales generalmente ganan suficientes electrones para darles la configuración electrónica del siguiente gas noble, mientras que los metales tienden a perder electrones suficientes para dejarlos con la configuración electrónica del gas noble precedente. Para los elementos no metálicos esta tendencia se resume en la dueto y regla del octeto, y para los metales existe una regla de los 18 electrones menos rigurosamente predictiva.[53]  Los no metales suelen tener valores de energías de ionización, afinidades electrónicas, electronegatividad y  potencial de reducción estándar más altos que los metales. En general, cuanto más altos son estos valores, más no metálico es el elemento.[54]  Las diferencias químicas entre metales y no metales surgen en gran medida de la fuerza de atracción entre la carga nuclear positiva de un átomo individual y sus electrones exteriores cargados negativamente. De izquierda a derecha a través de cada periodo de la tabla periódica, la carga nuclear aumenta a medida que aumenta el número de protones en el núcleo atómico.[55] Hay una reducción asociada en el radio atómico[56] a medida que el aumento de la carga nuclear acerca los electrones exteriores al núcleo.[57] En los metales, el efecto de la carga nuclear suele ser más débil que en los elementos no metálicos. En el enlace, por tanto, los metales tienden a perder electrones, y forman átomos o ioness cargados positivamente o polarizados mientras que los no metales tienden a ganar esos mismos electrones debido a su carga nuclear más fuerte, y forman iones o átomos polarizados cargados negativamente.[58]  El número de compuestos formados por no metales es enorme.[59] Los diez primeros puestos de una tabla de "
quimica,"Se denominan metales  a los elementos químicos caracterizados por ser buenos conductores del calor y la electricidad. Poseen alta densidad y son sólidos a temperatura ambiente (excepto el mercurio y galio)(elemento); sus sales forman iones electropositivos (cationes) en disolución.[1]   La ciencia de materiales define un metal como un material en el que existe un solapamiento entre la banda de valencia y la banda de conducción en su estructura electrónica (enlace metálico).[2] Esto le da la capacidad de conducir fácilmente calor y electricidad (tal como el cobre) y generalmente la capacidad de reflejar la luz, lo que le da su peculiar brillo. En ausencia de una estructura electrónica conocida, se usa el término para describir el comportamiento de aquellos materiales en los que, en ciertos rangos de presión y temperatura, la conductividad eléctrica disminuye al elevar la temperatura, en contraste con los semiconductores. Reaccionan químicamente con no metales, no son reactivos entre sí la gran mayoría de las veces, aunque algunos formen aleaciones entre sí.  Dentro de los metales se encuentran los alcalinos (como el sodio) y los alcalinotérreos (como el magnesio) los cuales presentan baja densidad, son buenos conductores del calor y la electricidad, además de ser muy reactivos.[3] También se incluyen los metales de transición (los cuales conforman la mayoría de los metales), los que se encuentran en diversos grupos y los lantánidos, actínidos y transactínidos.[4] Teóricamente, el resto de elementos que queda por descubrir y sintetizar serían metales.  El concepto de metal se refiere tanto a elementos puros, así como aleaciones con características metálicas, como el acero y el bronce. Los metales comprenden la mayor parte de la tabla periódica de los elementos y se separan de los no metales por una línea diagonal entre el boro y el polonio.[5] En comparación con los no metales tienen baja electronegatividad y baja energía de ionización, por lo que es más fácil que los metales cedan electrones y más difícil que los ganen.  En astrofísica, el término ""metal"" se utiliza de forma más amplia para referirse a todos los elementos químicos de una estrella que son más pesados que el helio, y no solo a los metales tradicionales. En este sentido, los primeros cuatro ""metales"" que se acumulan en los núcleos estelares a través de la nucleosíntesis son el carbono, el nitrógeno , el oxígeno y el neón , todos los cuales son estrictamente no metales en química. Una estrella fusiona átomos más ligeros, principalmente hidrógeno y helio, en átomos más pesados durante su vida. Usado en ese sentido, la metalicidad de un objeto astronómico es la proporción de su materia compuesta por los elementos químicos más pesados.[6][7][8]  Se obtienen a partir de los minerales que los contienen, llamados menas metálicas.  Historia Prehistoria El cobre, que se presenta en forma nativa, puede haber sido el primer metal descubierto, dado su aspecto distintivo, su pesadez y su maleabilidad en comparación con otras piedras o guijarros. El oro, la plata, el hierro (en forma de hierro meteórico) y el plomo también se descubrieron en la prehistoria. De esta época proceden las formas de latón, una aleación de cobre y zinc fabricada mediante la fundición simultánea de los minerales de estos metales (aunque el zinc puro no se aisló hasta el siglo XIII). La maleabilidad de los metales sólidos dio lugar a los primeros intentos de fabricar adornos, herramientas y armas de metal. De vez en cuando se descubría hierro meteórico que contenía níquel y que, en algunos aspectos, era superior a cualquier acero industrial fabricado hasta la década de 1880, cuando los aceros aleados cobraron protagonismo.  Metales como el oro, la plata, el hierro y el cobre, fueron utilizados desde la prehistoria. Al principio, solo se usaron los que se encontraban fácilmente en estado puro (en forma de elementos nativos), pero paulatinamente se fue desarrollando la tecnología necesaria para obtener nuevos metales a partir de sus menas, calentándolos en un horno mediante carbón de madera.  Antigüedad El primer gran avance se produjo con el descubrimiento del bronce, fruto de la utilización de mineral de cobre con incursiones de estaño, entre 3500 a. C. y 2000 a. C., en diferentes regiones del planeta, surgiendo la denominada Edad del Bronce, que sucede a la Edad de Piedra. El descubrimiento del bronce (una aleación de cobre con arsénico o estaño) permitió al hombre crear objetos metálicos más duros y duraderos que los anteriores. Las herramientas de bronce, las armas, las armaduras y los materiales de construcción, como las baldosas decorativas, eran más duros y duraderos que sus predecesores de piedra y cobre (""Calcolítico""). Inicialmente, el bronce se fabricaba con cobre y arsénico (formando el bronce arsenical) mediante la fundición de minerales de cobre y arsénico mezclados natural o artificialmente.[9]  Otro hecho importante en la historia fue la utilización del hierro, hacia 1400 a. C. Los hititas fueron uno de los primeros pueblos en utilizarlo para elaborar armas, tales como espadas, y las civilizaciones que todavía estaban en la Edad del Bronce, como los egipcios.  No obstante, en la antigüedad no se sabía alcanzar la temperatura necesaria para fundir el hierro, por lo que se obtenía un metal impuro que había de ser moldeado a martillazos. Hacia el año 1400 se empezaron a utilizar los hornos provistos de fuelle,[10] que permiten alcanzar la temperatura de fusión del hierro, unos 1535 °C.  Edad Media Los alquimistas árabes y medievales creían que todos los metales y la materia estaban compuestos por el principio del azufre, el padre de todos los metales y portador de la propiedad de combustible, y el principio del mercurio, la madre de todos los metales —en la antigüedad, el plomo se consideraba el padre de todos los metales— y portador de las propiedades de liquidez, fusibilidad y volatilidad. Estos principios no eran necesariamente las sustancias comunes azufre y mercurio que se encuentran en la mayoría de los laboratorios. Esta teoría reforzaba la creencia de que todos los metales estaban destinados a convertirse en oro en las entrañas de la tierra mediante las combinaciones adecuadas de calor, digestión, tiempo y eliminación de contaminantes, todo lo cual podía desarrollarse y acelerarse mediante los conocimientos y métodos de la alquimia. Paracelso, un escritor posterior del Renacimiento alemán, añadió el tercer principio de la sal, que conlleva las propiedades no volátiles e incombustibles, en su doctrina tria prima. Estas teorías mantuvieron los cuatro elementos clásicos como base de la composición del azufre, el  mercurio y la sal.  Se conocieron el arsénico, el zinc, el antimonio y el bismuto, aunque al principio se les llamó semimetales o metales bastardos por su carácter inmóvil. Es posible que los cuatro se utilizaran incidentalmente en épocas anteriores sin reconocer su naturaleza. Se cree que Albertus Magnus fue el primero en aislar el arsénico de un compuesto en 1250, calentando jabón junto con trisulfuro de arsénico. El zinc metálico, que es frágil si es impuro, se aisló en la India hacia el año 1300. La primera descripción de un procedimiento para aislar el antimonio se encuentra en el libro de 1540 De la pirotecnia de Vannoccio Biringuccio. El bismuto fue descrito por Agricola en De natura fossilium (c. 1546); se había confundido en los primeros tiempos con el estaño y el plomo por su parecido con estos elementos.  Renacimiento El primer texto sistemático sobre las artes de la minería y la metalurgia fue De la pirotechnia (1540) (1540) de Vannoccio Biringuccio, que trata del examen, la fusión y el trabajo de los metales.  Dieciséis años más tarde, Georgius Agricola publicó en 1556 De re metallica, un relato claro y completo de la profesión de la minería, la metalurgia y las artes y ciencias accesorias, además de calificarse como el mayor tratado sobre la industria química a lo largo del siglo XVI.  En su De natura fossilium dio la siguiente descripción de un metal (1546):   Tradicionalmente existen seis tipos de metales diferentes, a saber, el oro, la plata, el cobre, el hierro, el estaño y el plomo. En realidad hay otros, pues el azogue es un metal, aunque los alquimistas no están de acuerdo con nosotros en este tema, y el bismuto también lo es. Los antiguos escritores griegos parecen haber ignorado el bismuto, por lo que Amonio afirma con razón que hay muchas especies de metales, animales y plantas que nos son desconocidas. El estibio, cuando se funde en el crisol y se refina, tiene tanto derecho a ser considerado como un metal propio como el que los escritores conceden al plomo. Si cuando se funde, se añade una cierta porción al estaño, se produce una aleación para libreros con la que se hace el tipo que utilizan los que imprimen libros en papel.  Cada metal tiene su propia forma que conserva cuando se separa de los metales que se mezclaron con él. Por lo tanto, ni el electrum ni el stannum [no se refiere a nuestro estaño] son en sí mismos un verdadero metal, sino una aleación de dos metales. El electrum es una aleación de oro y plata, el Stannum de plomo y plata. Sin embargo, si se separa la plata del electrum, queda el oro y no el electrum; si se quita la plata del Stannum, queda el plomo y no el Stannum.  Sin embargo, no se puede determinar con certeza si el latón es un metal nativo o no. Sólo conocemos el latón artificial, que consiste en cobre teñido con el color del mineral calamina. Y sin embargo, si se desenterrara alguno, sería un metal propiamente dicho. El cobre blanco y el negro parecen ser diferentes del rojo.  El metal, por tanto, es por naturaleza o bien sólido, como he dicho, o bien fluido, como en el caso único del azogue.  El platino, tercer metal precioso después del oro y la plata, fue descubierto en Ecuador entre 1736 y 1744 por el astrónomo español Antonio de Ulloa y su colega el matemático Jorge Juan y Santacilia. Ulloa fue el primero en escribir una descripción científica del metal, en 1748.  En 1789, el químico alemán Martin Heinrich Klaproth consiguió aislar un  óxido de uranio, que pensó que era el propio metal. Posteriormente, Klaproth fue reconocido como el descubridor del uranio. No fue hasta 1841 cuando el químico francés Eugène-Melchior Péligot pudo preparar la primera muestra de uranio metálico. Posteriormente, Henri Becquerel descubrió la radiactividad en 1896 utilizando el uranio.  En la década de 1790, Joseph Priestley y el químico holandés Martinus van Marum observaron la acción transformadora de las superficies metálicas en la deshidrogenación del alcohol, un desarrollo que posteriormente condujo, en 1831, a la síntesis a escala industrial del ácido sulfúrico utilizando un catalizador de platino.  En 1803, el cerio fue el primero de los  metales lantánidos en ser descubierto, en Bastnäs (Suecia) por Jöns Jakob Berzelius y Wilhelm Hisinger, e independientemente por Martin Heinrich Klaproth en Alemania. Los metales lantánidos se consideraron en gran medida como rarezas hasta la década de 1960, cuando se desarrollaron métodos para separarlos de forma más eficaz. Posteriormente se han utilizado en teléfonos móviles, imanes, láseres, iluminación, baterías, convertidores catalíticos y otras aplicaciones que permiten las tecnologías modernas.  Otros metales descubiertos y preparados durante esta época fueron el cobalto, el níquel, el manganeso, el molibdeno, el tungsteno y el cromo; y algunos de los metales del grupo del platino, el paladio, el osmio, el iridio y el rodio.  Época contemporánea Henry Bessemer descubrió un modo de producir acero en grandes cantidades con un coste razonable. Tras numerosos intentos fallidos, dio con un nuevo diseño de horno (el convertidor Thomas-Bessemer) y, a partir de entonces, mejoró la construcción de estructuras en edificios y puentes, pasando el hierro a un segundo plano.  Poco después se utilizó el aluminio y el magnesio, que permitieron desarrollar aleaciones mucho más ligeras y resistentes, muy utilizadas en aviación, transporte terrestre y herramientas portátiles. "
quimica,"Una reacción química, también llamada cambio químico o fenómeno químico, es todo proceso termodinámico en el cual dos o más especies químicas o sustancias (llamadas reactantes o reactivos), se transforman,  cambiando su estructura molecular y sus enlaces, en otras sustancias llamadas productos.[1] Los reactantes pueden ser elementos o compuestos. Un ejemplo de reacción química es la formación de óxido de hierro producida al reaccionar el oxígeno del aire con el hierro de forma natural, o una cinta de magnesio al colocarla en una llama se convierte en óxido de magnesio, como un ejemplo de reacción inducida.  La reacción química también se puede definir desde dos enfoques, el macroscópico que la define como «un proceso en el cual una o varias sustancias se forman a partir de otra u otras» y el nanoscópico cuya definición sería: «redistribución de átomos e iones, formándose otras estructuras (moléculas o redes)».[2]  Las reacciones químicas ocurren porque las moléculas se están moviendo y cuando se golpean con energía suficiente una contra otras, los enlaces se rompen y los átomos se intercambian para formar nuevas moléculas. También una molécula que está vibrando con energía suficiente puede romperse en moléculas más pequeñas.[3]  A la representación simbólica de cada una de las reacciones se le denomina ecuación química.[4]  Los productos obtenidos a partir de ciertos tipos de reactivos dependen de las condiciones bajo las que se da la reacción química. No obstante, tras un estudio cuidadoso se comprueba que, aunque los productos pueden variar según cambien las condiciones, determinadas cantidades permanecen constantes en cualquier reacción química. Estas cantidades constantes, las magnitudes conservadas, incluyen el número de cada tipo de átomo presente, la carga eléctrica y la masa total.  Fenómeno químico Se llama fenómeno químico  a los sucesos observables y posibles de ser medidos en los cuales las sustancias intervinientes cambian su composición química al combinarse entre sí.[5] Las reacciones químicas implican una interacción que se produce a nivel de los electrones de valencia de las sustancias intervinientes. Dicha interacción es el enlace químico.  En estos fenómenos, no se conserva la sustancia original, que se transforma su estructura química, manifiesta energía, no se observa a simple vista y son irreversibles,[6] en su mayoría.   La sustancia sufre modificaciones irreversibles. Por ejemplo, al quemarse, un papel no puede volver a su estado original. Las cenizas resultantes formaron parte del papel original, y sufrieron una alteración química.  Clases de reacciones Reacciones de la química inorgánica Desde un punto de vista de la química inorgánica se pueden postular dos grandes modelos para las reacciones químicas de los compuestos inorgánicos: reacciones ácido-base o de neutralización (sin cambios en los estados de oxidación) y reacciones redox (con cambios en los estados de oxidación).[7] Sin embargo podemos clasificarlas de acuerdo con los siguientes tres criterios:  CO2 (g) → CO2 (g) C(s) + O2 (g)  2Mg+O2 + ΔH → 2MgO + Luz  Reacciones de la química orgánica Respecto a las reacciones de la química orgánica, nos referimos a ellas teniendo como base a diferentes tipos de compuestos como alcanos, alquenos, alquinos, alcoholes, aldehídos, cetonas, entre otras; que encuentran su clasificación, reactividad y/o propiedades químicas en el grupo funcional que contienen y este último será el responsable de los cambios en la estructura y composición de la materia. Entre los grupos funcionales más importantes tenemos a los dobles y triples enlaces y a los grupos hidroxilo, carbonilo y nitro.  Factores que afectan la velocidad de reacción La velocidad de reacción puede ser independiente de la temperatura (no Arrhenius) o disminuir con el aumento de la temperatura (anti Arrhenius). Las reacciones sin una barrera de activación (por ejemplo, algunas reacciones de radicales) tienden a tener una dependencia de la temperatura de tipo anti Arrhenius: la constante de velocidad disminuye al aumentar la temperatura.  Rendimiento químico La cantidad de producto que se suele obtener de una reacción  química, es menor que la cantidad teórica. Esto depende de varios factores, como la pureza del reactivo y las reacciones secundarias que puedan tener lugar (es posible que no todos los productos reaccionen), cabe mencionar que la recuperación del 100 % de la muestra es prácticamente imposible.  El rendimiento de una reacción se calcula mediante la siguiente fórmula:  Cuando uno de los reactivos esté en exceso, el rendimiento deberá calcularse respecto al reactivo limitante. Y el rendimiento depende del calor que expone la reacción.  Grado de avance de la reacción y afinidad Una reacción se puede representar mediante la siguiente expresión matemática:  donde      ν  i     {\displaystyle \nu _{i}}   son los coeficientes estequiométricos de la reacción, que pueden ser positivos (productos) o negativos (reactivos). La ecuación presenta dos formas posibles de estar químicamente en la naturaleza (como suma de productos o como suma de reactivos).  Si      d   m  i     {\displaystyle \mathrm {d} m_{i}}   es la masa del producto que aparece, o del reactivo que desaparece, resulta que:  constante     ∀ i   {\displaystyle \forall i}  .      M  i     {\displaystyle M_{i}}   sería la masa molecular del compuesto correspondiente y     ξ   {\displaystyle \xi }   se denomina grado de avance. Este concepto es importante pues es el único grado de libertad en la reacción.  Cuando existe un equilibrio en la reacción, la energía libre de Gibbs es un mínimo, "
quimica,"Una fórmula química es la representación de los elementos químicos que forman un compuesto químico y la proporción en que se encuentran, o del número de átomos que forman una molécula. También puede darnos información adicional como la manera en que se unen dichos átomos mediante enlaces químicos e incluso su distribución en el espacio. Para nombrarlas, se emplean las reglas de la nomenclatura química. Ejemplo: La fórmula de los silanos[1]​  A veces, los miembros de una familia química se diferencian entre sí por una unidad constante, generalmente un átomo de carbono adicional en una cadena carbonada.  En química, una fórmula química es una forma de presentar información sobre las proporciones químicas de átomos que constituyen un compuesto químico o molécula en particular, utilizando símbolos de elemento químico, números y, a veces, también otros símbolos, como paréntesis, guiones, corchetes, comas y signos más (+) y menos (-). Se limitan a una sola línea tipográfica de símbolos, que puede incluir subíndices y superíndices. Una fórmula química no es una nombre químico, ya que no contiene ninguna palabra. Aunque una fórmula química puede implicar ciertas  estructuras químicass simples, no es lo mismo que una fórmula estructural química completa. Las fórmulas químicas sólo pueden especificar completamente la estructura de las moléculas y sustancias químicas más sencillas, y en general tienen un poder más limitado que los nombres químicos y las fórmulas estructurales.  Los tipos más sencillos de fórmulas químicas se denominan  fórmulas empíricase, que utilizan letras y números que indican las proporciones numéricas de átomos de cada tipo. Las  fórmulas moleculares indican simplemente los números de cada tipo de átomo en una molécula, sin información sobre la estructura. Por ejemplo, la fórmula empírica de la glucosa es CH2O (el doble de átomos de hidrógeno que de carbono y oxígeno), mientras que su fórmula molecular es C6H12O6 (12 átomos de hidrógeno, seis de carbono y oxígeno).  A veces una fórmula química se complica al escribirse como una  fórmula condensada (o fórmula molecular condensada, a veces llamada ""fórmula semiestructural""), que transmite información adicional sobre las formas particulares en que los átomos están enlace químico unidos entre sí, ya sea en enlace covalente, enlace iónico, o varias combinaciones de estos tipos. Esto es posible si el enlace correspondiente es fácil de mostrar en una dimensión. Un ejemplo es la fórmula molecular/química condensada del etanol, que es CH3−CH2−OH o CH3CH2OH. Sin embargo, incluso una fórmula química condensada está necesariamente limitada en su capacidad para mostrar relaciones de enlace complejas entre átomos, especialmente átomos que tienen enlaces con cuatro o más sustituyentess diferentes.  Dado que una fórmula química debe expresarse como una única línea de  símbolos de elementos químicos, a menudo no puede ser tan informativa como una fórmula estructural real, que es una representación gráfica de la relación espacial entre los átomos de los compuestos químicos (véase, por ejemplo, la figura de las fórmulas estructurales y químicas del butano, a la derecha). Por razones de complejidad estructural, una misma fórmula química condensada (o fórmula semiestructural) puede corresponder a distintas moléculas, conocidas como isómeross. Por ejemplo, la glucosa comparte su fórmula molecular C6H12O6 con otros azúcares, como la fructosa, la galactosa y la manosa. Existen nombres químicos equivalentes lineales que pueden especificar y especifican de forma única cualquier fórmula estructural compleja (véase nomenclatura química), pero tales nombres deben utilizar muchos términos (palabras), en lugar de los símbolos de elementos simples, números y símbolos tipográficos simples que definen una fórmula química.  Las fórmulas químicas pueden utilizarse en  ecuaciones químicas para describir reacciones químicass y otras transformaciones químicas, como la disolución de compuestos iónicos en una solución. Aunque, como se ha señalado, las fórmulas químicas no tienen todo el poder de las fórmulas estructurales para mostrar las relaciones químicas entre los átomos, son suficientes para llevar la cuenta de los números de átomos y los números de cargas eléctricas en las reacciones químicas, por lo que balancing chemical equations para que estas ecuaciones se puedan utilizar en problemas químicos que implican la conservación de los átomos, y la conservación de la carga eléctrica.  Existen varios tipos de fórmulas químicas:[2]​  Resumen Una fórmula química identifica cada elemento constituyente por su símbolo químico e indica el número proporcional de átomos de cada elemento. En las fórmulas empíricas, estas proporciones comienzan con un elemento clave y luego asignan números de átomos de los otros elementos en el compuesto, por proporciones al elemento clave. En el caso de los compuestos moleculares, todas estas proporciones pueden expresarse como números enteros. Por ejemplo, la fórmula empírica del etanol puede escribirse C2H6O porque todas las moléculas de etanol contienen dos átomos de carbono, seis átomos de hidrógeno y un átomo de oxígeno. Sin embargo, algunos tipos de compuestos iónicos no pueden escribirse con fórmulas empíricas de números enteros. Un ejemplo es el carburo de boro, cuya fórmula de CBn  es una relación variable de números no enteros con n que oscila entre más de 4 y más de 6,5.  Cuando el compuesto químico de la fórmula está formado por moléculas simples, las fórmulas químicas suelen emplear formas de sugerir la estructura de la molécula. Estos tipos de fórmulas se conocen como fórmulas moleculares y fórmulas condensadas'. Una fórmula molecular enumera el número de átomos para reflejar los de la molécula, de modo que la fórmula molecular de la glucosa es C6H12O6 en lugar de la fórmula empírica de la glucosa, que es CH2O. Sin embargo, salvo en el caso de sustancias muy simples, las fórmulas químicas moleculares carecen de la información estructural necesaria y son ambiguas.  Para las moléculas simples, una fórmula condensada (o semiestructural) es un tipo de fórmula química que puede implicar plenamente una fórmula estructural correcta. Por ejemplo, el etanol puede representarse mediante la fórmula química condensada CH3CH2OH, y el éter dimetílico mediante la fórmula condensada CH3OCH3. Estas dos moléculas tienen las mismas fórmulas empírica y molecular (C2H6O), pero pueden diferenciarse por las fórmulas condensadas que se muestran, que son suficientes para representar la estructura completa de estos compuestos orgánicos simples.  Las fórmulas químicas condensadas también pueden utilizarse para representar  compuestos iónicoss que no existen como moléculas discretas, pero que, sin embargo, contienen agrupaciones unidas covalentemente en su interior. Estos  iones poliatómicoss son grupos de átomos que están unidos covalentemente y tienen una carga iónica global, como el sulfato. [SO4]2- . Cada ion poliatómico en un compuesto se escribe individualmente para ilustrar las agrupaciones separadas. Por ejemplo, el compuesto hexóxido de dicloro tiene una fórmula empírica ClO3, y una fórmula molecular Cl2O6, pero en forma líquida o sólida, este compuesto se muestra más correctamente mediante una fórmula condensada iónica [ClO2]+[ClO4]−, que ilustra que este compuesto consta de iones [ClO2]+ e iones [ClO4]−. En tales casos, la fórmula condensada sólo necesita ser lo suficientemente compleja como para mostrar al menos una de cada especie iónica.  Las fórmulas químicas tal como se describen aquí son distintas de los nombres sistemáticos químicos mucho más complejos que se utilizan en varios sistemas de nomenclatura química. Por ejemplo, un nombre sistemático de la glucosa es (2R,3S,4R,5R)-2,3,4,5,6-pentahidroxihexanal. Este nombre, interpretado por las reglas que lo sustentan, especifica plenamente la fórmula estructural de la glucosa, pero el nombre no es una fórmula química tal y como se entiende habitualmente, y utiliza términos y palabras que no se emplean en las fórmulas químicas. Estos nombres, a diferencia de las fórmulas básicas, pueden representar fórmulas estructurales completas sin gráficos.  Tipos de fórmulas Fórmula molecular La fórmula molecular de un compuesto químico es una representación gráfica de la estructura molecular, que muestra cómo se ordenan o distribuyen espacialmente los átomos. Se muestran los enlaces químicos dentro de la molécula, ya sea explícitamente o implícitamente  Las fórmulas moleculares indican simplemente los números de cada tipo de átomo en una molécula de una sustancia molecular. Son iguales que las fórmulas empíricas para moléculas que sólo tienen un átomo de un tipo concreto, pero que por lo demás pueden tener números mayores. Un ejemplo de la diferencia es la fórmula empírica de la glucosa, que es CH2O (proporción 1:2:1), mientras que su fórmula molecular es C6H12O6 (número de átomos 6:12:6). En el caso del agua, ambas fórmulas son H2O. Una fórmula molecular proporciona más información sobre una molécula que su fórmula empírica, pero es más difícil de establecer.  Una fórmula molecular muestra el número de elementos en una molécula, y determina si es un compuesto binario, compuesto ternario, compuesto cuaternario, o tiene aún más elementos.  Fórmula empírica   En química, la fórmula empírica de una sustancia química es una expresión simple del número relativo de cada tipo de átomo o proporción de los elementos en el compuesto. Las fórmulas empíricas son el estándar para  compuestos iónicos, como CaCl2, y para macromoléculas, como SiO2. Una fórmula empírica no hace referencia al isómeroismo, estructura o número absoluto de átomos. El término empírica se refiere al proceso de análisis elemental, una técnica de química analítica utilizada para determinar la composición porcentual relativa de una sustancia química pura por elemento.  Por ejemplo, el hexano tiene una fórmula molecular de C6H14, y (para uno de sus isómeros, el n-hexano) una fórmula estructural CH3CH2CH2CH2CH2CH3, lo que implica que tiene una estructura de cadena de 6 átomos de carbono y 14 átomos de hidrógeno. Sin embargo, la fórmula empírica del hexano es C3H7. Del mismo modo, la fórmula empírica del peróxido de hidrógeno, H2O2, es simplemente HO, expresando la proporción 1:1 de los elementos componentes. El formaldehído y el ácido acético tienen la misma fórmula empírica, CH2O. Ésta es la fórmula química real del formaldehído, pero el ácido acético tiene el doble de átomos.  Fórmula semidesarrollada La fórmula semidesarrollada muestra todos los átomos que forman una molécula covalente, y los enlaces entre átomos de carbono (en compuestos orgánicos) o de otros tipos de átomos.  No se indican los enlaces carbono-hidrógeno  Fórmula desarrollada La fórmula desarrollada es el paso siguiente en complejidad de la semidesarrollada, indicando el enlace y la ubicación de cada átomo del compuesto dentro de sus respectivas moléculas, en un plano cartesiano, representando la totalidad de la estructura del compuesto  Fórmula estructural La fórmula estructural es similar a las anteriores, pero señalando la geometría espacial de la molécula mediante la indicación de distancias, ángulos o el empleo de perspectivas en diagramas bi o tridimensionales.[4]​  Fórmula de Lewis Estructura de Lewis, también llamada diagrama de punto, modelo de Lewis o representación de Lewis, es una representación gráfica que muestra los enlaces entre los átomos de una molécula y los pares de electrones solitarios que puedan existir.  Diagramas En un diagrama 2D, se aprecia la orientación de los enlaces usando símbolos especiales. Una línea continua representa un enlace en el plano; si el enlace está por detrás, se representa mediante una línea de puntos; si el enlace está por delante, se indica con un símbolo en forma de cuña triangular. A veces se emplean otro tipo de convenios o proyecciones para grupos de compuestos específicos (proyección de Newman, Reactivo de Tollens, etc).  Fórmula general La fórmula general de un grupo de compuestos puede representarse de diferentes maneras:  Expresando el número de átomos de cada clase por ejemplo En química inorgánica, una familia de compuestos se puede representar por una fórmula general cuyos subíndices (número de átomos de cada clase) son variables (x, y, z).  Incluyendo expresiones matemáticas en los subíndices En química orgánica, es frecuente que los subíndices sean expresiones matemáticas que incluyen la variable n (número de átomos de carbono). Se llama serie homóloga al conjunto de compuestos que comparten la misma fórmula general. Por ejemplo, la fórmula general de los alcoholes es: CnH(2n + 1)OH (donde n ≥ 1)  Incluyendo radicales y grupo funcional En la expresión de la fórmula general, en química orgánica, suele aparecer la estructura de los compuestos de una serie homóloga, incluyendo la parte del sustituyente (que se representa por R, R', etc) y el grupo funcional. Ejemplo: La fórmula general de los alcoholes primarios es R-OH.  Sistema de Hill El sistema de Hill (o notación de Hill) es un sistema de escritura de fórmulas químicas empíricas, fórmulas químicas moleculares y componentes de una fórmula condensada tal que el número de átomos de carbono en un molécula se indica primero, el número de átomos de hidrógeno a continuación, y luego el número de todos los demás elementos químicos posteriormente, en orden alfabético de los símbolos químicos. Cuando la fórmula no contiene carbono, todos los elementos, incluido el hidrógeno, se enumeran alfabéticamente.  Clasificando las fórmulas de acuerdo con el número de átomos de cada elemento presente en la fórmula de acuerdo con estas reglas, y las diferencias en elementos o números anteriores se tratan como más significativas que las diferencias en cualquier elemento o número posterior, como clasificar cadenas de texto en orden lexicográfico: es posible cotejar fórmulas químicas en lo que se conoce como orden del sistema de Hill.  El sistema de Hill fue publicado por primera vez por Edwin A. Hill de la Oficina de Marcas y Patentes de los Estados Unidos en 1900.[5]​ Es el sistema más utilizado en bases de datos químicas e índices impresos para clasificar listas de compuestos.[6]​  Una lista de fórmulas en el orden del sistema de Hill está ordenada alfabéticamente, como se muestra arriba, con elementos de una sola letra que van antes de los símbolos de dos letras cuando los símbolos comienzan con la misma letra (por lo que ""B"" viene antes de ""Be"", que viene antes de ""Br "").[6]​  Las siguientes fórmulas de ejemplo se escriben utilizando el sistema de Hill y se enumeran en orden de "
biologia,"Un ser vivo u organismo es un conjunto material de organización compleja, en la que intervienen sistemas de comunicación molecular que lo relacionan internamente y con el medio ambiente en un intercambio de materia y energía de una forma ordenada, teniendo la capacidad de desempeñar las funciones básicas de la vida que son la nutrición, la relación y la reproducción, de tal manera que los seres vivos funcionan por sí mismos sin perder su nivel estructural hasta su muerte.[2]  La materia que compone los seres vivos está formada en un 95 % por cuatro elementos (bioelementos) que son el carbono, hidrógeno, oxígeno y nitrógeno, a partir de los cuales se forman biomoléculas:[3][4]  Estas moléculas se repiten constantemente en todos los seres vivos, por lo que el origen de la vida procede de un antecesor común, pues sería muy improbable que hayan aparecido independientemente dos seres vivos con las mismas moléculas orgánicas.[5][6] Se han encontrado microfósiles con una antigüedad de 3770-4280 millones de años, por lo que la vida podría haber surgido sobre la Tierra durante el Hádico.[1] Los relojes moleculares también la estiman en el Hádico.[7][8][9]  Todos los seres vivos están constituidos por células (véase teoría celular). En el interior de éstas se realizan las secuencias de reacciones químicas, catalizadas por enzimas, necesarias para la vida.  La ciencia que estudia los seres vivos es la biología.  Definiciones Resulta fácil, habitualmente, decidir si algo está vivo o no. Ello es debido a que los seres vivos comparten muchos atributos. Asimismo, la vida puede definirse según estas propiedades básicas de los seres vivos, que nos permiten diferenciarlos de la materia inerte:[10][11][12][13]  Autopoiesis Una forma alternativa de definir a los seres vivos es mediante el concepto de autopoiesis, introducido por los doctores Humberto Maturana y Francisco Varela. La idea es definir a los sistemas vivientes por su organización más que por un conglomerado de funciones.[15] Un sistema se define como autopoiético cuando las moléculas producidas generan la misma red que las produjo y especifican su extensión. Los seres vivos son sistemas que viven mientras conserven su organización. Todos sus cambios estructurales son para adaptarse al medio en el cual ellos existen. Para un observador externo al sistema, esta organización aparece como auto-referida. Las células son los únicos sistemas vivos primarios, es decir aquellos capaces de mantener su autopoiesis en forma autónoma. Los organismos pluricelulares formados por células poseen características similares a las de las células, particularmente el estado estable, pero su vida les es concedida por la organización autopoiética de las células que los constituyen.  Duración de la vida Uno de los parámetros básicos del ser vivo es su longevidad.[16] Algunos animales viven tan poco como un día, mientras que algunas plantas pueden vivir millares de años. El envejecimiento puede utilizarse para determinar la edad de la mayoría de los organismos, incluyendo las bacterias.  Composición química de los seres vivos Los organismos son sistemas físicos soportados por reacciones químicas complejas, organizadas de manera que promueven la reproducción y en alguna medida la sostenibilidad y la supervivencia.[17] Los seres vivos están integrados por moléculas inanimadas; cuando se examinan individualmente estas moléculas se observa que se ajustan a todas las leyes físicas y químicas que rigen el comportamiento de la materia inerte y las reacciones químicas son fundamentales a la hora de entender los organismos, pero es un error filosófico (reduccionismo) considerar a la biología como únicamente física o química. También juega un papel importante la interacción con los demás organismos y con el ambiente. De hecho, algunas ramas de la biología, por ejemplo la ecología, están muy alejadas de esta manera de entender a los seres vivos.  Los organismos son sistemas físicos abiertos ya que intercambian materia y energía con su entorno. Aunque son unidades individuales de vida no están aislados del medio ambiente que los rodea; para funcionar absorben y desprenden constantemente materia y energía. Los seres autótrofos producen energía útil (bajo la forma de compuestos orgánicos) a partir de la luz del sol o de compuestos inorgánicos, mientras que los heterótrofos utilizan compuestos orgánicos de su entorno.  Elementos químicos La materia viva está constituida por unos 60 elementos, casi todos los elementos estables de la Tierra, exceptuando los gases nobles. Estos elementos se llaman bioelementos o elementos biogénicos. Se pueden clasificar en dos tipos: primarios y secundarios.  El elemento químico fundamental de todos los compuestos orgánicos es el carbono. Las características físicas de este elemento tales como su gran afinidad de enlace con otros átomos pequeños, incluyendo otros átomos de carbono, y su pequeño tamaño le permiten formar enlaces múltiples y lo hacen ideal como base de la vida orgánica. Es capaz de formar compuestos pequeños que contienen pocos átomos (por ejemplo el dióxido de carbono) así como grandes cadenas de muchos miles de átomos denominadas macromoléculas; los enlaces entre átomos de carbono son suficientemente fuertes para que las macromoléculas sean estables y suficientemente débiles como para ser rotos durante el catabolismo; las macromoléculas a base de silicio (siliconas) son virtualmente indestructibles en condiciones normales, lo que las descartan como componentes de un ser vivo con metabolismo.  Macromoléculas Los compuestos orgánicos presentes en la materia viva muestran una enorme variedad y la mayor parte de ellos son extraordinariamente complejos. A pesar de ello, las macromoléculas biológicas están constituidas a partir de un pequeño número de pequeñas moléculas fundamentales (monómeros), que son idénticas en todas las especies de seres vivos. Todas las proteínas están constituidas solamente por 20 aminoácidos distintos y todos los ácidos nucleicos por cuatro nucleótidos. Se ha calculado que, aproximadamente un 90 % de toda la materia viva, que contiene muchos millones de compuestos diferentes, está compuesta, en realidad por unas 40 moléculas orgánicas pequeñas.[18]  Por ejemplo, aún en las células más pequeñas y sencillas, como la bacteria Escherichia coli, hay unos 5000 compuestos orgánicos diferentes, entre ellos, unas 3000 clases diferentes de proteínas y se calcula que en el cuerpo humano puede haber hasta 5 millones de proteínas distintas; además ninguna de las moléculas proteicas de E. coli es idéntica a alguna de las proteínas humanas, aunque varias actúen del mismo modo.[18]  La mayor parte de las macromoléculas biológicas que componen los organismos pueden clasificarse en uno de los siguientes cuatro grupos: ácidos nucleicos, proteínas, lípidos y glúcidos.  Los ácidos nucleicos (ADN y ARN) son macromoléculas formadas por secuencias de nucleótidos que los seres vivos utilizan para almacenar información. Dentro del ácido nucleico, un codón es una secuencia particular de tres nucleótidos que codifica un aminoácido particular, mientras que una secuencia de aminoácidos forma una proteína.  Las proteínas son macromoléculas formadas por secuencias de aminoácidos que debido a sus características químicas se pliegan de una manera específica y así realizan una función particular. Se distinguen las siguientes funciones de las proteínas:  Los lípidos forman la membrana plasmática que constituye la barrera que limita el interior de la célula y evita que las sustancias puedan entrar y salir libremente de ella. En algunos organismos pluricelulares se utilizan también para almacenar energía y para mediar en la comunicación entre células.  Los glúcidos (o hidratos de carbono) son el combustible básico de todas las células; la glucosa está al principio de una de las rutas metabólicas más antiguas, la glucólisis. También almacenan energía en algunos organismos (almidón, glucógeno), siendo más fáciles de romper que los lípidos, y forman estructuras esqueléticas duraderas, como la celulosa (pared celular de los vegetales) o la quitina (pared celular de los hongos, cutícula de los artrópodos).  Estructura Todos los seres vivos están formados por unidades denominadas células; algunos están formados por una única célula (unicelulares) mientras que otros contienen muchas (pluricelulares). Los organismos pluricelulares pueden especializar sus células para realizar funciones específicas. Así, un grupo de tales células forma un tejido. Los cuatro tipos básicos de tejidos en los animales son: epitelio, tejido nervioso, músculo y tejido conjuntivo. En las plantas pueden distinguirse tres tipos básicos de tejidos: fundamental, epidérmico y vascular. Varios tipos de tejido trabajan juntos bajo la forma de un órgano para producir una función particular (tal como el bombeo de la sangre por el corazón o como barrera frente al ambiente como la piel). Este patrón continúa a un nivel más alto con varios órganos funcionando como sistema orgánico que permiten la reproducción, digestión, etc. Muchos organismos pluricelulares constan de varios sistemas orgánicos que se coordinan para permitir vida.  La célula La teoría celular, propuesta en el año 1839 por Schleiden y Schwann, establece que todos los organismos están compuestos de una o más células; todas las células provienen de otras células preexistentes; todas las funciones vitales de un ser vivo ocurren dentro de las células, y las células contienen información hereditaria necesaria para las funciones de regulación de la célula y para transmitir información a la siguiente generación de células.  Todas las células tienen una membrana plasmática que rodea a la célula, separa el interior del medio ambiente, regula la entrada y salida de compuestos manteniendo de esta manera el potencial de membrana, un citoplasma salino que constituye la mayor parte del volumen de la célula y material hereditario (ADN y ARN).  Según la localización y la organización del ADN se distinguen dos tipos de células:  Todas las células comparten varias habilidades:  Simetría corporal Es la disposición de las estructuras corporales respecto de algún eje del cuerpo. Se clasifican en:  Ecología Los seres vivos pueden ser estudiados a muchos niveles diferentes: químico, celular, tejido, individuo, población, comunidad, ecosistema y biósfera. La ecología plantea una visión integradora de los seres vivos con el medio ambiente, considerando la interacción de los distintos organismos entre sí y con el medio físico, así como los factores que afectan a su distribución y abundancia. El medio ambiente incluye tanto los factores físicos (factores abióticos) locales, tales como el clima y la geología, como los demás organismos que comparten el mismo hábitat (factores bióticos).  Los procariontes y los eucariontes han evolucionado de acuerdo con estrategias ecológicas diferentes. Los procariontes son pequeños y sencillos: esto les otorgó la posibilidad de una alta velocidad de crecimiento y reproducción, por lo que alcanzan altos tamaños poblacionales en poco tiempo, que les permite ocupar nichos ecológicos efímeros, con fluctuaciones dramáticas de nutrientes. Por el contrario, los eucariontes, más complejos y de mayor tamaño, poseen un crecimiento y reproducción más lentos, pero han desarrollado la ventaja de ser competitivos en ambientes estables con recursos limitantes. No se debe caer en el error de considerar a los procariontes como evolutivamente más primitivos que los eucariontes, ya que ambos tipos de organismos se hallan bien adaptados a su ambiente, y ambos fueron seleccionados hasta la actualidad debido a sus estrategias ecológicas exitosas.[19]  Formas propuestas Aparte de los seres vivos propiamente mencionados también se ha propuesto incluir a otras formas biológicas como los virus y los agentes subvirales (virus satélite, viroides y  virusoides), nanobios, nanobacterias que generalmente no se consideran seres vivos porque no cumplen con todas las características que definen a los seres vivos. Puede consultarse el artículo (Formas propuestas de vida) para información sobre los argumentos a favor y en contra de su inclusión. En 2012 se llevó a cabo el descubrimiento de un organismo celular nombrado Parakaryon myojinensis que no encaja en ninguno de los tres dominios existentes. Se diferencia de los procariotas en que tiene núcleo y de los eucariotas en que carece de orgánulos. Además el material genético está almacenado en filamentos y no en cromosomas lineales y la pared celular está compuesta por peptidoglucanos, características similares a la que poseen las bacterias. Una característica que lo distingue tanto de los procariotas como de los eucariotas es la ausencia de flagelos y citoesqueleto.[20][21] Debido a ello algunos autores consideran que debería conformar su propio dominio Parakaryota.  Clasificación de los seres vivos Los seres vivos comprenden unos 1,9 millones de especies descritas y se clasifican en dominios y reinos. "
biologia,"El genotipo se refiere a la información genética que posee un organismo en particular, en forma de ADN.[1] Normalmente el genoma de una especie incluye numerosas variaciones o polimorfismos en muchos de sus genes.[2] El genotipado se usa para determinar qué variaciones específicas existen en el individuo. El genotipo, junto con factores ambientales que actúan sobre el ADN (adaptación genética al entorno), determina las características del organismo, es decir, su fenotipo.[3]De otro modo, el genotipo puede definirse como el conjunto de alelos/genes de un organismo y el fenotipo como el conjunto de rasgos de un organismo. Por tanto, los científicos y los médicos hablan a veces por ejemplo del genotipo de un cáncer particular, separando así la enfermedad del enfermo. Aunque pueden cambiar los codones para distintos aminoácidos por una mutación aleatoria (combinando la secuencia que codifica un gen, eso no altera necesariamente el fenotipo).  Es importante tener en cuenta que el genoma hace referencia a los genes de los que dispone una especie, sin tener en cuenta qué información genética viene codificada por dichos genes. Así, los humanos y los cerdos compartimos el gen que codifica para la insulina, aunque la secuencia de nucleótidos en el ADN (residuos de aminoácidos en la proteína) sea distinta. El genotipo, por tanto, hace referencia a qué viene escrito en el conjunto de genes que posee un individuo. Así, la hemoglobinopatía S, anemia drepanocítica o falciforme se produce como consecuencia de una alteración genética.[4]El gen que está mutado es el mismo en un individuo sano y en un individuo enfermo si hablamos en el contexto o dimensión de ""genoma""; la variación ocurre en el contexto de ""genotipo"", ya que, aun siendo los mismos genes, estos pueden contener información distinta (distintos alelos). La interacción de estos alelos y el ambiente hará que se exprese un fenotipo u otro (personas heterocigóticas no  suelen expresar la enfermedad a no ser que haya factores precipitantes: interacción alelo-ambiente).  Genotipo y fenotipo Toda la información contenida en los cromosomas se conoce como genotipo, sin embargo dicha información puede o no manifestarse en el individuo. El fenotipo es el resultado de la expresión del genotipo conjuntamente con la influencia del medio ambiente y factores epigenéticos.  El botánico neerlandés Wilhelm Johannsen acuñó tanto el término gen como la distinción entre genotipo y fenotipo. Normalmente se refiere al genotipo de un individuo con respecto a un gen de interés particular y, en individuos  poliploides, se refiere a la combinación de los alelos que porta el individuo homocigoto y heterocigoto. Un cambio en un cierto gen provocará normalmente un cambio observable en un organismo, conocido como el fenotipo. Los términos genotipo y fenotipo son distintos por al menos dos razones: para distinguir la fuente del conocimiento de un observador puede conocerse el genotipo observando el ADN; y puede conocerse el fenotipo observando la apariencia externa de un organismo.  El genotipo y el fenotipo no están siempre correlacionados directamente. Algunos genes solo expresan un fenotipo dado bajo ciertas condiciones ambientales. Inversamente, algunos fenotipos pueden ser el resultado de varios genotipos, lo que se conoce como pleiotropismo. La distinción entre genotipo y fenotipo se constata a menudo al estudiar los patrones familiares para ciertas enfermedades o condiciones hereditarias, por ejemplo la hemofilia. Algunas personas que no tienen hemofilia pueden tener hijos con la enfermedad, porque ambos padres ""portaban"" los genes de la hemofilia en su cuerpo, aunque estos no tenían efecto en la salud de los padres. Los padres, en este caso, se llaman portadores. La gente sana que no es portadora y la gente sana que es portadora del gen de la hemofilia tienen la misma apariencia externa es decir, no tienen la enfermedad, y por tanto se dice que tienen el mismo fenotipo. Sin embargo, los portadores tienen el gen y el resto de la gente no, tienen distintos genotipos.  Estudios de asociación Con un diseño experimental adecuado, uno puede utilizar métodos estadísticos para correlacionar diferencias en los genotipos de las poblaciones con diferencias en sus fenotipos observados. Estos estudios de asociación se pueden utilizar para determinar los factores de riesgo asociados con una enfermedad. Pueden servir incluso para diferenciar entre poblaciones que pueden responder favorablemente o no a un tratamiento medicamentoso particular. Este enfoque se conoce como genómica personalizada.  Informática Inspirada por el concepto biológico y la utilidad de los genotipos, la informática emplea genotipos simulados en la programación genética y los algoritmos genéticos. Estas técnicas pueden ayudar a evolucionar soluciones matemáticas a ciertos tipos de problemas difíciles. "
biologia,"El ácido ribonucleico (ARN) es un ácido nucleico formado por una cadena de ribonucleótidos.[1] Está presente tanto en las células procariotas como en las eucariotas, y es el único material genético de ciertos virus (los virus ARN).  El ARN se puede definir como una molécula formada por una cadena simple de ribonucleótidos, cada uno de ellos formado por ribosa, un fosfato y una de las cuatro bases nitrogenadas (adenina, guanina, citosina y uracilo). El ARN celular es lineal y monocatenario (de una sola cadena), pero en el genoma de algunos virus es de doble hebra.[2]  En los organismos celulares desempeña diversas funciones. Es la molécula que dirige las etapas intermedias de la síntesis proteica; el ADN no puede actuar solo, y se vale del ARN para transferir esta información vital durante la síntesis de proteínas (producción de las proteínas que necesita la célula para sus actividades y su desarrollo).[3] Varios tipos de ARN regulan la expresión génica, mientras que otros tienen actividad catalítica. El ARN es, pues, mucho más versátil que el ADN.  Descubrimiento e historia Los ácidos nucleicos fueron descubiertos en 1867 por Friedrich Miescher, que los llamó nucleína, ya que los aisló del núcleo celular.[4] Más tarde, se comprobó que las células procariotas, que carecen de núcleo, también contenían ácidos nucleicos. El papel del ARN en la síntesis de proteínas fue sospechado en 1939.[5] Severo Ochoa ganó el Premio Nobel de Medicina en 1959 tras descubrir cómo se sintetizaba el ARN.[6]  En 1965 Robert W. Holley halló la secuencia de 77 nucleótidos de un ARN de transferencia de una levadura,[7] con lo que obtuvo el Premio Nobel de Medicina en 1968. En 1967, Carl Woese comprobó las propiedades catalíticas de algunos ARN y sugirió que las primeras formas de vida usaron ARN como portador de la información genética tanto como catalizador de sus reacciones metabólicas (hipótesis del mundo de ARN).[8][9] En 1976, Walter Fiers y sus colaboradores determinaron la secuencia completa del ARN del genoma de un virus ARN (bacteriófago MS2).[10]  En 1990 se descubrió en Petunia que genes introducidos pueden silenciar genes similares de la misma planta, lo que condujo al descubrimiento del ARN interferente.[11][12] Aproximadamente al mismo tiempo se hallaron los micro-ARN, pequeñas moléculas de 22 nucleótidos que tenían algún papel en el desarrollo de Caenorhabditis elegans.[13] El descubrimiento de ARN que regulan la expresión génica ha permitido el desarrollo de medicamentos hechos de ARN, como los ARN pequeños de interferencia que silencian genes.[14]  En el año 2023 se tiene comprobado que las moléculas de ARN fueron la primera forma de vida propiamente dicha en habitar el planeta Tierra (Hipótesis del mundo de ARN).  Bioquímica del ARN Como el ADN, el ARN está formado por una cadena de monómeros repetitivos llamados nucleótidos. Los nucleótidos se unen uno tras otro mediante enlaces fosfodiéster cargados negativamente.  Cada nucleótido está formado por tres componentes:  Los carbonos de la ribosa se numeran de 1' a 5' en sentido horario. La base nitrogenada se une al carbono 1'; el grupo fosfato se une al carbono 5' y al carbono 3' de la ribosa del siguiente nucleótido. El pico tiene una carga negativa a pH fisiológico lo que confiere al ARN carácter polianiónico. Las bases púricas (adenina y guanina) pueden formar puentes de hidrógeno con las pirimidínicas (uracilo y citosina) según el esquema C=G y A=U.[15] Además, son posibles otras interacciones, como el apilamiento de bases[16] o tetrabucles con apareamientos G=A.[15] Muchos ARN contienen además de los nucleótidos habituales, nucleótidos modificados, que se originan por transformación de los nucleótidos típicos; son característicos de los ARN de transferencia (ARNt) y el ARN ribosómico (ARNr); también se encuentran nucleótidos metilados en el ARN mensajero eucariótico.[17]  Apareamiento doble La interacción por puentes de hidrógeno descrita por Watson y Crick[18] forma pares de bases entre una purina y una pirimidina. A este patrón se le conoce como apareamiento Watson y Crick. En este, la adenina se aparea con el uracilo (timina, en ADN) y la citosina con la guanina. Sin embargo, en el ARN se presentan muchas otras formas de apareamiento, de las cuales la más ubicua es el apareamiento wobble (también apareamiento por balanceo o apareamiento titubeante)  para la pareja G-U. Este fue propuesto por primera vez por Crick para explicar el apareamiento codón-anticodón en los ARNt y ha sido confirmado en casi todas las clases de ARN en los tres dominios filogenéticos.[19]  Estructura Estructura primaria Se refiere a la secuencia lineal de nucleótidos en la molécula de ARN. Los siguientes niveles estructurales (estructura secundaria, terciaria) son consecuencia de la estructura primaria. Además, la secuencia misma puede ser información funcional; esta puede traducirse para sintetizar proteínas (en el caso del ARNm) o funcionar como región de reconocimiento, región catalítica, entre otras.   Estructura primaria de tRNAPhe  Estructura secundaria El ARN se pliega como resultado de la presencia de regiones cortas con apareamiento intramolecular de bases, es decir, pares de bases formados por secuencias complementarias más o menos distantes dentro de la misma hebra. La estructura secundaria se refiere, entonces, a las relaciones de apareamiento de bases: «El término ‘estructura secundaria’ denota cualquier patrón plano de contactos por apareamiento de bases. Es un concepto topológico y no debe ser confundido con algún tipo de estructura bidimensional».[20] La estructura secundaria puede ser descrita a partir de motivos estructurales que se suelen clasificar de la siguiente manera:  (tallo, stack)  (ciclo, loop)  (tallo y bucle, hairpin loop)  (internal loop)  (buldge)  (helical junction)  Estructura terciaria La estructura terciaria es el resultado de las interacciones en el espacio entre los átomos que conforman la molécula. Algunas interacciones de este tipo incluyen el apilamiento de bases y los apareamientos de bases distintos a los propuestos por Watson y Crick, como el apareamiento Hoogsteen, los apareamientos triples y los zíperes de ribosa.   A diferencia del ADN las moléculas de ARN suelen ser de cadena simple y no forman dobles hélices extensas, no obstante, en las regiones con bases apareadas sí forma hélices como motivo estructural terciario. Una importante característica estructural del ARN que lo distingue del ADN es la presencia de un grupo hidroxil en posición 2' de la ribosa, que causa que las dobles hélices de ARN adopten una conformación A, en vez de la conformación B que es la más común en el ADN.[22] Esta hélice A tiene un surco mayor muy profundo y estrecho y un surco menor amplio y superficial.[23] Una segunda consecuencia de la presencia de dicho hidroxilo es que los enlaces fosfodiéster del ARN de las regiones en que no se forma doble hélice son más susceptibles de hidrólisis química que los del ADN; los enlaces fosfodiéster del ARN se hidrolizan rápidamente en disolución alcalina, mientras que los enlaces del ADN son estables.[24] La vida media de las moléculas de ARN es mucho más corta que las del ADN, de unos minutos en algunos ARN bacterianos o de unos días en los ARNt humanos.[17]  Biosíntesis La biosíntesis de ARN está catalizada normalmente por la enzima ARN polimerasa que usa una hebra de ADN como molde, proceso conocido con el nombre de transcripción. Por tanto, todos los ARN celulares provienen de copias de genes presentes en el ADN.  La transcripción comienza con el reconocimiento por parte de la enzima de un promotor, una secuencia característica de nucleótidos en el ADN situada antes del segmento que va a transcribirse; la doble hélice del ADN es abierta por la actividad helicasa de la propia enzima. A continuación, la ARN polimerasa progresa a lo largo de la hebra de ADN en sentido 3' → 5', sintetizando una molécula complementaria de ARN; este proceso se conoce como elongación, y el crecimiento de la molécula de ARN se produce en sentido 5' → 3'. La secuencia de nucleótidos del ADN determina también dónde acaba la síntesis del ARN, gracias a que posee secuencias características que la ARN polimerasa reconoce como señales de terminación.[25]  Tras la transcripción, la mayoría de los ARN son modificados por enzimas. Por ejemplo, al pre-ARN mensajero eucariota recién transcrito se le añade un nucleótido de guanina modificado (7-Metilguanosina) en el extremo 5' por medio de un puente de trifosfato formando un enlace 5'→ 5' único, también conocido como «capucha» o «caperuza», y una larga secuencia de nucleótidos de adenina en el extremo 3' (cola poli-A); posteriormente se le eliminan los intrones (segmentos no codificantes) en un proceso conocido como empalme o ayuste.  En virus, hay también varias ARN polimerasas ARN-dependientes que usan ARN como molde para la síntesis de nuevas moléculas de ARN. Por ejemplo, varios virus ARN, como los poliovirus, usan este tipo de enzimas para replicar su genoma.[26][27]  Clases de ARN El ARN mensajero (ARNm) es el tipo de ARN que lleva la información del ADN a los ribosomas, el lugar de la síntesis de proteínas. La secuencia de nucleótidos del ARNm determina la secuencia de aminoácidos de la proteína.[28] Por ello, el ARNm es denominado ARN codificante.  No obstante, muchos ARN no codifican proteínas, y reciben el nombre de ARN no codificantes; se originan a partir de genes propios (genes ARN), o son los intrones rechazados durante el proceso de empalme o ayuste. Son ARN no codificantes el ARN de transferencia (ARNt) y el ARN ribosómico (ARNr), que son elementos fundamentales en el proceso de traducción, y diversos tipos de ARN reguladores.[29]  Ciertos ARN no codificantes, denominados ribozimas, son capaces de catalizar reacciones químicas como cortar y unir otras moléculas de ARN,[30] o formar enlaces peptídicos entre aminoácidos en el ribosoma durante la síntesis de proteínas.[31]  ARN implicados en la síntesis de proteínas El ARN mensajero (ARNm) es el que lleva la información sobre la secuencia de aminoácidos de la proteína desde el ADN, lugar en que está inscrita, hasta el ribosoma, lugar en que se sintetizan las proteínas de la célula. Es por lo tanto, una molécula intermediaria entre el ADN y la proteína, y el apelativo de «mensajero», es del todo descriptivo. En los eucariotas, el ARNm se sintetiza en el nucleoplasma del núcleo celular y donde es procesado antes de acceder al citosol, donde se hallan los ribosomas, a través de los poros de la envoltura nuclear.  Los ARN de transferencia (ARNt) son cortos polímeros de unos 80 nucleótidos, que transfiere un aminoácido específico al polipéptido en crecimiento; se unen a lugares específicos del ribosoma durante la traducción. Tienen un sitio específico para la fijación del aminoácido (extremo 3') y un anticodón formado por un triplete de nucleótidos que se une al codón complementario del ARNm mediante puentes de hidrógeno.[29] Estos ARNt, al igual que otros tipos de ARN, pueden ser modificados post-transcripcionalmente por enzimas. La modificación de alguna de sus bases es crucial para la descodificación de ARNm y para mantener la estructura tridimensional del ARNt.[32]  El ARN ribosómico (ARNr) se halla combinado con proteínas para formar los ribosomas, donde representa unas 2/3 partes de los mismos. En procariotas, la subunidad mayor del ribosoma contiene dos moléculas de ARNr y la subunidad menor, una. En los eucariotas, la subunidad mayor contiene tres moléculas de ARNr y la menor, una. En ambos casos, sobre el armazón constituido por los ARNm se asocian proteínas específicas. El ARNr es muy abundante y representa el 80 % del ARN hallado en el citoplasma de las células eucariotas.[33] Los ARN ribosómicos son el componente catalítico de los ribosomas; se encargan de crear los enlaces peptídicos entre los aminoácidos del polipéptido en formación durante la síntesis de proteínas; actúan, pues, como ribozimas.  ARN reguladores Muchos tipos de ARN regulan la expresión génica gracias a que son complementarios de regiones específicas del ARNm o de genes del ADN.  Los ARN interferentes (ARNi) son moléculas de ARN que suprimen la expresión de genes específicos mediante mecanismos conocidos globalmente como ribointerferencia o interferencia por ARN. Los ARN interferentes son moléculas pequeñas (de 20 a 25 nucléotidos) que se generan por fragmentación de precursores más largos. Se pueden clasificar en tres grandes grupos"
biologia,"En biología, la mitosis es un proceso que ocurre en el núcleo de las células eucariotas y que precede inmediatamente a la división celular. Consiste en el reparto equitativo del material hereditario (ADN) característico.[1][2] Este tipo de división ocurre en las células somáticas y normalmente concluye con la formación de dos núcleos (cariocinesis), seguido de otro proceso independiente de la mitosis que consiste en la separación del citoplasma (citocinesis), para formar dos células hijas.  Fue descubierto por Walther Flemming, que entre 1879 y 1882 visualizó y describió cómo se dividían y se replicaban. Pero fue descrita por primera vez en 1848 por el botánico Wilhelm Hofmeister.  La mitosis completa, que produce células genéticamente idénticas, es el fundamento del crecimiento, de la reparación tisular y de la reproducción asexual. La otra forma de división del material genético de un núcleo se denomina meiosis.  Introducción La mitosis es la división del núcleo celular en la que se conserva intacta la información genética contenida en los cromosomas, que pasa de esta manera sin modificaciones a las dos células hijas resultantes. La mitosis es igualmente un verdadero proceso de multiplicación celular que participa en el desarrollo, el crecimiento y la regeneración del organismo. Este  proceso tiene lugar por medio de una serie de operaciones sucesivas que se desarrollan de una manera continua, pero para facilitar su estudio han sido separadas en varias etapas.  El resultado esencial de la mitosis es la continuidad de la información hereditaria de la célula madre en cada una de las dos células hijas. El genoma se compone de una determinada cantidad de genes organizados en cromosomas, hebras de ADN muy enrolladas que contienen la información genética vital para la célula y el organismo. Dado que cada célula debe contener completa la información genética propia de su especie, la célula madre debe hacer una copia de cada cromosoma antes de la mitosis, de forma que las dos células hijas reciban completa la información. Esto ocurre durante la fase S de la interfase, el período que alterna con la mitosis en el ciclo celular y en el que la célula entre otras cosas se prepara para dividirse.[3]  Tras la duplicación del ADN, cada cromosoma consistirá en dos copias idénticas de la misma hebra de ADN, llamadas cromátidas hermanas, unidas entre sí por una región del cromosoma llamada centrómero.[4] Cada cromática hermana no se considera en esa situación un cromosoma en sí mismo, sino parte de un cromosoma que provisionalmente consta de dos cromáticas.  En animales y plantas, pero no siempre en hongos o protistas, la envoltura nuclear que separa el ADN del citoplasma se desintegra, desapareciendo la capa que separaba el contenido nuclear del citoplasma. Los cromosomas se ordenan en el plano ecuatorial de la célula, perpendicular a un eje definido por un huso acromático. Este es una estructura citoesquelética compleja, de forma ahusada, constituido por fibras que son filamentos de microtúbulos. Las fibras del huso dirigen el reparto de las cromátidas hermanas, una vez producida su separación, hacia los extremos del huso.  Por convenio científico, a partir de este momento cada cromátida hermana sí se considera un cromosoma completo, y empezamos a hablar de cromosomas hermanos para referirnos a las estructuras idénticas que hasta ese momento llamábamos cromátidas. Como la célula se alarga, las fibras del huso «jalan» por el centrómero a los cromosomas hermanos dirigiéndolos cada uno a uno de los polos de la célula. En las mitosis más comunes, llamadas abiertas, la envoltura nuclear se deshace al principio de la mitosis y se forman dos envolturas nuevas sobre los dos grupos cromosómicos al acabar. En las mitosis cerradas, que ocurren por ejemplo en levaduras, todo el reparto ocurre dentro del núcleo, que finalmente se estrangula para formar dos núcleos separados.[5]  Se llama cariocinesis a la formación de los dos núcleos con que concluye habitualmente la mitosis. Es posible, y ocurre en ciertos casos, que el reparto mitótico se produzca sin cariocinesis (endomitosis) dando lugar a un núcleo con el material hereditario duplicado (doble número de cromosomas).  La mitosis se completa casi siempre con la llamada citocinesis o división del citoplasma. En las células animales la citocinesis se realiza por estrangulación: la célula se va estrechando por el centro hasta que al final se separa en dos. En las células de las plantas se realiza por tabicación, es decir, las células hijas “construyen” una nueva región de pared celular que dividirá la una de la otra dejando puentes de citoplasma (plasmodesmos). Al final, la célula madre se parte por la mitad, dando lugar a dos células hijas, cada una con una copia equivalente y completa del genoma original.  Cabe señalar que las células procariotas experimentan un proceso similar a la mitosis llamado fisión binaria. No se puede considerar que las células procariotas experimenten mitosis, dado que carecen de núcleo y únicamente tienen un cromosoma sin centrómero.[6]  Cariocinesis La cariocinesis (del griego cario = núcleo y cinesis = movimiento), mitosis astral o mitosis anfiastral, es la división del núcleo celular. Consiste en la primera fase de la mitosis, que es el proceso por el cual el material genético de una célula madre se distribuye de manera idéntica entre dos células hijas.  En células animales poseen un organelo no membranoso llamado áster o centro celular, formado por un par de centriolos, que al dividirse en profase temprana, se dirigen hacia los polos opuestos de la célula, formando el aparato del huso mitótico, acrosómico o acromático.  Fases del ciclo celular La división de las células eucariotas es parte de un ciclo vital continuo, el ciclo celular, en el que se distinguen dos períodos mayores, la interfase, durante la cual se produce la duplicación del ADN, y la mitosis, durante la cual se produce el reparto idéntico del material antes duplicado.   La mitosis es una fase relativamente corta en comparación con la duración de la interfase.  Interfase Durante la interfase, la célula se encuentra en estado basal de funcionamiento. En dicha fase se lleva a cabo la replicación del ADN y la duplicación de los orgánulos para tener un duplicado de todo antes de dividirse. Es la etapa previa a la mitosis donde la célula se prepara para dividirse, en ésta, los centríolos y la cromatina se duplican, aparecen los cromosomas los cuales se observan dobles. El primer proceso clave para que se dé la división celular es que todas las cadenas de ADN se dupliquen (replicación del ADN); esto se da inmediatamente antes de que comience la división, en un período del ciclo celular llamado interfase, que es aquel momento de la vida celular en que ésta no se está dividiendo. Tras la replicación tendremos dos juegos de cadenas de ADN, por lo que la mitosis consistirá en separar esas cadenas y llevarlas a las células hijas. Para conseguir esto se da otro proceso crucial que es la conversión de la cromatina en cromosomas.  La duración del ciclo celular en una célula típica es de 16 horas: 5 horas para G1, 7 horas para S, tres horas para G2 y 1 hora para la división. Este tiempo depende del tipo de célula que sea.[3]  Se produce en ella la condensación del material genético (ADN), para formar unas estructuras altamente organizadas, los cromosomas. Como el material genético se ha duplicado previamente durante la fase S de la Interfase, los cromosomas replicados están formados por dos cromátidas, unidas a través del centrómero por moléculas de cohesinas.  Uno de los hechos más tempranos de la profase en las células animales es la duplicación del centrosoma; los dos centrosomas hijos (cada uno con dos centriolos) migran entonces hacia extremos opuestos de la célula. Los centrosomas actúan como centros organizadores de unas estructuras fibrosas, los microtúbulos, controlando su formación mediante la polimerización de tubulina soluble.[7] De esta forma, el huso de una célula mitótica tiene dos polos que emanan microtúbulos.   En la profase tardía desaparece el nucléolo y se desorganiza la envoltura nuclear.  La envoltura nuclear se ha disuelto, y los microtúbulos (verde) invaden el espacio nuclear. Los microtúbulos pueden anclar cromosomas (azul) a través de los cinetocoros (rojo) o interactuar con microtúbulos emanados por el polo opuesto. Esto se denomina mitosis abierta. Los hongos y algunos protistas, como las algas o las tricomonas, realizan una variación denominada mitosis cerrada, en la que el huso se forma dentro del núcleo o sus microtúbulos pueden penetrar a través de la  envoltura nuclear intacta.[8][9]  Cada cromosoma ensambla dos cinetocoros hermanos sobre el centrómero, uno en cada cromátida. Un cinetocoro es una estructura proteica compleja a la que se anclan los microtúbulos.[10] Aunque la estructura y la función del cinetocoro no se conoce completamente, contiene varios motores moleculares, entre otros componentes.[11] Cuando un microtúbulo se ancla a un cinetocoro, los motores se activan, utilizando energía de la hidrólisis del ATP para ""ascender"" por el microtúbulo hacia el centrosoma de origen. Esta actividad motora, acoplada con la polimerización/despolimerización de los microtúbulos, proporciona la fuerza de empuje  necesaria para separar más adelante las dos cromátidas de los cromosomas.[11]  Cuando el huso crece hasta una longitud suficiente, los microtúbulos asociados a cinetocoros empiezan a buscar cinetocoros a los que anclarse. Otros microtúbulos no se asocian a cinetocoros, sino a otros microtúbulos originados en el centrosoma opuesto para formar el huso mitótico.[12] La prometafase se considera a veces como parte de la profase.  A medida que los microtúbulos encuentran y se anclan a los cinetocoros durante la prometafase, los centrómeros de los cromosomas se congregan en la ""placa metafásica"" o ""plano ecuatorial"", una línea imaginaria que es equidistante de los dos centrosomas que se encuentran en los 2 polos del huso.[12] Este alineamiento equilibrado en la línea media del huso se debe a las fuerzas iguales y opuestas que se generan por los cinetocoros hermanos. El nombre ""metafase"" proviene del griego μετα que significa ""después"".  Dado que una separación cromosómica correcta requiere que cada cinetocoro esté asociado a un conjunto de microtúbulos (que forman las fibras cinetocóricas), los cinetocoros que no están anclados generan una señal para evitar la progresión prematura hacia la anafase antes de que todos los cromosomas estén correctamente anclados y alineados en la placa metafásica. Esta señal activa el checkpoint de mitosis.[13]  Cuando todos los cromosomas están correctamente anclados a los microtúbulos del huso y alineados en la placa metafásica, la célula procede a entrar en anafase (del griego ανα que significa ""arriba"", ""contra"", ""atrás"" o ""re-""). Es la fase crucial de la mitosis, porque en ella se realiza la distribución de las dos copias de la información genética original.   Entonces tienen lugar dos sucesos. Primero, las proteínas que mantenían unidas ambas cromátidas hermanas (las cohesinas), son cortadas, lo que permite la separación de las cromátidas. Estas cromátidas hermanas, que ahora son cromosomas hermanos diferentes, son separados por los microtúbulos anclados a sus cinetocoros al desensamblarse, dirigiéndose hacia los centrosomas respectivos.   A continuación, los microtúbulos no asociados a cinetocoros se alargan, empujando a los centrosomas (y al conjunto de cromosomas que tienen asociados) hacia los extremos opuestos de la célula. Este movimiento parece estar generado por el rápido ensamblaje de los microtúbulos.[14]  Estos dos estados se denominan a veces anafase temprana (A) y anafase tardía (B). La anafase temprana viene definida por la separación de cromátidas hermanas, mientras que la tardía por la elongación de los microtúbulos que produce la separación de los centrosomas. Al final de la anafase, la célula ha conseguido separar dos juegos idénticos de material genético en dos grupos definidos, cada uno alrededor de un centrosoma.   La telofase (del griego τελος, que significa ""finales"") es la reversión de los procesos que tuvieron lugar durante la profase y prometafase. Durante la telofase, los microtúbulos no unidos a cinetocoros continúan alargándose, estirando aún más la célula. Los cromosomas hermanos se encuentran cada uno asociado a uno de los polos. La envoltura nuclear se reforma alrededor de ambos grupos cromosómicos, utilizando fragmentos de la envoltura nuclear de la célula original. Ambos juegos de cromosomas, ahora formando dos nuevos núcleos, se descondensan de nuevo en cromatina. "
biologia,"La división celular es una parte muy importante del ciclo celular en la que una célula inicial se divide para formar células hijas.[1]  Debido a la división celular se produce el crecimiento de los seres vivos. En los organismos pluricelulares este crecimiento se produce gracias al desarrollo de los tejidos y en los seres unicelulares mediante la reproducción asexual.  El ciclo celular es el conjunto ordenado de sucesos que conducen al crecimiento de la célula y la división en dos células hijas. Estos procesos incluyen la duplicación previa del material genético (genoma) y la posterior segregación de los cromosomas duplicados en las células hijas, además de la multiplicación de otros orgánulos y macromoléculas celulares.[2]  Los seres pluricelulares reemplazan su dotación celular gracias a la división celular y suele estar asociada con la diferenciación celular. En algunos animales la división celular se detiene en algún momento y las células acaban envejeciendo. Las células senescentes se deterioran y mueren debido al envejecimiento del cuerpo. Las células dejan de dividirse porque los telómeros se vuelven cada vez más cortos en cada división y no pueden proteger a los cromosomas como tal.  Las células hijas de las divisiones celulares, en el desarrollo temprano embrionario, contribuyen de forma desigual a la generación de los tejidos adultos.  Variantes de división celular Las células se clasifican ampliamente en dos categorías principales: células procariotas no nucleadas simples y células eucariotas nucleadas complejas. Debido a sus diferencias estructurales, las células eucariotas y procariotas no se dividen de la misma manera. En eucariotas, además, el patrón de división celular que transforma las células madre eucariotas en gametos (espermatozoides en los machos u óvulos en las hembras), denominado meiosis, es diferente al de la división de las células somáticas en organismos pluricelulares (mitosis).  División celular en procariotas La división de las células bacterianas ocurre a través de la fisión binaria o la gemación. El divisoma es un complejo de proteínas responsable de la división celular en bacterias, la constricción de las membranas internas y externas durante la división y la síntesis de peptidoglicano (PG) en el sitio de división. Una proteína similar a la tubulina, FtsZ, juega un papel crítico en la formación de un anillo contráctil para la división celular.  División celular en eucariotas La división celular en eucariotas es mucho más compleja que en procariotas. En eucariotas, la fase M del ciclo celular comprende dos procesos: la división del núcleo (mitosis) y la división del citoplasma (citocinesis).[2] Dependiendo del número cromosómico reducido o no, las divisiones de células eucariotas se pueden clasificar como mitosis (división ecuacional) y meiosis (división reduccional).  También se encuentra una forma primitiva de división celular que se llama amitosis. La división celular, amitótica o mitótica, es más atípica y diversa en diversos grupos de organismos tales como protistas (a saber, diatomeas, dinoflagelados, etc.) y hongos.[3]  En la metafase mitótica (ver más abajo), típicamente los cromosomas (cada uno con 2 cromátidas hermanas que desarrollaron debido a la replicación en la fase S de la interfase) se ordenan y las cromátidas hermanas se dividen y distribuyen hacia las células hijas.  En la meiosis, típicamente en la Meiosis-I, los cromosomas homólogos se emparejan y luego se separan y distribuyen en células hijas. La meiosis-II es como una mitosis en la que se separan las cromátidas. En humanos y otros animales superiores y muchos otros organismos, la meiosis se llama meiosis gamética, es decir, la meiosis da lugar a gametos. En otros grupos de organismos, especialmente en plantas (observable en plantas inferiores pero etapa vestigial en plantas superiores), la meiosis da lugar al tipo de esporas que germinan en fase vegetativa haploide (gametofito). Este tipo de meiosis se llama meiosis espórica.  Tipos de reproducción asociados a la división celular Bipartición: es la división de la célula madre en dos células hijas, cada nueva célula es un nuevo individuo con estructuras y funciones idénticas a la célula madre. Este tipo de reproducción la presentan organismos como bacterias, amebas y algas.  Gemación: se presenta cuando unos nuevos individuos se producen a partir de yemas. El proceso de gemación es frecuente en esponjas, celentereos y briozoos. En una zona son varias del organismo progenitor se produce una envaginación o yema que se va desarrollando y en un momento dado sufre una constricción en la base y se separa del progenitor comenzando su vida como nuevo ser. Las yemas hijas pueden presentar otras yemas a las que se les denomina yemas secundarias.  En algunos organismos se pueden formar colonias cuando las yemas no se separan del organismo progenitor. En las formas más evolucionadas de briozoos se observa en el proceso de gemación que se realiza de forma más complicada. La gemación es el proceso evolutivo del ser vivo por meiosis. El número de individuos de una colonia, la manera en que están agrupados y su grado de diferenciación varía y a menudo es característica de una especie determinada. Los briozoos pueden originar nuevos individuos sobre unas prolongaciones llamados estolones y al proceso se le denomina estolonización.  Ciertas especies de animales pueden tener gemación interna, yemas que sobreviven en condiciones desfavorables, gracias a una envoltura protectora. En el caso de las esponjas de agua dulce, las yemas tienen una cápsula protectora y en el interior hay sustancia de reserva. Al llegar la primavera se pierde la cápsula protectora y a partir de la yema surge la nueva esponja. En los briozoos de agua dulce se produce una capa de quitina y de calcio y no necesitan sustancia de reserva pues se encuentra en estado de hibernación.  Esporulación: esputación o esporogénesis consiste en un proceso de diferenciación celular para llegar a la producción de células reproductivas dispersivas de resistencia llamadas esporas. Este proceso ocurre en hongos, amebas, líquenes, algunos tipos de bacterias, protozoos, esporozoos (como el Plasmodium causante de malaria), y es frecuente en vegetales (especialmente algas, musgos y helechos), grupos de muy diferentes orígenes evolutivos, pero con estrategias reproductivas semejantes, todos ellos pueden recurrir a la formación de células de resistencia para favorecer la dispersión. Durante la esporulación se lleva a cabo la división del núcleo en varios fragmentos, y por una división celular asimétrica una parte del citoplasma rodea cada nuevo núcleo dando lugar a las esporas. Dependiendo de cada especie se puede producir un número apreciable de esporas y a partir de cada una de ellas se desarrollará un individuo independiente.  La división celular es el proceso por el cual el material celular se divide entre dos nuevas células hijas. En los organismos unicelulares esto aumenta el número de individuos de la población. En las plantas y organismos multicelulares es el procedimiento en virtud del cual crece el organismo, partiendo de una sola célula, y también son reemplazados y reparados los tejidos estropeados.  Procesos de división celular Los seres pluricelulares reemplazan su dotación celular gracias a la división celular y suele estar asociada a la diferenciación celular. En algunos animales, la división celular se detiene en algún momento y las células acaban envejeciendo. Las células senescentes se deterioran y mueren, debido al envejecimiento del cuerpo. Las células dejan de dividirse porque los telómeros se vuelven cada vez más cortos en cada división y no pueden proteger a los cromosomas. Las células cancerosas se consideran ""inmortales"" debido a que una enzima llamada telomerasa permite a estas células dividirse indefinidamente.  La característica principal de la división celular en organismos eucariotas es la conservación de los mecanismos genéticos del control del ciclo celular y de la división celular, puesto que se ha mantenido prácticamente inalterable desde organismos tan simples como las levaduras a criaturas tan complejas como el ser humano, a lo largo de la evolución biológica.  Factores que explican la división celular Una teoría afirma que existe un momento en el que la célula comienza a crecer mucho, lo que hace que disminuya la proporción área/volumen. Cuando el área de la membrana plasmática de la célula es mucho más pequeña en relación con el volumen total de ésta, se presentan dificultades en la reabsorción y en el transporte de nutrientes, siendo así necesario que se produzca la división celular.  Hay tres tipos de reproducción celular: la fisión binaria, relativamente simple y dos tipos más complicados que implican tanto la mitosis o la meiosis.   Los organismos como las bacterias típicamente tienen un solo cromosoma. Al inicio del proceso de fisión binaria, la molécula de ADN del cromosoma de la célula se replica, produciendo dos copias del cromosoma. Un aspecto clave de la reproducción celular de la bacteria es asegurarse de que cada célula hija recibe una copia del cromosoma. Citocinesis es la separación física de las dos células hijas nuevas.   La mayoría de los organismos eucariotas como los humanos tienen más de un cromosoma. Con el fin de asegurarse de que una copia de cada cromosoma sea segregado en cada célula hija se utiliza el huso mitótico. Los cromosomas se mueven a lo largo de los microtúbulos largos y delgados como los trenes en movimiento a lo largo de las vías del tren. Los seres humanos son diploides, tenemos dos copias de cada tipo de cromosoma, uno del padre y uno de la madre.   Las células sexuales, denominadas también «gametos»,  son producidas por meiosis. Para la producción de espermatozoide hay dos pasos (citocinesis) que producen un total de cuatro células N, cada una con la mitad del número normal de cromosomas. La situación es diferente: en los ovarios la producción de huevos en uno de los cuatro conjuntos de cromosomas que se segrega se coloca en una célula huevo grande, listo para ser combinado con el ADN de una célula de esperma (véase la meiosis para más detalles).  Envejecimiento, cáncer y división celular Los organismos pluricelulares reemplazan las células desgastadas a través de la división celular. En algunos animales, sin embargo, la división celular finalmente se detiene. En los humanos esto ocurre, en promedio, después de 52 divisiones, lo que se conoce como límite de Hayflick.[4] La célula se denomina entonces senescente. Con cada división, los telómeros de las células, secuencias protectoras de ADN en el extremo de un cromosoma que evitan la degradación del ADN cromosómico, se acortan. Este acortamiento se ha correlacionado con efectos negativos como enfermedades relacionadas con la edad y la reducción de la esperanza de vida en los seres humanos. Por otro lado, no se cree que las células cancerosas se degraden de esta manera, si es que lo hacen. Un complejo enzimático llamado telomerasa, presente en grandes cantidades en las células cancerosas, reconstruye los telómeros mediante la síntesis de repeticiones de ADN telomérico, lo que permite que la división continúe indefinidamente.[5][6][7]  Divisiones silenciosas Las divisiones silenciosas son divisiones sin mutaciones. A veces se confunden con mutaciones que no son detectables. Pero se puede comprobar si las divisiones son silenciosas por comparación de las frecuencias de los alelos de la línea celular, con la suma de las frecuencias de los alelos de las dos células derivadas, lo cual tiene que ser similar. No se puede detectar una división que origina una célula sin mutaciones (silenciosa) y otra muerta (que no contribuye al adulto)"
biologia,"Un microorganismo, también llamado microbio (del griego científico μικρόβιος [microbios]; de μικρός [micrós], ""pequeño"", y βίος [bíos], ‘vida’;[1] ser vivo diminuto) o microbionte,  es un ser vivo o un sistema biológico que solo puede visualizarse con el microscopio. Son organismos dotados de individualidad (unicelulares) que presentan, a diferencia de las plantas y los animales, una organización biológica elemental. La disciplina científica que estudia los microorganismos es la microbiología.   El concepto de microorganismo es operativo y carece de cualquier implicación taxonómica o filogenética, dado que engloba organismos unicelulares heterogéneos, que no están relacionados evolutivamente entre sí, tales como bacterias, arqueas (procariotas), protozoos, algas microscópicas y hongos microscópicos (eucariotas).  Los microbios tienen múltiples formas y tamaños. Si un virus de tamaño promedio tuviera el tamaño de una pelota de tenis, una bacteria sería del tamaño de media cancha de tenis y una célula eucariota sería como un estadio entero de fútbol.[2]  Algunos microorganismos son patógenos y causan enfermedades a personas, animales y plantas, algunas de las cuales han sido un azote para la humanidad desde tiempos inmemoriales. No obstante, la inmensa mayoría de los microbios no son en absoluto perjudiciales y bastantes juegan un papel clave en la biosfera al proporcionar oxígeno (algas y cianobacterias), y, otros, al descomponer la materia orgánica, mineralizarla y hacerla de nuevo accesible a los productores, cerrando el ciclo de la materia.  Historia del descubrimiento de los microorganismos Antonie van Leeuwenhoek (1632–1723) fue uno de los primeros en observar los microorganismos, utilizando microscopios de diseño propio.[3] Robert Hooke, un contemporáneo de Leeuwenhoek, también utilizó microscopios para observar la vida microbiana; en su libro de 1665, Micrographia, describió esas observaciones y acuñó el término de célula.  Antes del descubrimiento de los microorganismos de Leeuwenhoek, en 1675, había sido un misterio por qué las uvas podían convertirse en vino, la leche en queso, o por qué los alimentos se echaban a perder. Leeuwenhoek no hizo la conexión entre estos procesos y los microorganismos, pero usando un microscopio estableció que había allí signos de vida que no eran visibles a simple vista.[4][5] El descubrimiento de Leeuwenhoek, junto con las observaciones posteriores de Spallanzani y Pasteur, terminaron con la antigua creencia de que la vida aparecía espontáneamente a partir de sustancias muertas durante el proceso de deterioro.  Lazzaro Spallanzani (1729–1799) encontró que hirviendo caldo lo esterilizaba, matando a los microorganismos en él. También encontró que los nuevos microorganismos sólo podían instalarse en un caldo si el caldo se exponía al aire.  Louis Pasteur (1822–1895) amplió los hallazgos de Spallanzani mediante la exposición al aire de caldos hervidos, en recipientes que contenían un filtro que evitaba que cualquier partícula pasara al medio de crecimiento, y también en recipientes sin ningún filtro, que admitían aire a través de un tubo curvado que no permitía que las partículas de polvo entrasen en contacto con el caldo. Hirviendo el caldo con antelación, Pasteur se aseguró de que no había microorganismos supervivientes en los caldos al comienzo del experimento. Nada crecía en los caldos en el curso del experimento de Pasteur. Esto significaba que los organismos vivos que crecían en estos caldos venían desde afuera, como esporas en polvo, en lugar de generarse espontáneamente en el caldo. Por lo tanto, Pasteur dio el golpe a la teoría de la generación espontánea, dando apoyo a la teoría microbiana de la enfermedad.  En 1876 Robert Koch (1843–1910) estableció que los microorganismos pueden causar enfermedades. Encontró que la sangre del ganado que estaba infectado con ántrax siempre tenía un gran número de Bacillus anthracis.  Koch descubrió que podía transmitir el ántrax de un animal a otro, tomando una pequeña muestra de sangre del animal infectado e inyectándola en uno sano, que hacía que el animal enfermase. También descubrió que podía hacer crecer la bacteria en un caldo nutriente, luego lo inyectaba en un animal sano, y causaba la enfermedad. Basándose en estos experimentos, ideó los criterios para establecer una relación causal entre un microorganismo y una enfermedad, ahora conocidos como los postulados de Koch.[6] Aunque estos postulados no pueden aplicarse en todos los casos, conservan su importancia histórica en el desarrollo del pensamiento científico y todavía se utilizan hoy en día.[7]  El 8 de noviembre de 2013 se informó del descubrimiento de lo que pueden ser los primeros signos de vida en la Tierra: los fósiles completos más antiguos de una estera microbiana (asociada con arenisca en Australia occidental) que se estima que tienen 3480 millones de años.[8][9]  Clases de microorganismos En los microorganismos están representados cuatro grupos de seres: bacterias, arqueas, protistas y hongos. Los virus generalmente no se consideran seres vivos y de hecho se los asocía más a los elementos genéticos móviles, viroides y priones por lo que no se les consideran microorganismos,[10][11] aunque la virología, el estudio de los virus es un subcampo de la microbiología.[12][13][14]  Microorganismos procariotas Las bacterias y las arqueas son microorganismos procariontes de forma esférica (cocos), de bastón recto (bacilos) o curvado (vibrios), o espirales (espirilos). Pueden existir como organismos individuales, formando cadenas, pares, tétradas, masas irregulares, etc. Las bacterias son una de las formas de vida más abundantes en la tierra. Tienen una longitud entre 0,4 y 14 μm. Consecuentemente solo se pueden ver mediante microscopio. Las bacterias se reproducen mediante la multiplicación del ADN, y división en dos células independientes; en circunstancias normales este proceso dura entre 30 y 60 minutos.  Cuando las condiciones del medio son desfavorables, cuando cambia la temperatura o disminuye la cantidad de los nutrientes, determinadas bacterias forman endosporas como mecanismo de defensa, caracterizadas por presentar una capa protectora resistente al calor, a la desecación, a la radiación y a la trituración mecánica y que protege la bacteria de manera muy eficiente. De esta manera, pueden soportar temperaturas elevadas, periodos de sequía, heladas, etc. Cuando las condiciones del medio mejoran, se desarrolla una nueva bacteria que continúa el crecimiento y la multiplicación.  Las bacterias tienen un papel funcional ecológico específico. Por ejemplo, algunas realizan la degradación de la materia orgánica, otras integran su metabolismo con el de los seres humanos.  Si bien algunas bacterias son patógenas (causantes de diversas enfermedades), una gran parte de ellas son inocuas o incluso buenas para la salud.  Microorganismos eucariotas Se denomina eucariotas a todas las células que tienen su material hereditario (su información genética) encerrado dentro de una doble membrana, la envoltura nuclear, que delimita un núcleo celular.  Hay tres tipos de microorganismos eucariotas, los protozoos (heterótrofos y sin pared celular), las algas microscópicas (autótrofos y con pared celular de celulosa) y los hongos microscópicos (heterótrofos y con pared celular de quitina).  Los protozoos y las algas microscópicas son microorganismos unicelulares del reino Protista cuyo tamaño va de 10-60 μm hasta más de 1 milímetro, y pueden fácilmente ser vistos a través de un microscopio. Viven en ambientes húmedos o directamente en medios acuáticos, ya sean aguas saladas o aguas dulces. La reproducción puede ser asexual por bipartición y también sexual por isogametos o por conjugación intercambiando material genético. Los protozoos son heterótrofos, fagótrofos, depredadores o detritívoros, a veces mixótrofos (parcialmente autótrofos), mientras que las algas microscópicas son autótrofos, sin embargo algunas pueden mixótrofos (parcialmente autótrofos)  En el reino Protista se incluye grupos muy diversos, algunos de los cuales están relacionados con el origen de los animales, hongos y plantas por lo que se le considera un taxón parafilético.  El reino Fungi incluye una variedad de especies microscópicas que en absoluto no encajan en la definición de microorganismo, pero también formas microscópicas, como las levaduras, que son campo de estudio de la microbiología. Los hongos poseen un amplio rango de hábitats, que incluyen ambientes extremos como los desiertos, áreas de extremada salinidad, expuestas a radiación ionizante, o en los sedimentos de los fondos marinos;  la mayoría son terrestres, aunque algunos, como Batrachochytrium dendrobatidis son estrictamente acuáticos. Además, numerosos hongos producen enfermedades infecciosas en animales y plantas y tienen un gran interés sanitario y agropecuario.  Microorganismos patógenos Algunos microorganismos son capaces de penetrar y multiplicarse en otros seres vivos, a los que perjudican, originando una infección; son los denominados microorganismos patógenos. Los problemas que causa una infección dependen del tipo de patógeno, el modo en que se transfiere, dosis o concentración de patógenos, persistencia de los microorganismos y la resistencia del organismo infectado.  La dosis de infección significa el número de microorganismos. Esta dosis es muy baja para los protozoos parásitos. La persistencia de los microorganismos depende del tiempo viable de los microorganismos cuando no se encuentran en el huésped humano. Por ejemplo, las bacterias son generalmente menos persistentes mientras los quistes de los protozoos son los más persistentes.  Los jóvenes, personas mayores y enfermos de otras patologías son los menos resistentes a las enfermedades y por lo tanto son más vulnerables. Cuando una persona es infectada, los patógenos se multiplican en ella, y esto supone un riesgo de infección o enfermedad.  Las personas que enferman pueden contagiar y extender la enfermedad mediante las secreciones y mediante contacto directo de alguna manera con la mucosa del infectado.  Métodos de cultivo Existen dos grandes clasificaciones en cuanto a los métodos de cultivo de microorganismos: aerobios y anaerobios. Normalmente, se incuban en condiciones aerobias, es decir, en condiciones atmosféricas normales; esta técnica es la más sencilla. Con ella proliferan del mismo modo microorganismos aerobios y anaerobios facultativos. Sin embargo, algunas bacterias aisladas tan solo se reproducen en condiciones de estricta anaerobiosis. Así pues, hay que recurrir a un medio de cultivo en el que previamente ha sido eliminado todo el oxígeno atmosférico y ha sido substituido por otro gas (nitrógeno)."
biologia,"Un organismo unicelular es aquel que está constituido por una sola célula en comparación con los organismos pluricelulares constituidos por varias células. Algunos ejemplos de organismos unicelulares son la mayoría de los procariotas (bacterias y arqueas), los protozoos, algunos hongos como las levaduras y algunas algas como las diatomeas.   Los organismos unicelulares representan la inmensa mayoría de los seres vivos que pueblan actualmente la Tierra; en número sobrepasan con mucho a los organismos pluricelulares en el planeta.   La mayoría de organismos unicelulares son procariotas, como las bacterias, pero existen algunos organismos unicelulares eucariotas, como los protozoos.  La circulación en los organismos unicelulares se realiza por el movimiento del citoplasma de la célula que se denomina ciclosis.  Los organismos unicelulares están constituidos por una célula, en cambio los organismos pluricelulares están formados por varias células juntas especializadas en determinadas funciones. Juntas hacen tejidos; esos tejidos se unen y forman órganos, y un conjunto de órganos forman un sistema de órganos, y finalmente una agrupación de estos forma un organismo complejo.  Al ser organismos conformados por una única célula, los organismos unicelulares se clasifican como organismos microscópicos, sin embargo existen excepciones; por ejemplo los xenofióforos,[1] son foraminíferos unicelulares que han desarrollado un gran tamaño, los cuales alcanzan tamaños macroscópicos de hasta 20 cm.  Origen  Los protobiontes un tipo de estructura abiótica lipídica, fueron los precursores de los organismos unicelulares actuales. Aunque el origen de la vida sigue siendo en gran parte un misterio, la teoría que prevalece actualmente, conocida como la hipótesis del mundo de ARN sostiene que, las primeras moléculas de ARN habrían sido la base para catalizar las reacciones químicas orgánicas y la autorreplicación.  La compartimentación era necesaria para que las reacciones químicas fueran más probables, así como para diferenciar las reacciones con el entorno externo. Por ejemplo, una ribozima replicadora de ARN temprana o un viroide puede haber replicado otras ribozimas replicadoras de diferentes secuencias de ARN si no se mantienen separadas.   Cuando los anfífilos, como los lípidos, se colocan en agua, las colas hidrofóbicas (temerosas del agua) se agregan para formar micelas y vesículas, con los extremos hidrofílicos (amantes del agua) mirando hacia afuera. Es probable que los protobiontes usaran vesículas de ácidos grasos autoensambladas para separar las reacciones químicas del medio ambiente. Debido a su simplicidad y capacidad para autoensamblarse en agua, es probable que estas membranas simples fueran anteriores a otras formas de moléculas biológicas tempranas.   Se cree que el proceso de la abiogénesis duró entre 4500 y 4350 millones de años y que los primeros organismos unicelulares surgieron hace 4350 millones de años con el último antepasado común universal (LUCA).[2][3]Las bacterias y las arqueas pudieron surgir hace 4330 millones de años y los eucariotas hace 2500 millones de años. Tal como lo sugiere el registro fósil y los relojes moleculares. Hasta ahora los fósiles más antiguos de bacterias y arqueas tienen 4280-3770 millones de años y los de eucariotas 2200 millones de años.  Procariotas Los procariotas carecen de orgánulos unidos a la membrana, como las mitocondrias o un núcleo.[4] En cambio, la mayoría de los procariotas tienen una región irregular que contiene ADN, conocida como nucleoide.[5]La mayoría de los procariotas tienen un cromosoma circular único, que contrasta con los eucariotas, que normalmente tienen cromosomas lineales.[6] Nutricionalmente, los procariotas tienen la capacidad de utilizar una amplia gama de material orgánico e inorgánico para su uso en el metabolismo, incluidos azufre, celulosa, amoníaco o nitrito.[7] Los procariotas en su conjunto son ubicuos en el medio ambiente y también existen en ambientes extremos.  Bacterias Las bacterias son una de las formas de vida más antiguas del mundo y se encuentran prácticamente en todas partes en la naturaleza.[7] Muchas bacterias comunes tienen plásmidos, que son moléculas de ADN cortas, circulares y autorreplicantes que están separadas del cromosoma bacteriano.[8] Los plásmidos pueden portar genes responsables de nuevas capacidades, siendo la resistencia a los antibióticos de importancia crítica en la actualidad.[9]Las bacterias se reproducen predominantemente asexualmente a través de un proceso llamado fisión binaria. Sin embargo, alrededor de 80 especies diferentes pueden sufrir un proceso sexual conocido como transformación genética natural.[10] La transformación es un proceso bacteriano para transferir ADN de una célula a otra y aparentemente es una adaptación para reparar el daño del ADN en la célula receptora. [11] Además, los plásmidos se pueden intercambiar mediante el uso de un pilus en un proceso conocido como conjugación.[9]   Las cianobacterias fotosintéticas son posiblemente las bacterias más exitosas y cambiaron la atmósfera primitiva de la tierra oxigenándola. [12] Los estromatolitos, estructuras formadas por capas de carbonato de calcio y sedimentos atrapados que quedan de las cianobacterias y las bacterias comunitarias asociadas, dejaron extensos registros fósiles.[12]La existencia de estromatolitos da un registro excelente en cuanto al desarrollo de cianobacterias, que están representadas en los eones Arcaico (hace 4 mil millones a 2.5 mil millones de años), Proterozoico (hace 2.5 mil millones a 540 millones de años) y Fanerozoico (540 millones de años hasta la actualidad).[13]Gran parte de los estromatolitos fosilizados del mundo se pueden encontrar en Australia Occidental. Allí, se han encontrado algunos de los estromatolitos más antiguos, algunos datan de hace unos 3.460 millones de años.[13]   El envejecimiento clonal ocurre naturalmente en las bacterias y aparentemente se debe a la acumulación de daño que puede ocurrir incluso en ausencia de factores estresantes externos.[14]  Arqueas Las arqueas son similares en apariencia a las bacterias y por ello se clasificaron tradicionalmente como bacterias, pero tienen diferencias moleculares significativas más notables en su estructura de membrana y ARN ribosómico.[15] Al secuenciar el ARN ribosómico, se encontró que las arqueas se separaron tempranamente de las bacterias verdaderas y fueron las precursoras de los eucariotas y de hecho algunas arqueas están más relacionadas filogenéticamente con los eucariotas que con otras arqueas. Se creé que los eucariotas surgieron de una asgardarqueota en un evento endosimbiótico con una alfaproteobacteria que originaría las mitocondrias. Como sugiere su nombre, ""Archaea"" proviene de una palabra griega archaios, que significa original, antiguo o primitivo.[16]   Algunas arqueas habitan en los entornos biológicamente más inhóspitos de la tierra, y se cree que de alguna manera imitan las condiciones tempranas y duras a las que probablemente estuvo expuesta la vida. Ejemplos de estos extremófilos arcaicos son los siguientes:  Los metanógenos son un subconjunto significativo de arqueas e incluyen muchos extremófilos, pero también son ubicuos en ambientes de humedales, así como en los rumiantes y el intestino posterior de los animales. Este proceso utiliza hidrógeno para reducir el dióxido de carbono en metano, liberando energía en la forma utilizable de trifosfato de adenosina. Son los únicos organismos conocidos capaces de producir metano. Bajo condiciones ambientales estresantes que causan daño al ADN, algunas especies de arqueas se agregan y transfieren ADN entre las células. La función de esta transferencia parece ser reemplazar la información de la secuencia de ADN dañada en la célula receptora por información de la secuencia no dañada de la célula donante.  Eucariotas Los eucariotas contienen orgánulos unidos a la membrana, como mitocondrias, un núcleo y cloroplastos. Las células procariotas probablemente dieron origen a las células eucariotas hace 2500 millones de años.[17] Este fue un paso importante en la evolución. A diferencia de los procariotas, los eucariotas se reproducen mediante mitosis y meiosis. El sexo parece ser un atributo omnipresente, antiguo e inherente de la vida eucariota.[18]La meiosis, un verdadero estado sexual, permite la reparación recombinacional eficiente del daño del ADN  y una mayor variedad de diversidad genética al combinar el ADN de los padres seguido de la recombinación. Las funciones metabólicas en eucariotas son más especializadas y también al dividir procesos específicos en orgánulos.  La teoría endosimbiótica ampliamente aceptada sostiene que las mitocondrias y los cloroplastos tienen un origen bacteriano. Ambos orgánulos contienen sus propios conjuntos de ADN y tienen ribosomas similares a bacterias. Es probable que las mitocondrias modernas fueran una vez una alfaproteobacteria similar a Rickettsia, con la capacidad parasitaria de entrar en una célula.[19]Sin embargo, si las bacterias fueran capaces de respirar, habría sido beneficioso para la célula más grande permitir que el parásito viviera a cambio de energía y desintoxicación de oxígeno. Los cloroplastos probablemente se convirtieron en simbilantes a través de un conjunto similar de eventos, y son descendientes de cianobacterias.[20]Si bien no todos los eucariotas tienen mitocondrias o cloroplastos, las mitocondrias se encuentran en la mayoría de los eucariotas y los cloroplastos se encuentran en todas las plantas y algas. La fotosíntesis y la respiración son esencialmente opuestas, y el advenimiento de la respiración junto con la fotosíntesis permitió un acceso mucho mayor a la energía que la fermentación sola.  Protozoos Los protozoos se definen en gran medida por su método de locomoción, incluidos flagelos, cilios y pseudópodos.[21]Los protozoos, al igual que las plantas y los animales, se pueden considerar heterótrofos o autótrofos. Los autótrofos como Euglena son capaces de producir su energía mediante la fotosíntesis, mientras que los protozoos heterótrofos consumen alimentos canalizándolos a través de un esófago con forma de boca o envolviéndolos con pseudópodos, una forma de fagocitosis. Los protozoos se reproducen mayormente de manera asexual, aunque algunos son capaces de reproducirse sexualmente. Ejemplos de protozoos con capacidad de reproducción sexual son las especies patógenas Plasmodium falciparum, Toxoplasma gondii, Trypanosoma brucei, Giardia duodenalis y Leishmania.   Los ciliados, son un grupo de protozoos que utilizan cilios para la locomoción. Los ejemplos incluyen Paramecium, Stentor y Vorticella. Los ciliados son muy abundantes en casi todos los entornos donde se puede encontrar agua, y los cilios laten rítmicamente para impulsar al organismo. Muchos ciliados tienen tricocistos, que son orgánulos en forma de lanza que se pueden descargar para atrapar presas, anclarse o para defenderse. Los ciliados también son capaces de reproducirse sexualmente y utilizan dos núcleos exclusivos de los ciliados: un macronúcleo para el control metabólico normal y unmicronúcleo que sufre meiosis. Ejemplos de tales ciliados son Paramecium y Tetrahymena que probablemente emplean la recombinación meiótica para reparar el daño del ADN adquirido en condiciones estresantes.  Las amebas utilizan pseudópodos y flujo citoplásmico para moverse en su entorno. Entamoeba histolytica es la causa de la amebiasis y parece ser capaz de producir meiosis.  Algas unicelulares Las algas unicelulares son autótrofas similares a las plantas y contienen clorofila.[22]Incluyen grupos que tienen especies tanto multicelulares como unicelulares:  Hongos unicelulares Los hongos unicelulares incluyen las levaduras, los opistosporidios y algunos quitridios. Los hongos se encuentran en la mayoría de los hábitats,[26]aunque la mayoría se encuentran en tierra. Las levaduras se reproducen a través de la mitosis, y muchas usan un proceso llamado gemación, donde la mayor parte del citoplasma está retenido por la célula madre.[27] Saccharomyces cerevisiae fermenta los carbohidratos en dióxido de carbono y alcohol. Se utiliza en la elaboración de cerveza y pan. S. cerevisiae también es un organismo modelo importante, ya que es un organismo eucariota que es fácil de cultivar. Se ha utilizado para investigar el cáncer y las enfermedades neurodegenerativas, así como para comprender el ciclo celular. Además, la investigación con S. cerevisiae ha jugado un papel central en la comprensión del mecanismo de recombinación meiótica y la función adaptativa de la meiosis. Candida responsable de la candidiasis, causa infecciones de la boca y / o garganta (conocidas como aftas) y la vagina (comúnmente llamadas infecciones por hongos).[28]  Nuevo tipo (paracariotas) En 2012 en fuentes hidrotermales de Japón se llevó a cabo el descubrimiento de Parakaryon myojinensis un organismo unicelular que presenta características que no encajan con las células de los otros tres dominios y probablemente constituya su propio dominio Parakaryota, es decir organismos paracariotas. Parakaryon myojinensis al igual que los eucariotas tiene núcleo y otros endosimbiontes en su célula, sin embargo su envoltura nuclear es de una sola capa, no hecha de dos membranas concéntricas como en cualquier eucariota y el material genético está almacenado como en las bacterias, en filamentos y no en cromosomas lineales. Además no presenta retículo endoplásmico, aparato de Golgi, citoesqueleto, mitocondrias, poros nucleares y carece totalmente de flagelo. "
biologia,"El material genético se emplea para guardar la información genética de una forma de vida orgánica y, en eucariotas, está almacenado en el núcleo de la célula. Para todos los organismos conocidos actualmente, el material genético es casi exclusivamente ácido desoxirribonucleico (ADN). Algunos genomas de virus usan ácido ribonucleico (ARN) en vez de ADN.  Se cree generalmente que el primer material genético fue el ARN, (ácido ribonucleico) inicialmente manifestado por moléculas de ARN que autoreplicaban flotando en masas de agua. Este período hipotético en la evolución de la vida celular se llama la hipótesis del mundo de ARN. Esta hipótesis  está basada en la capacidad del ARN de actuar como un material genético y como un catalizador, conocido como una ribozima. Sin embargo, cuando las proteínas (que pueden formar enzimas) llegaron a la existencia, la molécula más estable, el ADN, se convirtió en el material genético dominante, una situación que continúa hoy. La naturaleza de la doble cadena del ADN permite que las mutaciones se corrijan, y para construir proteínas de las instrucciones del ADN, en la forma de ARN mensajero, ARN ribosómico y ARN de transferencia.  El ARN y el ADN son macromoléculas compuestas de nucleótidos (Son polinucleótidos), de los cuales hay cuatro en cada molécula. Tres nucleótidos componen un codón, un tipo de ""palabra genética"", que es como un aminoácido en una proteína. La traducción codón-aminoácido se conoce como traducción.  Los cromosomas están formados por la sustancia del ADN(ácido desoxirribonucleico) Cada característica del ser humano, como la cantidad de melanina en ciertas partes del cuerpo (iris, piel, pelo), morfología del cuerpo (altura, masa muscular, forma de las orejas, tipo de cabello, etc.), Características no visibles (metabolismo, presión arterial, sistema inmunológico, sexualidad, etc.) O entre otras muchas más cosas, está codificada en una parte del ADN. Cada de una de estas partes se llama gen, que son pequeños fragmentos de cromosomas que portan la información para una característica determinada. Al borde del núcleo, encontramos jugo nuclear el cual se encarga de sostener el ADN.  "
biologia,"La microbiología es la ciencia encargada del estudio y el análisis de los microorganismos,[1] seres vivos diminutos no visibles al ojo humano (del griego «μικρος» mikros ""pequeño"", «βιος» bios, ""vida"" y «-λογία» -logía, tratado, estudio, ciencia), también conocidos como microbios.[2] Se dedica a estudiar los organismos que son solo visibles a través del microscopio: organismos procariotas y eucariotas simples. Son considerados microbios todos aquellos seres vivos microscópicos, estos pueden estar constituidos por una sola célula (unicelulares), así como pequeños agregados celulares formados por células equivalentes (sin diferenciación celular); estos pueden ser eucariotas (células que poseen envoltura nuclear) tales como hongos y protistas; y procariotas (células sin envoltura nuclear) como las bacterias. Sin embargo la microbiología tradicional se ha ocupado especialmente de los microorganismos patógenos entre bacterias, virus y hongos, dejando a otros microorganismos en manos de la parasitología y otras categorías de la biología.  Aunque los conocimientos microbiológicos de que se dispone en la actualidad son muy amplios, todavía es mucho lo que queda por conocer y constantemente se efectúan nuevos descubrimientos en este campo. Tanto es así que, según las estimaciones más habituales, solo un 1 % de los microbios existentes en la biosfera han sido estudiados hasta el momento. Por lo tanto, a pesar de que han pasado más de 300 años desde el descubrimiento de los microorganismos, la ciencia de la microbiología se halla todavía en su infancia en comparación con otras disciplinas biológicas tales como la zoología, la botánica o incluso la entomología.  Al tratar la microbiología sobre todo los microorganismos patógenos para el hombre, se relaciona con categorías de la medicina como patología, inmunología y epidemiología.  Historia La microbiología, como ciencia, existe aproximadamente desde la segunda mitad del siglo XIX. En el siglo III antes de Cristo, Teofrasto, sucesor de Aristóteles en el liceo, escribió gruesos volúmenes acerca de las propiedades curativas de las plantas. Aunque el término bacteria, derivado del griego βακτηριον (""bastoncillo""), no fue introducido hasta el año 1828 por Christian Gottfried Ehrenberg, ya en 1676 Anton van Leeuwenhoek, usando un microscopio de una sola lente que él mismo había construido basado en el modelo creado por el erudito Robert Hooke en su libro Micrographia, realizó la primera observación microbiológica registrada de ""animáculos"", como van Leeuwenhoek los llamó y dibujó entonces.  Eugenio Espejo (1747-1795) publicó importantes trabajos de medicina, como las Reflexiones acerca de la viruela (1785), el cual se convertiría en el primer texto científico que refería la existencia de microorganismos (inclusive antes que Louis Pasteur) y que definiría como política de salud conceptos básicos de la actualidad como la asepsia y antisepsia de lugares y personas.  La bacteriología (más tarde una subdisciplina de la microbiología) se considera fundada por el botánico Ferdinand Cohn (1828-1898). Cohn fue también el primero en formular un esquema para la clasificación taxonómica de las bacterias.  Louis Pasteur (1822-1895), considerado el padre de la Microbiología Médica, y Robert Koch (1843-1910) fueron contemporáneos de Cohn. Quizá el mayor triunfo de Pasteur consistió en la refutación mediante cuidadosos experimentos de la por aquel entonces muy respetada teoría de la generación espontánea, lo cual permitió establecer firmemente a la microbiología dentro de las ciencias biológicas. Pasteur también diseñó métodos para la conservación de los alimentos (pasteurización) y vacunas contra varias enfermedades como el carbunco, el cólera aviar y la rabia. Robert Koch es especialmente conocido por su contribución a la teoría de los gérmenes de la enfermedad, donde, mediante la aplicación de los llamados postulados de Koch, logró demostrar que enfermedades específicas están causadas por microorganismos patogénicos específicos. Koch fue uno de los primeros científicos en concentrarse en la obtención de cultivos puros de bacterias, lo cual le permitió aislar y describir varias especies nuevas de bacterias, entre ellas Mycobacterium tuberculosis, el agente causal de la tuberculosis.  Mientras Louis Pasteur y Robert Koch son a menudo considerados los fundadores de la microbiología, su trabajo no reflejó fielmente la auténtica diversidad del mundo microbiano, dado su enfoque exclusivo en microorganismos de relevancia médica. Dicha diversidad no fue revelada hasta más tarde, con el trabajo de Martinus Beijerinck (1851-1931) y Sergei Winogradsky (1856-1953). Martinus Beijerinck hizo dos grandes contribuciones a la microbiología: el descubrimiento de los virus y el desarrollo de técnicas de cultivo microbiológico. Mientras que su trabajo con el virus del mosaico del tabaco estableció los principios básicos de la virología, fue su desarrollo de nuevos métodos de cultivo el que tuvo mayor impacto inmediato, pues permitió el cultivo de una gran variedad de microbios que hasta ese momento no habían podido ser aislados. Sergei Winogradsky fue el primero en desarrollar el concepto de quimiolitotrofía y de este modo revelar el papel esencial que los microorganismos juegan en los procesos geoquímicos. Fue el responsable del aislamiento y descripción por vez primera tanto de las bacterias nitrificantes como de las fijadoras de nitrógeno.  El cirujano inglés Joseph Lister (1827-1912) aportó pruebas indirectas de que los microorganismos eran agentes de enfermedades humanas, a través de sus estudios sobre la prevención de infecciones de heridas. Lister, impresionado por las investigaciones de Pasteur sobre la participación de los microorganismos en la fermentación y la putrefacción, desarrolló un método de cirugía antiséptica, con el fin de evitar que los microorganismos penetrasen en las heridas. Los instrumentos se esterilizaban con calor y se trataban los vendajes quirúrgicos con fenol, que de vez en cuando se empleaba para rociar el campo quirúrgico. Este método tuvo resultados muy satisfactorios y transformó la cirugía después de que Lister publicase sus resultados en 1867.Al mismo tiempo, aportaba pruebas indirectas sobre el papel de los microorganismos en las enfermedades, pues el fenol, que destruía las bacterias, evitaba las infecciones en las heridas.  Empirismo y especulación El conocimiento humano sobre los efectos producidos por los microorganismos ha estado presente incluso desde antes de tener conciencia de su existencia; debido a procesos de fermentación provocados por levaduras se puede hacer pan, bebidas alcohólicas y productos derivados de la leche. En la antigüedad la causa de las enfermedades era atribuida a castigos divinos, fuerzas sobrenaturales o factores físicos (La palabra malaria significa “mal aire”, se creía que era el aire viciado de los pantanos el que provocaba esta enfermedad). Durante este periodo previo al descubrimiento de los microorganismos, los naturalistas solo podían especular sobre el origen de las enfermedades.  Tipos de microbiología El campo de la microbiología puede ser dividido en varias subdisciplinas:  Subdisciplinas y otras disciplinas relacionadas Beneficios de la microbiología Históricamente, los microorganismos han sido vistos de manera negativa a causa de su asociación con muchas enfermedades humanas y animales. Sin embargo, los microorganismos patológicos son un porcentaje muy minoritario dentro del total de microorganismos, la mayoría de los cuales desempeñan papeles absolutamente imprescindibles y que de no existir harían inviable la vida en la Tierra. Algunos ejemplos son las bacterias que fijan nitrógeno atmosférico (posibilitando la vida de los organismos vegetales), las bacterias del ciclo del carbono (indispensables para reincorporar al suelo la materia orgánica) o la multitud de microorganismos que viven de manera simbiótica en nuestro tubo digestivo, sin las cuales la digestión no sería viable. Así pues, los ""organismos superiores"" (animales, plantas, etc.) no podríamos vivir de no ser por las funciones desempeñadas por estos seres microscópicos. Además, tienen amplias aplicaciones en el terreno industrial, como las fermentaciones (por ejemplo para la producción de bebidas alcohólicas o productos lácteos), la producción de antibióticos o la de otros productos de interés farmacéutico o biotecnológico (hormonas, enzimas, etc.).[6] Finalmente, cabe también destacar el papel esencial que los microorganismos juegan en los laboratorios de investigación biológica de todo el mundo como herramientas para la clonación de genes y la producción de proteínas.  Refutación de la teoría de la generación espontánea En el siglo XIX tuvo lugar una gran polémica sobre la teoría de la generación espontánea. La idea básica de la generación espontánea puede comprenderse fácilmente. El alimento se pudre si permanece durante cierto tiempo a la intemperie. Cuando este material putrefacto se examina al microscopio, se observa que está repleto de bacterias. ¿De dónde provienen estas bacterias que no se ven en el alimento fresco? Algunos pensaban que provenían de semillas o gérmenes que llegaban al alimento a través del aire, mientras otros opinaban que se originaban espontáneamente a partir del material inerte.  El adversario más ferviente de la generación espontánea fue el químico francés Louis Pasteur, cuyo trabajo sobre este problema fue el más riguroso y convincente. En primer lugar, Pasteur demostró que en el aire había estructuras que se parecían mucho a los microorganismos encontrados en el material putrefacto. Descubrió que el aire normal contiene continuamente una diversidad de células microbianas que son indistinguibles de las que se encuentran en mucha mayor cantidad en los materiales en putrefacción. Por tanto, concluyó que los organismos encontrados en tales materiales se originaban a partir de microorganismos presentes en el aire. Además postuló que dichas células en suspensión se depositan constantemente sobre todos los objetos. Pasteur pensó que si sus conclusiones eran correctas, entonces no debería estropearse un alimento tratado, de tal modo que todos los organismos que lo contaminaran fueran destruidos.  Pasteur empleó el calor para eliminar los contaminantes, pues ya se sabía que el calor destruye con efectividad los organismos vivos. De hecho, otros investigadores ya habían mostrado que si una solución de nutrientes se introducía en un matraz de vidrio, se sellaba y se calentaba luego hasta ebullición, nunca se descomponía. Los defensores de la generación espontánea criticaban tales experimento argumentando que se necesitaba aire fresco para la generación espontánea y que el aire dentro del matraz cerrado se modificaba por el calentamiento, de modo que no era capaz de permitir la generación espontánea. Pasteur superó esta objeción de modo simple y brillante, construyendo un matraz con forma de cuello de cisne, que ahora se designa como un matraz Pasteur. En tales recipientes, las soluciones nutritivas se podían calentar hasta ebullición; luego, cuando el matraz se enfriaba, el aire podía entrar de nuevo, pero la curvatura del cuello del matraz, evitaba que el material particulado, las bacterias y otros microorganismos, alcanzasen el interior del matraz. El material esterilizado en tal recipiente no se descomponía y no aparecían microorganismos mientras el cuello del matraz no contactara con el líquido estéril. Sin embargo, bastaba con que el cuello del matraz se inclinara lo suficiente como para permitir que el líquido estéril contactara con el cuello, para que ocurriera la putrefacción y el líquido se llenara de microorganismos. Este sencillo experimento bastó para aclarar definitivamente la controversia sobre la generación espontánea.  Eliminar todas las bacterias o microorganismos de un objeto es un proceso que ahora denominamos esterilización. Los procedimientos que usaron Pasteur, Cohn y otros investigadores fueron finalmente mejorados y aplicados a la investigación microbiológica. El fin de la teoría de la generación espontánea condujo, por tanto, al desarrollo de procedimientos eficaces de esterilización, sin los cuales la microbiología no podría haberse desarrollado como ciencia.  La microbiología en la actualidad Actualmente, el conocimiento microbiológico se ha especializado tanto que lo encontramos divididos: la microbiología médica estudia los microorganismos patógenos y la posible cura para las enfermedades que producen, la inmunología averigua las causas de la aparición de las enfermedades desde una perspectiva inmunológica, la microbiología ecológica estudia el nicho que le corresponde a los microorganismos en el medio, la microbiología agricultural las relaciones existentes entre plantas y microorganismos, y la biotecnología los posibles beneficios que puede llevar para el hombre la explotación de microbios.  Importancia Los microbiólogos han hecho contribuciones a la biología y a la medicina, especialmente en los campos de la bioquímica, genética y biología celular. Los microorganismos tienen muchas características que los hacen ""organismos modelo"" ideales:  La importancia de la microbiología se fundamenta en sus repercusiones en variados aspectos de la vida cotidiana, que no se limitan en forma excluyente a las ciencias de la salud. Por el contrario, el conocimiento de las formas de vida microscópicas genera impacto en áreas como la industria, los recursos energéticos y la administración pública. "
biologia,"En biología y taxonomía, Eukaryota o Eukarya (del griego: εὖ eu —‘bueno’, ‘bien’, 'verdadero'— y κάρυον karyon —‘nuez’, ‘carozo’, ‘núcleo’—) es el dominio (o imperio) que incluye los organismos formados por células con núcleo verdadero. La castellanización adecuada del término es eucariota o eucarionte.[5] Estos organismos constan de una o más células eucariotas, abarcando desde organismos unicelulares hasta verdaderos pluricelulares en los que las diferentes células se especializan para diferentes tareas y que, en general, no pueden sobrevivir de forma aislada.   Pertenecen al dominio o imperio eucariota los reinos de los animales, plantas y hongos, así como varios grupos incluidos en el parafilético reino Protista. Todos ellos presentan semejanzas a nivel molecular (estructura de los lípidos, proteínas y genoma), comparten un origen común, y principalmente, comparten el plan corporal de los eucariotas, muy diferente al de procariotas.  Con excepción de algunos organismos unicelulares, el ciclo de vida eucariota alterna una fase haplonte y otra diplonte, que se consigue mediante la alternancia de meiosis y fecundación, procesos que dan células haplontes y diplontes respectivamente.  Estructura celular Las células eucariotas son generalmente mucho más grandes que las procariotas y están mucho más compartimentadas. Poseen una gran variedad de membranas con núcleo rodeado de la envoltura nuclear, retículo endoplasmático y aparato de Golgi, además de mecanismos para la gemación y fusión de vesículas, incluida la exocitosis y endocitosis. Estructuras internas llamadas orgánulos se encargan de realizar funciones especializadas dentro de la célula. Presencia de lisosomas, peroxisomas y mitocondrias.  También caracteriza a todos los eucariotas un esqueleto interno o endoesqueleto, en este caso llamado citoesqueleto, formado por dos entramados de proteínas: el sistema de microtúbulos y el sistema contráctil de actina/miosina, que desempeñan un papel importante en la definición de la organización y forma de la célula, en el tráfico intracelular (por ejemplo, los movimientos de vesículas y orgánulos) y en la división celular. El característico flagelo eucariota y sus motores moleculares asociados se encuentran anclados al citoesqueleto.  El ADN de las células eucariotas está contenido en un núcleo celular separado del resto de la célula por una doble membrana permeable. El material genético se divide en varios bloques lineales llamados cromosomas, que son separados por un huso microtubular durante la división nuclear. Los cromosomas contienen histonas, varios replicones, centrómeros y telómeros. Hay un característico ciclo celular con segregación mitótica y reproducción sexual por meiosis. Se incluye un complejo de poros nucleares, transporte trans-membranal de ARN y proteínas a través de la envoltura nuclear, intrones y nuevos patrones de procesamiento del ARN utilizando espliceosomas.   Para una comparación con las características procariotas, véase: Tabla comparativa.  Plan corporal La célula eucariota debe en gran parte su forma y capacidad de movimiento al citoesqueleto, ya que le otorga rigidez y flexibilidad. En los organismos flagelados ancla los flagelos al resto de la célula y permite su batido durante la locomoción o para la creación de corrientes de agua que le lleven el alimento. En los organismos ameboides permite la extensión de ""pies"" o seudópodos para la locomoción o la alimentación. También fija los surcos de alimentación de los excavados y el complejo apical que permite a los apicomplejos entrar en las células parasitadas.  Solo después de desarrollar su citoesqueleto pudo el eucariota ancestral realizar la fagocitosis, ya que es este el que, mediante crecimiento diferencial de sus fibras, logra que la célula se deforme para que la fagocitosis ocurra. La fagocitosis es también una propiedad ancestral de los eucariotas, si bien se ha perdido en grupos que se adaptaron a otras formas de alimentación. Hongos y plantas perdieron esta capacidad al desarrollar una pared celular rígida externa a la célula, pero ya contaban con otros modos de nutrición, la saprotrofia o el parasitismo en hongos y la fotosíntesis en plantas.  La mitocondria, derivada de la fagocitosis y posterior simbiogénesis de una proteobacteria, permitió al eucariota ancestral la respiración aerobia y con ello aprovechar al máximo la energía contenida en la materia orgánica. Como no es sorprendente en la evolución de un carácter tan antiguo, en varios grupos la mitocondria ha perdido esa capacidad ancestral y a cambio se ha modificado para cumplir otras funciones. También proceden de un evento de endosimbiosis los cloroplastos, en este caso con una cianobacteria, que permiten a las plantas realizar la fotosíntesis. Posteriormente otros grupos de eucariotas consiguieron sus cloroplastos mediante la endosimbiosis secundaria con un alga verde o roja.  Reproducción Además de la división asexual de las células (mitosis), la mayoría de los eucariontes tiene algún proceso de reproducción sexual basado en la meiosis que no se encuentra entre los procariontes. La reproducción de los eucariontes típicamente implica la existencia de una fase haploide, donde está presente solamente una copia de cada cromosoma en las células, y diploide, donde están presentes dos. Las células diploides surgen por fusión nuclear (fecundación) y las haploides, por meiosis. En los organismos multicelulares, se distinguen tres tipos de ciclos biológicos:  Los organismos unicelulares pueden reproducirse asexualmente por bipartición, gemación o esporulación y sexualmente mediante gametos o por conjugación.[6]  En los eucariontes, la relación de superficie frente a volumen es más pequeña que los procariontes, y así tienen tasas metabólicas más bajas y tiempos de generación más largos.  Origen El origen de la célula eucariota es el proceso biológico más revolucionario desde el origen de la vida desde varios puntos de vista, como es el caso de la morfología, desarrollo evolutivo, estructura genética, relaciones simbióticas y ecología.[7] Todas las células complejas son de este tipo y constituyen la base de casi todos los organismos pluricelulares.[8] Aunque no hay acuerdo sobre cuándo se originaron los eucariotas, en general, se ha sugerido a comienzos del Paleoproterozoico hace unos 2500 millones de años.[9][10] Hasta ahora el fósil más antiguo que puede considerarse eucariota tiene 2200 millones de años y se le conoce como Diskagma el cual representa los primeros indicios de vida pluricelular junto con la biota francevillense de hace 2100 millones de años. La separación entre los eucariotas y su grupo hermano las arqueas Asgard se estimó a finales del Arcaico (periodo Neoarcaico).[10]  Eukarya se relaciona con Archaea desde el punto de vista del ADN nuclear y de la maquinaria genética, y ambos grupos son clasificados a veces juntos en el clado Neomura. Desde otros puntos de vista, tales como por la composición de la membrana, se asemejan más a Bacteria. Se han propuesto para ello tres posibles explicaciones principales:[12][13][14]  Cada vez son mayores las evidencias que parecen demostrar que el origen eucariota es producto de la fusión de una arquea y una bacteria. Mientras el núcleo celular tiene elementos genéticos relacionados con las arqueas, las mitocondrias y la membrana celular tienen características bacterianas. La fusión genética es más evidente al constatar que los genes informativos parecen de origen arqueano y los genes operacionales de origen bacteriano. En todo caso también es cierto que un cierto número de rasgos presentes exclusivamente en los eucariontes son difíciles de explicar por medio de un evento de fusión.[15]  Tampoco está claro el retraso de mil millones de años entre el origen de los eucariotas y su diversificación, pues las bacterias dominaron la biosfera hasta hace unos 800 millones de años.[16] Este intervalo de estabilidad ambiental, litosférica y evolutiva se conoce con el nombre de ""boring billion"" (""millardo -de años- aburrido"").[17] Este retraso quizá se debiera simplemente a la dificultad de introducción en una biosfera ocupada enteramente por los procariotas. Otras explicaciones están relacionadas con la lenta evolución de los eucariotas o con el aumento del oxígeno, que no alcanzó los niveles ideales para los eucariotas hasta el final de dicho período. El diseño compartimentado de la célula eucariota es el más adecuado para el metabolismo aerobio y es comúnmente aceptado que todos los eucariotas actuales, incluidos los anaerobios, descienden de antecesores aerobios con mitocondrias.[18][19]  Evolución Algún tiempo después de que surgiera la primera célula eucariota se produjo una radiación explosiva que las llevó a ocupar la mayoría de los nichos ecológicos disponibles.  Evolución unicelular La primera célula eucariota era probablemente flagelada, aunque con tendencias ameboides al no tener una cubierta rígida.[21] Desde el antecesor flagelado, algunos grupos perdieron ulteriormente los flagelos, mientras que otros se convirtieron en multiflagelados o ciliados. Cilios y flagelos (incluidos los que tienen los espermatozoides) son estructuras homólogas con nueve dobletes de microtúbulos que se originan a partir de los centriolos.[22]  El carácter ameboide surgió varias veces a lo largo de la evolución de los protistas dando lugar a los diversos tipos de seudópodos de los distintos grupos. El que los ameboides procedan de los flagelados y no al revés, como se pensaba en el pasado, tiene como base estudios moleculares (fusión, partición o duplicación de genes, inserción o borrado de intrones, etc.).[23] [24]  Está generalmente aceptado que los cloroplastos se originaron por endosimbiosis de una cianobacteria y que todas las algas eucariotas evolucionaron en última instancia de antepasados heterótrofos. Se piensa que la diversificación primaria de la célula eucariota tuvo lugar entre los zooflagelados: células predadoras no fotosintéticas con uno o más flagelos para nadar, y a menudo también para generar corrientes de agua con las que capturar a las presas.[21]  En la actualidad hay discrepancia en dónde debe ponerse la raíz del árbol de Eukarya. La posibilidad más aceptada es situarlo entre o próximo a los excavados, que serían el grupo basal de los eucariontes.[3][25]  Evolución pluricelular Durante la primera parte de su historia los eucariontes permanecieron unicelulares. A partir del período Ediacárico los pluricelulares comienzan a profilerar, aunque el proceso con seguridad comenzó bastante antes. Los organismos unicelulares de vida colonial comenzaron a cumplir funciones específicas en una zona del colectivo. Se formaron así los primeros tejidos y órganos. La pluricelularidad se desarrolló independientemente en varios grupos de eucariontes: plantas, hongos, animales, algas rojas, algas pardas y mohos mucilaginosos. A pesar de su pluricelularidad, estos dos últimos grupos se siguen clasificando en el reino Protista.   Las algas verdes, las primeras plantas, se desarrollaron para formar las primeras hojas. En el Silúrico surgen las primeras plantas terrestres y de ellas las plantas vasculares o cormófitas.  Los hongos unicelulares constituyeron filas de células o hifas que agrupadas se convirtieron en organismos pluricelulares absortivos con un marcado micelio. Inicialmente, los hongos fueron acuáticos y probablemente en el período Silúrico apareció el primer hongo terrestre, justo después de la aparición de las primeras plantas terrestres. Estudios moleculares sugieren que los hongos están más relacionados con los animales que con las plantas.  El reino animal comenzó con organismos similares a los actuales poríferos que carecen de verdaderos tejidos. Posteriormente se diversifican para dar lugar a los distintos grupos de invertebrados y vertebrados.  Clasificación y filogenia Los eucariontes se dividen tradicionalmente en cuatro reinos: Protista, Plantae, Animalia y Fungi (aunque Cavalier-Smith 2004,[30] 2015[31] reemplaza Protista por dos nuevos reinos, Protozoa y Chromista). Esta clasificación es el punto de vista generalmente aceptado en actualidad, aunque ha de tenerse en cuenta que el reino Protista, definido como los eucariontes que no encajan en ninguno de los otros tres grupos, es parafilético. Por esta razón, la diversidad de los protistas coincide con la diversidad fundamental de los eucariontes.  La reciente clasificación de Adl et al. (2018)[3] evita la clasificación en reinos, sustituyéndola por una acorde con la filogenia actualmente conocida, en la que por otra parte a los clados o taxones no se les atribuye ya categoría alguna, para evitar los inconvenientes que suponen éstas para su posterior actualización. Los principales grupos de esta clasificación (equivalentes a reinos en clasificaciones anteriores) es como sigue:  Algunos grupos de protistas tienen una clasificación dudosa, en particular Cryptista (criptofitas) y Haptista (haptofitas), mientras que otros parecen situarse fuera de los grandes grupos, en particular CRuMs, Ancyromonadida y Hemimastigophora.  Adicionalmente se reconocen dos agrupaciones más grandes. Diaphoretickes (o Corticata) engloba a Archaeplastida y Sar, mientras que Amorphea (o Podiata) agrupa a Amoebozoa y Opisthokonta. Nótese que una forma ameboide o flagelar no indica la pertenencia a un grupo taxonómico concreto, como se creía en clasificaciones tradicionales, creando grupos artificiales desde el punto de visto evolutivo "
biologia,"En biología, el término Fungi (plural latino de fungus, lit. «hongos»)[5][6] se utiliza para designar a un taxón o grupo de organismos eucariotas entre los que se encuentran los mohos, las levaduras y los organismos productores de setas.  Están clasificados en un reino distinto al de las plantas, animales, protozoos y cromistas. Se distinguen de las plantas en que son heterótrofos; y de los animales en que poseen paredes celulares, como las plantas, compuestas por quitina, en vez de celulosa. Es el reino de la naturaleza más cercano filogenéticamente a los animales (Animalia).[7]  Se ha descubierto que organismos que parecían hongos en realidad no lo eran, y que organismos que no lo parecían en realidad sí lo eran, si llamamos ""hongo"" a todos los organismos derivados del que ancestralmente adquirió la capacidad de formar una pared celular de quitina. Debido a ello, si bien este taxón está bien delimitado desde el punto de vista evolutivo, aún se están estudiando las relaciones filogenéticas de los grupos menos conocidos, y su lista de subtaxones ha cambiado mucho con el tiempo en lo que respecta a grupos muy derivados o muy basales.  Los hongos se encuentran en hábitats muy diversos: pueden ser pirófilos (Pholiota carbonaria) o coprófilos (Psilocybe coprophila). Según su ecología, se pueden clasificar en cuatro grupos: saprofitos, liquenizados, micorrizógenos y parásitos. Los hongos saprofitos pueden ser sustrato específicos: Marasmius buxi o no específicos: Mycena pura. Los simbiontes pueden ser: hongos liquenizados basidiolíquenes: Omphalina ericetorum  y ascolíquenes: Cladonia coccifera y hongos micorrízicos: específicos: Lactarius torminosus (solo micorriza con abedules) y no específicos: Hebeloma mesophaeum. En la mayoría de los casos, sus representantes son poco conspicuos debido a su diminuto tamaño; suelen vivir en suelos y juntos a materiales en descomposición y como simbiontes de plantas, animales u otros hongos. Cuando fructifican, no obstante, producen esporocarpos llamativos (las setas son un ejemplo de ello). Realizan una digestión externa de sus alimentos, secretando enzimas, y que absorben luego las moléculas disueltas resultantes de la digestión. A esta forma de alimentación se le llama osmotrofia, la cual es similar a la que se da en las plantas, pero, a diferencia de aquellas, los nutrientes que toman son orgánicos. Los hongos son los descomponedores primarios de la materia muerta de plantas y de animales en muchos ecosistemas, y como tales poseen un papel ecológico muy relevante en los ciclos biogeoquímicos.  Los hongos tienen una gran importancia económica: las levaduras son las responsables de la fermentación de la cerveza y el pan, y se da la recolección y el cultivo de setas como las trufas. Desde 1940 se han empleado para producir industrialmente antibióticos, así como enzimas (especialmente proteasas). Algunas especies son agentes de biocontrol de plagas. Otras producen micotoxinas, compuestos bioactivos (como los alcaloides) que son tóxicos para humanos y otros animales. Las enfermedades fúngicas afectan a humanos, otros animales y plantas; en estas últimas, afecta a la seguridad alimentaria y al rendimiento de los cultivos.  Los hongos se presentan bajo dos formas principales: hongos filamentosos (antiguamente llamados ""mohos"") y hongos levaduriformes. El cuerpo de un hongo filamentoso tiene dos porciones, una reproductiva y otra vegetativa.[8] La parte vegetativa, que es haploide y generalmente no presenta coloración, está compuesta por filamentos llamados hifas (usualmente microscópicas); un conjunto de hifas conforma el micelio[9] (usualmente visible). A menudo las hifas están divididas por tabiques llamados septos.  Los hongos levaduriformes —o simplemente levaduras— son siempre unicelulares, de forma casi esférica. No existe en ellos una distinción entre cuerpo vegetativo y reproductivo.  Dentro del esquema de los cinco reinos de Wittaker y Margulis, los hongos pertenecen en parte al reino protista (los hongos ameboides y los hongos con zoosporas) y al reino Fungi (el resto). En el esquema de ocho reinos de Cavalier-Smith pertenecen en parte al reino Protozoa (los hongos ameboides), al reino Chromista (los Pseudofungi) y al reino Fungi todos los demás. La diversidad de taxones englobada en el grupo está poco estudiada; se estima que existen unas 1,5 millones de especies, de las cuales apenas el 5 % han sido clasificadas. Durante los siglos XVIII y XIX, Carlos Linneo, Christiaan Hendrik Persoon, y Elias Magnus Fries clasificaron a los hongos de acuerdo a su morfología o fisiología. Actualmente, las técnicas de biología molecular han permitido el establecimiento de una taxonomía molecular basada en secuencias de ácido desoxirribonucleico (ADN) y ácido ribonucleico (ARN) que divide al reino en un determinado número de divisiones.  La especialidad de la biología que se ocupa de los hongos se llama micología, donde se emplea el sufijo -mycota para las divisiones y -mycetes para las clases.  Etimología El término «fungi» es el plural de la palabra latina fungus, empleado ya por el poeta Horacio y el naturalista Plinio el Viejo para nombrar a sus cuerpos fructíferos, que en español dio origen a la palabra «hongo» así como a la palabra fungus en inglés.[10] En cambio, en otros idiomas la raíz es el vocablo de griego antiguo σφογγος (esponja), que hace referencia a las estructuras macroscópicas de mohos y setas; de esta han derivado los términos alemanes Schwamm (esponja), Schimmel (moho), el francés champignon y a través de este último el español «champiñón».[11] La disciplina que estudia los hongos, la micología, deriva del griego mykes/μύκης (hongo) y logos/λόγος (tratado, estudio, ciencia);[12] se cree que fue creada por el naturalista inglés Miles Joseph Berkeley en su publicación de 1836 The English Flora of Sir James Edward Smith, Vol. 5.[11]  Características Antes del desarrollo de los análisis moleculares de ARN y su aplicación en la dilucidación de la filogenia del grupo, los taxónomos clasificaban a los hongos en el grupo de las plantas debido a la semejanza entre sus formas de vida (fundamentalmente, la ausencia de locomoción y una morfología semejante). Como ellas, los hongos crecen en el suelo y, en el caso de las setas, forman cuerpos fructíferos que en algunos casos guardan parecido con ejemplares de plantas, como los musgos. No obstante, los estudios filogenéticos indicaron que forman parte de un reino separado del de los animales y plantas, de los cuales se separó hace aproximadamente mil millones de años.[13][14]  Algunas de las características morfológicas, bioquímicas y genéticas de los hongos son comunes a otros organismos; no obstante, otras son exclusivas, lo que permite su separación de otros seres vivos.  Como otros eucariotas, los hongos poseen células delimitadas por una membrana plasmática rica en esteroles y que contienen un núcleo que alberga el material genético en forma de cromosomas. Este material genético contiene genes y otros elementos codificantes así como elementos no codificantes, como los intrones. Poseen orgánulos celulares, como las mitocondrias y los ribosomas de tipo 80S. Como compuestos de reserva y glúcidos solubles poseen polialcoholes (p.e. el manitol), disacáridos (como la trehalosa) y polisacáridos (como el glucógeno, que, además,se encuentra presente en animales). Al igual que los animales, los hongos carecen de cloroplastos. Esto se debe a su carácter heterotrófico, que exige que obtengan como fuente de carbono, energía y poder reductor compuestos orgánicos.  A semejanza de las plantas, los hongos poseen pared celular[15] y vacuolas.[16] Se reproducen de forma sexual y asexual, y, como los helechos y musgos, producen esporas. Debido a su ciclo vital, poseen núcleos haploides habitualmente, al igual que los musgos y las algas.[17]  Los hongos guardan parecido con euglenoides y bacterias. Todos ellos producen el aminoácido L-lisina mediante la vía de biosíntesis del ácido alfa-aminoadípico.[18][19]  Las células de los hongos suelen poseer un aspecto filamentoso, siendo tubulares y alargadas. En su interior, es común que se encuentren varios núcleos; en sus extremos, zonas de crecimiento, se da una agregación de vesículas que contienen proteínas, lípidos y moléculas orgánicas llamadas Spitzenkörper.[20] Hongos y oomicetos poseen un tipo de crecimiento basado en hifas.[21] Este hecho es distintivo porque otros organismos filamentosos, las algas verdes, forman cadenas de células uninucleadas mediante procesos de división celular continuados.[22]  Al igual que otras especies de bacterias, animales y plantas, más de sesenta especies de hongos son bioluminiscentes (es decir, que producen luz).[23]  La combinación de crecimiento apical y ramificación/bifurcación conduce al desarrollo de un micelio, una red interconectada de hifas.[33] Las hifas pueden ser septadas o cenocíticas: las hifas septadas se dividen en compartimentos separados por paredes transversales (paredes celulares internas, denominadas tabiques, que se forman en ángulo recto con respecto a la pared celular que da forma a la hifa), y cada compartimento contiene uno o más núcleos; las hifas cenocíticas no están compartimentadas.[34] Los septos tienen poros que permiten el paso de citoplasma, orgánuloss y, a veces, núcleos; un ejemplo es el tabique doliporo en los hongos de Basidiomycota.[35] Las hifas cenocíticas son esencialmente multinucleadas supercélulas.[36]  Muchas especies han desarrollado estructuras de hifas especializadas para la absorción de nutrientes de los huéspedes vivos; los ejemplos incluyen haustoria en especies parásitas de plantas de la mayoría de los grupos fúngicos, y arbuscules de varios hongos micorrizicos, que penetran en las células huésped para consumir nutrientes.[37]  Aunque los hongos son opistocontos, una agrupación de organismos relacionados evolutivamente caracterizados ampliamente por un solo flagelo, todos los grupos excepto los quitridios y opistoporidios han perdido sus flagelos posteriores.[38] Los hongos son inusuales entre los eucariotas por tener una pared celular que, además de glucano (p. ej., β-1,3-glucano) y otros componentes típicos, también contiene biopolímeros de quitina.[39]  Estructuras macroscópicas Los micelios fúngicos pueden volverse visibles a simple vista, por ejemplo, en varias superficies y sustratos, como paredes húmedas y alimentos en mal estado, donde comúnmente se les llama moho. Los micelios que crecen en medios sólidos placas de agar en laboratorio generalmente se denominan colonias. Estas colonias pueden exhibir formas y colores de crecimiento (debido a esporas o pigmentación) que pueden usarse como características de diagnóstico en la identificación de especies o grupos.[40] Algunas colonias fúngicas individuales pueden alcanzar dimensiones y edades extraordinarias como es el caso de una colonia clonal de Armillaria ostoyae, que se extiende sobre un área de más de 900 ha, con una edad estimada de casi 9.000 años.[41]  El apotecio, una estructura especializada importante en la reproducción sexual de los ascomicetos, es un cuerpo fructífero en forma de copa que sostiene el himenio, una capa de tejido que contiene la espora y células portadoras.[42] Los cuerpos fructíferos de los basidiomicetos (basidiocarpos) y algunos ascomicetos a veces pueden crecer bastante y muchos son bien conocidos como setas.  Reproducción Los hongos se reproducen sobre todo por medio de esporas, las cuales se dispersan en un estado latente, que se interrumpe solo cuando se hallan condiciones favorables para su germinación. Cuando estas condiciones se dan, la espora germina, surgiendo de ella una primera hifa, por cuya extensión y ramificación se va constituyendo un micelio. La velocidad de crecimiento de las hifas de un hongo es verdaderamente espectacular: en un hongo tropical llega hasta los 5 mm por minuto. Se puede decir, sin exagerar, que incluso es posible ver crecer a algunos hongos en tiempo real.  Las esporas de los hongos se producen en esporangios, ya sea asexualmente o como resultado de un proceso de reproducción sexual. En este último caso la producción de esporas es precedida por la meiosis de las células, de la cual se originan las esporas mismas. Las esporas producidas a continuación de la meiosis se denominan meiosporas. Como la misma especie del hongo es capaz de reproducirse tanto asexual como sexualmente, las meiosporas tienen una capacidad de resistencia que les permite sobrevivir en las condiciones más adversas, mientras que las esporas producidas asexualmente cumplen sobre todo con el objetivo de propagar el hongo con la máxima rapidez y extensión posible.  El micelio vegetativo de los hongos, o sea el que no cumple con las funciones reproductivas, tiene un aspecto muy simple, porque no es más que un conjunto de hifas dispuestas sin orden. La fantasía creativa de los hongos se manifiesta solo en la construcción de cuerpos fructíferos, los cuales, como indica el nombre, sirven para portar los esporangios que producen las esporas"
biologia,"En el ámbito de la biología, un reino representa cada una de las grandes subdivisiones taxonómicas en las que se clasifican los seres vivos respecto a su parentesco evolutivo. Reino es el segundo nivel de clasificación por debajo de dominio y por encima de filo y, al igual que el resto de grupos taxonómicos, su ortografía y pronunciación es latina. Históricamente, los primeros reinos que se establecieron para clasificar la naturaleza fueron el animal, el vegetal y el mineral, pero con el advenimiento del estudio biológico se popularizó el sistema de los cinco reinos de la vida (Animalia, Plantae, Fungi, Protista y Monera), clasificados más por sus apariencias que por su verdadera relación evolutiva.  Sin embargo, en la actualidad la clasificación de los reinos está siendo redefinida para que represente en mayor medida la filogenia de cada grupo. Por un lado, el reino Protista es una clasificación parafilética, donde algunos de sus filos tienen tantas diferencias entre sí como estos con Animalia, Plantae o Fungi. Por ello, y para facilitar su estudio, cada vez es más común que Protista se encuentre dividido entre los reinos Chromista y Protozoa.  Por otro lado, Monera es un término en desuso que incluía dos grupos bien diferenciados: los actuales dominios Archaea y Bacteria donde, además, Archaea estaría ligeramente más relacionado con Eukarya. Este esquema fue propuesto por Woese en 1977 al notar las grandes diferencias que a nivel de la genética ribosómica presentan arqueas y bacterias, a pesar de que ambos grupos están compuestos por organismos con células procariotas.[1][2]  La clasificación taxonómica de reino no se utiliza para la catalogación de procariotas (Archaea y Bacteria), en donde únicamente se utiliza la agrupación en dominios. Por ello, en la actualidad, el sistema de reinos queda restringido a los organismos eucariotas, esto es, animales, plantas, hongos, protozoos y algas.[3]  Historia Sistema de dos reinos Históricamente, la primera organización en reinos se debe a Aristóteles (siglo IV a. C.), que diferenció todas las entidades vivas de la naturaleza en dos reinos: vegetal y animal. El primero caracterizado por tener ""alma vegetativa"" que le da reproducción, crecimiento y nutrición. El segundo tiene adicionalmente ""alma sensitiva"" que le da además de lo anterior movimiento, percepción y deseo. Aristóteles sentó las bases del conocimiento sistemático, pues dividió al reino animal en dos géneros máximos: anaima para los animales sin sangre (invertebrados) y enaima los animales con sangre (vertebrados); y estos a su vez se dividían en géneros y especies.[4]  Carlos Linneo[5] también distinguió estos dos reinos de seres vivos y además trató a los minerales, colocándolos en un tercer reino, Lapides. Además, introdujo la nomenclatura binomial para referir a las especies y dividió los reinos en clases, las clases en órdenes, los órdenes en familias, las familias en géneros y los géneros en especies (la división de los reinos en filos se introdujo posteriormente).  Posteriormente, con la invención de la microscopía apareció un nuevo mundo de investigación biológica que cambiaría el concepto sobre los reinos. En la siguiente tabla se presenta una comparación de los sistemas de clasificación en reinos biológicos más notables:    Tres reinos En 1858, Richard Owen observó la dificultad de clasificar los seres microbianos en animales y vegetales, por lo que propuso crear el reino Protozoa[14] y los definió como los seres en su mayoría diminutos formados por células nucleadas.  En 1860 el biólogo inglés John Hogg, postula el tercer Regnium primigenium o Protoctista para los protozoos, protófitos y formas simples, como la esponja verde dulceacuícola Spongilla que en realidad es simbionte con algas verdes.[15] Hogg en realidad hablaba de cuatro reinos: animal, vegetal, primigenio y mineral. Las ideas de Hogg fueron eclipsadas por Ernst Haeckel, quien es considerado fundador de la protistología.  Haeckel en 1866[6] llamó al tercer reino Protista y lo definió como el ""primordial"", el reino de las formas primitivas e intermedio entre los reinos Animal y Plantae. Reconoció lo problemático de su clasificación por la presencia de caracteres animales, vegetales y mixtos, pero necesario para propósitos sistemáticos, antes que filogenéticos. Dentro de protista colocó a las bacterias en el filo Moneres. Fue el primero en distinguir entre organismos unicelulares (protistas) y pluricelulares (plantas y animales). En sucesivas publicaciones, Haeckel hizo correcciones a sus clasificaciones: por ejemplo movió las esponjas del reino protista al animal, a los hongos del reino planta al protista, a las algas verdeazuladas del reino planta al protista junto con las bacterias, Labyrinthulomycetes de animal a protista y los volvocales de protista a planta.[16]  Cuatro reinos El concepto del tercer reino fue puesto en duda por Otto Bütschli en los años 1880, pues se consideró a Protista como polifilético, especialmente por la inclusión de las bacterias. Poco a poco se puso de manifiesto la importancia de la distinción entre procariotas y eucariotas propuesta de Edouard Chatton[7] de 1925-1937 y se popularizó en los años 1950.  Herbert Copeland en sus publicaciones de 1938, 1947 y 1956, separa los protistas nucleados de las bacterias anucleadas en el sistema de cuatro reinos siguiente: Plantae (o Metaphyta), Animalia (o Metazoa), Protoctista (o Protista) y Mychota (o Monera) para las bacterias.[17]  En 1948, los editores del Manual de Bacteriología Determinativa de Bergey sugirieron llamar al nuevo reino Protophyta, para incluir tanto a bacterias como a virus.[18]  Cinco reinos Robert Whittaker reconoce el reino adicional de los hongos (Fungi) en 1959.[19] El resultado fue el sistema de los 5 reinos, propuesto en 1969,[9] que se convirtió en un estándar muy popular y que, con algunas modificaciones, aún hoy se utiliza en muchas obras o constituye la base para nuevos sistemas propuestos. Se basa principalmente en las diferencias en materia de nutrición: Plantae son en su mayoría pluricelulares autótrofos, Animalia pluricelulares heterótrofos y Fungi pluricelulares saprofitos. Los otros dos reinos, Protista (eucariotas) y Monera (procariotas), incluyen organismos unicelulares o coloniales.  Ante la posibilidad de que algunos de estos reinos pudieran no ser monofiléticos, Whittaker argüia: ""La monofilia es un valor primordial en sistemática, pero al igual que otros valores, no es absoluto y no siempre se debe seguir si se sacrifican otros objetivos"".[9]  Otros trabajos han apoyado este sistema de cinco reinos, como el de Margulis (1974)[20] y Margulis & Schwartz (1998),[21] quienes usaron el término original de Hogg Protoctista.  Sistemas multirreinos Desde que Haeckel propuso el reino Protista, muchos biólogos consideraron que este grupo era excesivamente polifilético o parafilético y que debía subdividirse en varios reinos monofiléticos. Hennig en 1950 propuso su teoría de la sistemática filogenética (posteriormente denominada cladista), que introducía explícitamente el concepto de evolución en sistemática por grupos monofiléticos, (tal como postulaba Darwin).  Uno de estos sistemas fue el de G. F. Leedale, que en 1974 propuso 19 reinos: Monera, Rhodophyta, Plantae, Heterokonta, Eustigmatophyta, Haptophyta, Cryptophyta, Dinophyta, Euglenophyta, Chytridiomycota, Fungi, Myxomycota, Zoomastigota (zooflagelados), Sarcodinia, Ciliophora, Sporozoa, Animalia, Porifera y Mesozoa.[22] Sin embargo, muchos de estos clados han sido reagrupados o superados por el estudio filogenético protista en continua actualización.  Se han propuesto otros sistemas. El más grande es probablemente el de A.L. Drozdov (2003),[23] quien propuso 26 reinos.  Seis reinos En los años 1980 se produjo un gran avance en filogenia procariota gracias al advenimiento del análisis genético. Sobre la base de estudios de ARNr (más específicamente ADN que codifica para el ARN ribosomal 16S procariota y 18S eucariota), Carl Woese y G. Fox dividieron en 1977 a los procariotas o moneras en dos superreinos: Eubacteria y Archaebacteria.[1] En 1990, Woese renombró los nuevos grupos por lo que postuló el sistema de tres dominios formado por Bacteria, Archaea  y Eucarya.[2] Este sistema es el más aceptado actualmente para la clasificación de los seres vivos y se opone al sistema de dos imperios.  Estos dos grupos procariotas Archaea (o Archaebacteria) y Bacteria (o Eubacteria), son considerados por otros autores como reinos junto con plantas, animales, hongos y protistas, lo que constituye el sistema de seis reinos, sistema que se ha convertido en estándar en muchas obras[25] y libros educativos.[26] Los seis reinos son atribuidos a Woese,[27] pero en realidad por una tangencial interpretación, ya que él hablaba en realidad de tres reinos primarios o superreinos (Woese 1977). Archaea y Bacteria están considerados como dominios o superreinos pero su trato también es de reino, pues se subdividen siempre en filos. Los Archaebacteria y los Bacteria anteriormente eran catalogados como organismos del reino Monera, pero en los años 70 y 80 se encontró una diferencia principalmente en la estructura de pared y membranas celulares lo cual hace que se divida el reino en 2 partes[28]  Los reinos[29] presentan las siguientes características:  Siete reinos Desde entonces, se han propuesto multitud de nuevos reinos eucariotas, pero la mayoría fueron rápidamente invalidados, reclasificados a nivel de filos o clases o abandonados. El único que aún era mencionado por algunos autores ha sido el sistema de seis reinos de Cavalier-Smith,[11][12] que propone al reino Chromista para abarcar organismos tales como algas pardas, algas verde-amarillas, algas doradas, diatomeas, oomicetos y otros relacionados; y al reino Protozoa (de los protozoarios) como un grupo eucariota basal. Esta propuesta ha ido recibiendo atención paulatinamente, aunque la cuestión de las relaciones y división en grupos de los seres vivos sigue siendo todavía materia de discusión.  La taxonomía más reciente (Sistema del Catálogo de la Vida 2015), busca establecer una clasificación manejable y práctica, por lo que los criterios evolutivos y filogenéticos son relativos, admitiéndose algunos grupos parafiléticos en determinados casos. Recoge parte de los postulados de Cavalier-Smith y presenta la siguiente clasificación en dos superreinos y siete reinos:[13]  Este sistema incluye dos nuevos reinos, los cuales pueden describirse como sigue:  Otros niveles de clasificación Debido a la elevada variedad de la vida se han establecido numerosos niveles de clasificación denominados taxones. El nivel de reino era hasta hace poco el nivel superior de la clasificación biológica. En las clasificaciones modernas el nivel superior es el dominio, superreino o imperio; cada uno de los cuales se subdivide en reinos, los reinos, a su vez, pueden organizarse en filos, etc. "
biologia,"En la clasificación científica de los seres vivos, los animales (Animalia) o metazoos (Metazoa) constituyen un reino que reúne un amplio grupo de organismos que son eucariotas, heterótrofos, pluricelulares y tisulares (excepto los poríferos). Se caracterizan por su amplia capacidad de movimiento, por no tener cloroplasto (aunque hay excepciones, como en el caso de Elysia chlorotica) ni pared celular, y por su desarrollo embrionario; que atraviesa una fase de blástula y determina un plan corporal fijo (aunque muchas especies pueden sufrir una metamorfosis posterior como los artrópodos). Los animales forman un grupo natural estrechamente emparentado con los hongos (reino Fungi). Animalia es uno de los cinco reinos del dominio Eukaryota, y a él pertenece el ser humano. La parte de la biología que estudia los animales es la zoología.  Los filos animales más conocidos aparecen en el registro fósil durante la denominada explosión cámbrica, sucedida en los mares hace unos 542 a 530 millones de años. Los animales se dividen en varios subgrupos, algunos de los cuales son vertebrados: (aves, mamíferos, anfibios, reptiles, peces) e invertebrados: artrópodos (insectos, arácnidos, miriápodos, crustáceos), anélidos (lombrices, sanguijuelas), moluscos (bivalvos, gasterópodos, cefalópodos), poríferos (esponjas), cnidarios (medusas, pólipos, corales), equinodermos (estrellas de mar), nematodos (gusanos cilíndricos), platelmintos (gusanos planos), etc.  Características La movilidad es la característica más llamativa de los organismos de este reino, pero no es exclusiva del grupo, lo que da lugar a que sean designados a menudo como animales ciertos organismos, los llamados protozoos, que pertenecen al reino Protista.  En el siguiente esquema se muestran las características comunes a todos los animales:  Con pocas excepciones, la más notable la de las esponjas (filo Porifera), los animales presentan tejidos diferenciados y especializados. Estos incluyen músculos, que pueden contraerse para controlar el movimiento, y un sistema nervioso, que envía y procesa señales. Suele haber también una cámara digestiva interna, con una o dos aberturas. Los animales con este tipo de organización son conocidos como eumetazoos, en contraposición a los parazoos y mesozoos, que son niveles de organización más simples ya que carecen de algunas de las características mencionadas.  Todos los animales tienen células eucariontes, rodeadas de una matriz extracelular característica compuesta de colágeno y glucoproteínas elásticas. Esta puede calcificarse para formar estructuras como conchas, huesos y espículas. Durante el desarrollo del animal se crea un armazón relativamente flexible por el que las células se pueden mover y reorganizarse, haciendo posibles estructuras más complejas. Esto contrasta con otros organismos pluricelulares como las plantas y los hongos, que desarrollan un crecimiento progresivo.  Funciones esenciales Los animales llevan a cabo las siguientes funciones esenciales: alimentación, respiración, circulación, excreción, respuesta, movimiento y reproducción:  Alimentación La mayoría de los animales no pueden absorber comida; la ingieren. Los animales han evolucionado de diversas formas para alimentarse. Los herbívoros comen plantas, los carnívoros comen otros animales; y los omnívoros se alimentan tanto de plantas como de animales. Los detritívoros comen material vegetal y animal en descomposición. Los comedores por filtración son animales acuáticos que cuelan minúsculos organismos que flotan en el agua. Los animales también forman relaciones simbióticas, en las que dos especies viven en estrecha asociación mutua. Por ejemplo, un parásito es un tipo de simbionte que vive dentro o sobre otro organismo, el huésped. El parásito se alimenta del huésped y lo daña.[4]  Respiración No importa si viven en el agua o en la tierra, todos los animales respiran; esto significa que pueden tomar oxígeno y despedir dióxido de carbono. Gracias a sus cuerpos muy simples y de delgadas paredes, algunos animales utilizan la difusión de estas sustancias a través de la piel. Sin embargo, la mayoría de los animales han evolucionado complejos tejidos y sistemas orgánicos para la respiración.[4]  Circulación Muchos animales acuáticos pequeños, como algunos gusanos, utilizan solo la difusión para transportar oxígeno y moléculas de nutrientes a todas sus células, y recoger de ellas los productos de desecho. La difusión basta porque estos animales apenas tienen un espesor de unas cuantas células. Sin embargo, los animales más grandes poseen algún tipo de sistema circulatorio para desplazar sustancias por el interior de sus cuerpos.[4]  Excreción Un producto de desecho primario de las células es el amoníaco, sustancia venenosa que contiene nitrógeno. La acumulación de amoniaco y otros productos de desecho podrían matar a un animal. La mayoría de los animales poseen un sistema excretor que bien elimina amoniaco o bien lo transforma en una sustancia menos tóxica que se elimina del cuerpo. Gracias a que eliminan los desechos metabólicos, los sistemas excretores ayudan a mantener la homeostasis. Los sistemas excretores varían, desde células que bombean agua fuera del cuerpo hasta órganos complejos como riñones.[4]  Respuesta Los animales usan células especializadas, llamadas células nerviosas, para responder a los sucesos de su medio ambiente. En la mayoría de los animales, las células nerviosas están conectadas entre sí para formar un sistema nervioso. Algunas células llamadas receptores, responden a sonidos, luz y otros estímulos externos. Otras células nerviosas procesan información y determinan la respuesta del animal. La organización de las células nerviosas dentro del cuerpo cambia dramáticamente de un fílum a otro.[4]  Movimiento Algunos animales adultos permanecen fijos en un sitio. Aunque muchos tienen movilidad. Sin embargo, tanto los fijos como los más veloces normalmente poseen músculos o tejidos musculares que se acortan para generar fuerza. La contracción muscular permite que los animales movibles se desplacen, a menudo en combinación con una estructura llamada esqueleto. Los músculos también ayudan a los animales, aun los más sedentarios, a comer y bombear agua y otros líquidos fuera del cuerpo.[4]  Reproducción La mayoría de los animales se reproducen sexualmente mediante la producción de gametos haploides. La reproducción sexual ayuda a crear y mantener la diversidad genética de una población. Por consiguiente, ayuda a mejorar la capacidad de una especie para evolucionar con los cambios del medio ambiente. Muchos invertebrados también pueden reproducirse asexualmente. La reproducción asexual da origen a descendiente genéticamente idénticos a los progenitores. Esta forma de reproducción permite que los animales aumenten rápidamente en cantidad.[4]  Clasificación Historia Los animales han sido estudiados desde la antigüedad, y aún hoy, la clasificación animal se muestra cambiante, pues depende de los estudios que revelan constantemente información novedosa. Los grupos animales se definieron sobre la base de sus caracteres biológicos, morfológicos y ultraestructurales; sin embargo, la filogenia del siglo XXI está basada principalmente en el estudio filogenómico molecular del ADN mitocondrial, ribosómico y nuclear, lo que ha determinado también cambios importantes. La siguiente tabla, resume históricamente los sistemas de clasificación más notables, dando relevancia al descubrimiento de los principales supergrupos:  La clasificación de Hyman (1940), que ha estado en vigencia hasta hace poco (Margulis & Chapman, 2009), ha sido invalidada por los estudios filogenéticos moleculares contemporáneos, ya que se demostró que grupos como los acelomados, pseudocelomados, celomados y esquizocelomados son en realidad grupos artificiales (polifiléticos).[17]  Filos del reino animal El reino animal se subdivide en una serie de grandes grupos denominados filos (el equivalente a las divisiones del reino vegetal); cada uno responde a un tipo de organización bien definido, aunque hay algunos de afiliación controvertida. En el siguiente cuadro, se enumeran los filos animales y sus principales características:  En esta tabla no figuran los filos Echiura, Pogonophora, Sipuncula y Orthonectida los cuales han sido reclasificados en Annelida,[23] y el filo Acanthocephala el cual fue reclasificado en Rotifera.[24] El filo Myxozoa ha sido reclasificado en Cnidaria y el filo Monoblastozoa es de dudosa existencia.  Origen y documentación fósil Mientras que en las plantas se conocen varias series de formas que conducen de la organización unicelular a la pluricelular, en el Reino Animal se sabe muy poco sobre la transición entre protozoos y metazoos. Dicha transición no está documentada por fósiles y las formas recientes supuestamente intermedias tampoco nos ayudan demasiado.  En este campo de la transición pueden mencionarse, por una parte, a Proterospongia, coanoflagelado marino y planctónico que forma una masa gelatinosa con coanocitos en la parte exterior y células ameboides en el interior, y por otra al pequeño organismo marino Trichoplax adhaerens (filo placozoos) que forma una placa cerrada por epitelio pavimentoso en la parte dorsal y cilíndrico en la parte central, y presenta en la cavidad interior células en forma de estrella; se reproduce por yemas flageladas y huevos. Otra forma sencilla de metazoo es Xenoturbella, que vive sobre los fondos fangosos del mar. Tienen algunos centímetros de largo y forma de hoja, una boca ventral que conduce a un estómago en forma de saco. Entre la epidermis y el intestino existe una capa de tejido conjuntivo con un tubo muscular longitudinal y células musculares en el mesénquima; en la parte basal de la epidermis existe un plexo nervioso y en la parte anterior presenta un estatocisto; produce óvulos y espermatozoides, estos idénticos a los de diferentes metazoos primitivos. Su posición sistemática es incierta, habiéndose propuesto como miembro de un filo independiente (xenoturbélidos), a emplazar tal vez en la base de los deuteróstomos. Por lo que respecta a los mesozoos, ya no son considerados un estado de transición entre protistas y metazoos; su modo de vida parásito parece que les condujo a una reducción y simplificación extremas a partir de vermes acelomados.  Por tanto, se debe recurrir a la morfología, fisiología y ontogenia comparadas de los metazoos para poder reconstruir esta etapa de la evolución. Los datos obtenidos con microscopía electrónica y análisis moleculares han apagado antiguas controversias sobre el origen de los metazoos. En este sentido, parece definitivamente rechazada la hipótesis sobre un origen polifilético; incluso los placozoos y los mesozoos, considerados a veces como originados directa e independientemente de los protistas, parecen a la luz de los nuevos datos claramente metazoos. Tres fueron las teorías sobre el origen de los metazoos:[25]  Cristidiscoidea  Fungi  Mesomycetozoa  Pluriformea  Filasterea  Choanoflagellatea  Animalia  Evolución del reino Animalia Los primeros fósiles que podrían representar a animales corresponden a Otavia hallado en Namibia de entre 760-550 millones de años. Estos fósiles se interpretan como esponjas tempranas.[1] Sin embargo estudios que usan relojes moleculares estiman el origen de los animales entre unos 850 millones de años durante la glaciación del Criogénico-Tónico.[27]   Los animales más antiguos que se conocen aparecen hacia el final del Precámbrico, hace alrededor de 580 millones de años, y se les conoce como vendobiontes o la biota del periodo Ediacárico.[28] No obstante, son muy difíciles de relacionar con los fósiles posteriores. Algunos de estos organismos podrían ser los precursores de los filos modernos, pero también podrían ser grupos separados, y es posible que no fueran realmente animales en sentido estricto. Entre los primeros animales conocidos estarían Cyclomedusa, Charnia, Charniodiscus, Parvancorina, Annulatubus, Spriggina, etc.  Aparte de ellos, muchos filos conocidos de animales hicieron una aparición más o menos simultánea durante el período Cámbrico, hace cerca de 570 millones de años. Todavía se discute si este evento, llamado explosión cámbrica, representa una rápida divergencia entre diferentes grupos o un cambio de condiciones que facilitó la fosilización. Algunos ejemplos serían Wiwaxia, Pikaia, Hallucigenia, Opabinia, etc.  Entre los ancestros de grupos posteriores destaca Anomalocaris, del Cámbrico, como posible ancestro de diversos grupos de artrópodos, por su cuerpo segmentado, evolucionado de Opabinia y otros similares. Los cordados podrían tener relación con Pikaia.  En cuanto a la evolución de los filos, tradicionalmente los animales se clasificaron por simetría y su nivel de complejidad en grupos como Radiata, Mesozoa, Acoelomata, Coelomata, y Pseudocoelomata que resultaron ser polifiléticos. Actualmente no está bien claro como fue el último ancestro común de todos los animales (""Urmetazoa""), todos los análisis moleculares respaldan la teoría colonial que afirma que los animales surgieron de la unión colonial de protozoos similares a los coanoflagelados que es la teoría ampliamente aceptada para explicar el origen de los animales. Parece poco probable que los animales hayan surgido de un único ancestro en común dado a la falta de homología entre los poríferos y los animales verdaderos (Eumetazoa), por lo que es probable que hayan surgido dos veces de las colonias de coanoflagelados. Los poríferos son muy diferentes de los eumetazoos puesto que carecen de tejidos,. "
biologia,"La biología celular (anteriormente citología, del griego κύτος, que significa ‘célula’)[1] es una rama de la biología que estudia la estructura, la función y el comportamiento de las células. La biología celular abarca tanto las células procariotas como las eucariotas y se puede dividir en muchos subtemas que pueden incluir el estudio del metabolismo celular, la comunicación celular, el ciclo celular, la bioquímica y la composición celular, la interacción con el ambiente y su ciclo vital.  Historia Estudios estructurales La primera referencia al concepto de célula data del siglo XVII cuando el inglés Robert Hooke utilizó este término, para referirse a los pequeños huecos poliédricos que observó con su microscopio, que formaban la estructura del tejido vegetal del corcho (y por su parecido con las habitaciones de los sacerdotes llamadas «celda» (cell en inglés).  No obstante, hasta el siglo XIX no se desarrolla este concepto considerando su estructura interior. Es en este siglo, cuando se desarrolla la teoría celular , que reconoce la célula como la unidad básica de estructura y función de todos los seres vivos, idea que constituye desde entonces uno de los pilares de la biología moderna.   Fue esta teoría celular la que impulsó en buena medida las investigaciones biológicas al terreno microscópico, pues las células no son visibles a simple vista. La unidad de medida utilizada es el micrómetro (μm) antes conocida como micra, existiendo células de entre 2 y 20 μm, aunque las neuronas pueden tener una longitud mayor.   La investigación microscópica pronto daría lugar al descubrimiento de la estructura celular interna incluyendo el núcleo, los cromosomas, el aparato de Golgi, las mitocondrias y otros orgánulos celulares, así como la identificación de la relación existente entre la estructura y la función de los orgánulos celulares.  Ya en siglo XX, la introducción del microscopio electrónico reveló detalles de la ultraestructura celular, y aparecieron la histoquímica y la citoquímica.[2]  También se descubrió la base material de la herencia, con los cromosomas y el ADN, y nació la citogenética.  Estudios bioquímicos La biología celular como tal, surgió como consecuencia de un cambio en la concepción del estudio de los organismos vivos, en tanto estos mostraban funciones que sobrepasaban lo estructural. Es esencial conocer los procesos de la vida de la célula durante su ciclo celular, como son la nutrición, la respiración, la síntesis de componentes, los mecanismos de defensa, la división celular y la muerte celular.  La historia de la bioquímica como la conocemos hoy en día, viene del siglo XIX cuando una buena parte de la biología y de la química se orientaron a la creación de una nueva disciplina integradora: la química fisiológica hoy conocida como bioquímica.  Podemos entender la bioquímica como una disciplina científica integradora, que aborda el estudio de las biomoléculas y los biosistemas. Integra de esta forma las leyes químico-físicas y la evolución biológica que determinan a los biosistemas y a sus componentes.  Estudios moleculares La biología molecular implica la comprensión de las interacciones de los diferentes sistemas de la célula lo que incluye muchas relaciones, entre ellas las del ADN con el ARN, la síntesis de proteínas, el metabolismo, y cómo todas esas interacciones son reguladas para conseguir un correcto funcionamiento de la célula.  La biología molecular tiene como objetivo el estudio, desde el punto de vista molecular, de los procesos que se desarrollan en la célula viva. Dos macromoléculas en particular son objeto de su estudio: el ADN y las Proteínas. Esta área específica de estudio está relacionada con otros campos de la Biología Celular, como son la Ingeniería  genética y la bioquímica.   El  estudio  mediante  métodos  físico-químicos  de  la  materia  viva  y  sus  procesos  biológicos, incluye  varias disciplinas  dentro  del  concepto  general  de  Biología Molecular, ellas son: Bioquímica Estructural, Bioquímica Inorgánica, Bioquímica Metabólica y Enzimología, Fisiología Molecular, Biología Molecular y Química Física.[3]  Estructura y función Estructura de células eucariotas  Artículo principal: Ecuariota  Las células eucariotas están compuestas por los siguientes orgánulos:  Las células eucariotas también pueden estar formadas por los siguientes componentes moleculares:   Metabolismo de la célula  El metabolismo celular es necesario para la producción de energía para la célula y, por tanto, para su supervivencia, e incluye muchas vías. En el caso de la respiración celular, una vez que la glucosa está disponible, la glucólisis se produce en el citosol de la célula para producir piruvato. El piruvato se descarboxila mediante el complejo multienzimático para formar acetil coA, que puede utilizarse en el ciclo TCA para producir NADH y FADH2. Estos productos intervienen en la cadena de transporte de electrones para formar finalmente un gradiente de protones a través de la membrana mitocondrial interna. Este gradiente puede entonces impulsar la producción de ATP y H2O durante la fosforilación oxidativa El metabolismo en las células vegetales incluye la fotosíntesis, que es simplemente lo opuesto a la respiración, ya que en última instancia produce moléculas de glucosa.  Señalización celular  Más información: Señalización celular  La señalización o comunicación celular es importante para la regulación celular y para que las células procesen la información del entorno y respondan en consecuencia. La señalización puede producirse por contacto celular directo o por señalización endocrina, paracrina y autocrina. El contacto directo célula-célula se produce cuando un receptor de una célula se une a una molécula que está adherida a la membrana de otra célula. La señalización endocrina se produce a través de moléculas secretadas en el torrente sanguíneo. La señalización paracrina utiliza moléculas que se difunden entre dos células para comunicarse. La autocrina es una célula que se envía una señal a sí misma mediante la secreción de una molécula que se une a un receptor de su superficie. Las formas de comunicación pueden ser a través de:  Crecimiento y desarrollo Ciclo de la célula eucariota  Las células son la base de todos los organismos y constituyen las unidades fundamentales de la vida. El crecimiento y desarrollo de las células son esenciales para el mantenimiento del huésped y la supervivencia del organismo. Para ello, la célula pasa por las fases del ciclo celular y del desarrollo, que implican el crecimiento celular, la replicación del ADN, la división celular, la regeneración y la muerte celular.  El ciclo celular se divide en cuatro fases distintas: G1, S, G2 y M. La fase G -que es la fase de crecimiento celular- constituye aproximadamente el 95% del ciclo. La proliferación de las células es instigada por los progenitores. Todas las células parten de una forma idéntica y, en esencia, pueden convertirse en cualquier tipo de célula. La señalización celular, como la inducción, puede influir en las células cercanas para determinar el tipo de célula en que se convertirá. Además, esto permite a las células del mismo tipo agregarse y formar tejidos, luego órganos y, por último, sistemas. Las fases G1, G2 y S (replicación, daño y reparación del ADN) se consideran la porción interfásica del ciclo, mientras que la fase M (mitosis) es la porción de división celular del ciclo. La mitosis se compone de muchas etapas que incluyen, profase, metafase, anafase, telofase y citocinesis, respectivamente. El resultado final de la mitosis es la formación de dos células hijas idénticas.  El ciclo celular está regulado en los puntos de control del ciclo celular por una serie de factores y complejos de señalización como las ciclinas, la quinasa dependiente de ciclinas y p53. Cuando la célula ha completado su proceso de crecimiento y si se detecta que está dañada o alterada, se somete a la muerte celular, ya sea por apoptosis o necrosis, para eliminar la amenaza que puede suponer para la supervivencia del organismo.  Mortalidad celular, inmortalidad del linaje celular  La ascendencia de cada célula actual se remonta presumiblemente, en un linaje ininterrumpido de más de 3.000 millones de años, al origen de la vida. En realidad, no son las células las que son inmortales, sino los linajes celulares multigeneracionales.La inmortalidad de un linaje celular depende del mantenimiento del potencial de división celular. Este potencial puede perderse en cualquier linaje particular debido al daño celular, la diferenciación terminal, como ocurre en las células nerviosas, o la muerte celular programada (apoptosis) durante el desarrollo. El mantenimiento del potencial de división celular a lo largo de generaciones sucesivas depende de que se eviten y reparen correctamente los daños celulares, en particular los daños en el ADN. En los organismos sexuales, la continuidad de la línea germinal depende de la eficacia de los procesos para evitar daños en el ADN y reparar los que se produzcan. Los procesos sexuales en eucariotas, así como en procariotas, ofrecen la oportunidad de reparar eficazmente los daños del ADN en la línea germinal mediante recombinación homóloga.  Fases del ciclo celular  El ciclo celular es un proceso de cuatro etapas por el que pasa una célula a medida que se desarrolla y se divide. Incluye la Brecha 1 (G1), la síntesis (S), la Brecha 2 (G2) y la mitosis (M). La célula reinicia el ciclo desde G1 o lo abandona por G0 tras completarlo. La célula puede progresar desde G0 hasta la diferenciación terminal.  La interfase se refiere a las fases del ciclo celular que ocurren entre una mitosis y la siguiente, e incluye G1, S y G2.  El tamaño de la célula crece.  El contenido de las células se replica.  Replicación del ADN.  La célula replica cada uno de los 46 cromosomas (23 pares).  La célula se multiplica.  En preparación para la división celular, se forman orgánulos y proteínas.  Tras la mitosis se produce la citocinesis (separación celular)  Formación de dos células hijas idénticas  Estas células abandonan G1 y entran en G0, una fase de reposo. Una célula en G0 está haciendo su trabajo sin prepararse activamente para dividirse.  Patología Artículo principal: Citopatología  La rama científica que estudia y diagnostica las enfermedades a nivel celular se denomina citopatología. La citopatología se utiliza generalmente en muestras de células libres o fragmentos de tejidos, a diferencia de la rama patológica de la histopatología, que estudia tejidos enteros. La citopatología se utiliza habitualmente para investigar enfermedades que afectan a una amplia gama de localizaciones corporales, a menudo para ayudar en el diagnóstico del cáncer, pero también en el diagnóstico de algunas enfermedades infecciosas y otras afecciones inflamatorias. Por ejemplo, una aplicación común de la citopatología es la prueba de papanicolau, una prueba de detección utilizada para identificar el cáncer cervical y las lesiones cervicales precancerosas que pueden derivar en cáncer de cuello de útero.  Puntos de control del ciclo celular y sistema de reparación de daños en el ADN El ciclo celular se compone de una serie de etapas bien ordenadas y consecutivas que dan lugar a la división celular. El hecho de que las células no comiencen la siguiente etapa hasta que no haya finalizado la última es un elemento significativo de la regulación del ciclo celular. Los puntos de control del ciclo celular son características que constituyen una excelente estrategia de control para la precisión del ciclo y las divisiones celulares. Las Cdks, sus homólogas ciclinas asociadas, las proteínas quinasas y las fosfatasas regulan el crecimiento y la división celular de una etapa a otra. El ciclo celular está controlado por la activación temporal de las Cdks, que se rige por la interacción de las ciclinas asociadas, la fosforilación por proteínas quinasas particulares y la desfosforilación por fosfatasas de la familia Cdc25. En respuesta al daño en el ADN, la reacción de reparación del ADN de una célula es una cascada de vías de señalización que conduce a la activación del punto de control, regula, el mecanismo de reparación en el ADN, las alteraciones del ciclo celular y la apoptosis. Numerosas estructuras bioquímicas, así como procesos que detectan daños en el ADN, son ATM y ATR, que inducen los puntos de control de reparación del ADN "
biologia,"Una célula procariota o procarionte es un organismo unicelular, cuyo material genético se encuentra disperso en el citoplasma, reunido en una zona denominada nucleoide.[1] Por el contrario, las células que sí tienen un núcleo diferenciado del citoplasma, se llaman eucariotas, es decir, aquellas en las que su ADN se encuentra dentro de un compartimento separado del resto de la célula.[2]  Además, el término procariota hace referencia a los organismos pertenecientes al dominio Prokaryota, cuyo concepto coincide con el reino Monera de las clasificaciones de Herbert Copeland o Robert Whittaker que, aunque anteriores, continúan siendo aún populares.[3]  Casi sin excepción los organismos basados en células procariotas son unicelulares.[4]  Se cree que todos los organismos que existen actualmente derivan de una forma unicelular procariota (LUCA).[5]   Existe una teoría, llamada endosimbiosis seriada, que considera que a lo largo de un lento proceso evolutivo, hace unos 2300 millones de años,[6] los procariontes derivaron en seres más complejos por asociación simbiótica: los eucariotes.  Estructura celular La estructura celular procariota básica tiene los siguientes componentes:  Adicionalmente también puede haber:  Para su comparación con la célula eucariota, véase la Tabla comparativa.  Diversidad bioquímica y metabólica Desde su aparición, no han sufrido gran diversificación. El metabolismo de las procariotas es enormemente variado (a diferencia de las eucariotas), y causa que algunas procariotas sean muy diferentes a otras. Algunas son muy resistentes a condiciones ambientales extremas como temperatura o acidez, se las llama Extremófilos.[7]  Nutrición La nutrición puede ser autótrofa (quimiosíntesis o fotosíntesis) o heterótrofa (saprofita, parásita o simbiótica). En cuanto al metabolismo los organismos pueden ser: anaerobios estrictos o facultativos, o aerobio.  Los organismos capaces de llevar a cabo este proceso se denominan fotótrofos y si además son capaces de fijar el CO2 atmosférico (lo que ocurre casi siempre) se llaman autótrofos.[10] Salvo en algunas bacterias, en el proceso de fotosíntesis se producen liberación de oxígeno molecular (proveniente de moléculas de agua) hacia la atmósfera (fotosíntesis oxigénica).  Es ampliamente admitido que el contenido actual de oxígeno en la atmósfera se ha generado a partir de la aparición y actividad de dichos organismos fotosintéticos.[11] Esto ha permitido la aparición evolutiva y el desarrollo de organismos aerobios capaces de mantener una alta tasa metabólica (el metabolismo aerobio es muy eficaz desde el punto de vista energético).  La otra modalidad de fotosíntesis, la fotosíntesis anoxigénica, en la cual no se libera oxígeno, es llevada a cabo por un número reducido de bacterias, como las bacterias púrpuras del azufre y las bacterias verdes del azufre; estas bacterias usan como donador de hidrógenos el H2S, con lo que liberan azufre.[12]  Reproducción Se da de dos maneras: reproducción asexual o parasexual  Tipos de Célula Procariota Según su morfología En otros casos, especialmente en arqueas, se puede encontrar una variada morfología, lo que incluye formas pleomórficas (formas cambiantes), irregulares, lobuladas, planas, rectangulares o cuadradas como Haloquadratum.  Según la envoltura celular Dependiendo del tipo de pared celular y el número de membranas, pueden haber los siguientes tipos de células procariotas:[19]  Clasificación biológica Según el Sistema de tres dominios los grupos procariotas principales son Archaea y Bacteria.[21] La diferencia más importante que sustentó en un inicio la diferencia entre estos dos grupos está en la secuencia de bases nitrogenadas de las fracciones del ARN ribosomal 16S"
biologia,"La genética (del griego antiguo: γενετικός, guennetikós, ‘genetivo’, y este de γένεσις, génesis, ‘origen’)[1][2][3] es el área de estudio de la biología que busca comprender y explicar cómo se transmite la herencia biológica de generación en generación mediante el ADN. Se trata de una de las áreas fundamentales de la biología moderna, abarcando en su interior un gran número de disciplinas propias e interdisciplinarias que se relacionan directamente con la bioquímica, medicina, y la biología celular.  El principal objeto de estudio de la genética son los genes, formados por segmentos de ADN y ARN, tras la transcripción de ARN mensajero, ARN ribosómico y ARN de transferencia, los cuales se sintetizan a partir de ADN. El ADN controla la estructura y el funcionamiento de cada célula, tiene la capacidad de crear copias exactas de sí mismo tras un proceso llamado replicación.  Primeros estudios genéticos Gregor Johann Mendel (20 de julio de 1822[4]-6 de enero de 1884) fue un monje agustino católico y naturalista nacido en Heinzendorf, Austria (actual Hynčice, distrito Nový Jičín, República Checa) que descubrió, por medio de la experimentación de mezclas de diferentes variedades de guisantes, chícharos o arvejas (Pisum sativum), las llamadas Leyes de Mendel que dieron origen a la herencia genética.   En 1941 Edward Lawrie Tatum y George Wells Beadle demostraron que los genes ARN mensajero codifican proteínas; luego en 1953 James D. Watson y Francis Crick determinaron que la estructura del ADN es una doble hélice en direcciones antiparalelas, polimerizadas en dirección 5' a 3', para el año 1977 Frederick Sanger, Walter Gilbert, y Allan Maxam secuencian ADN completo del genoma del bacteriófago y en 1990 se funda el Proyecto Genoma Humano.  La ciencia de la genética Aunque la genética juega con un papel muy significativo en la apariencia y el comportamiento de los organismos, es la combinación de la genética, replicación, transcripción y procesamiento (maduración del ARN) con las experiencias del organismo la cual determina el resultado final.  Los genes corresponden a regiones del ADN o ARN, dos moléculas compuestas de una cadena de cuatro tipos diferentes de bases nitrogenadas (adenina, timina, citosina y guanina en ADN), en las cuales tras la transcripción (síntesis de ARN) se cambia la timina por uracilo —la secuencia de estos nucleótidos es la información genética que heredan los organismos. El ADN existe naturalmente en forma bicatenaria, es decir, en dos cadenas en que los nucleótidos de una cadena complementan los de la otra.  La secuencia de nucleótidos de un gen es traducida por las células para producir una cadena de aminoácidos, creando proteínas —el orden de los aminoácidos en una proteína corresponde con el orden de los nucleótidos del gen. Esto recibe el nombre de código genético. Los aminoácidos de una proteína determinan cómo se pliega en una forma tridimensional y responsable del funcionamiento de la proteína. Las proteínas ejecutan casi todas las funciones que las células necesitan para vivir.  El genoma es la totalidad de la información genética que posee un organismo en particular. Por lo general, al hablar de genoma en los seres eucarióticos se refiere solo al ADN contenido en el núcleo, organizado en cromosomas, pero también la mitocondria contiene genes y es llamada genoma mitocondrial.  Subdivisiones de la genética La genética se subdivide en varias ramas, como:  Ingeniería genética La ingeniería genética es la especialidad que utiliza tecnología de la manipulación y trasferencia del ADN de unos organismos a otros, permitiendo controlar algunas de sus propiedades genéticas. Mediante la ingeniería genética se pueden potenciar y eliminar cualidades de organismos en el laboratorio (véase Organismo genéticamente modificado). Por ejemplo, se pueden corregir defectos genéticos (terapia génica), fabricar antibióticos en las glándulas mamarias de vacas de granja o clonar animales como la oveja Dolly.  Algunas de las formas de controlar esto es mediante transfección (lisar células y usar material genético libre), conjugación (plásmidos) y transducción (uso de fagos o virus), entre otras formas. Además se puede ver la manera de regular esta expresión genética en los organismos.  Respecto a la terapia génica, antes mencionada, hay que decir que todavía no se ha conseguido llevar a cabo un tratamiento, con éxito, en humanos para curar alguna enfermedad. Todas las investigaciones se encuentran en la fase experimental. Debido a que aún no se ha descubierto la forma de que la terapia funcione (tal vez, aplicando distintos métodos para introducir el ADN), cada vez son menos los fondos dedicados a este tipo de investigaciones. Por otro lado, aunque este es un campo que puede generar muchos beneficios económicos, este tipo de terapias son muy costosas, por lo que, en cuanto se consiga mejorar la técnica y  disminuir su coste, es de suponer que las inversiones subirán.  Genética muscular Investigaciones actuales afirman que los marcadores metabólicos entre los distintos tipos de genética muscular pueden diferenciarse en un 7-18%. La diferencia principal se encuentra en la reacción del cuerpo ante la ingesta de carbohidratos y los niveles de las hormonas sexuales como la testosterona.   La genética muscular es un área de la ciencia con potenciales herramientas para mejorar los resultados en el deporte. Determinar la predisposición genética de un individuo: ectomorfo, mesomorfo o endomorfo, es una estrategia utilizada por los profesionales del deporte para incrementar el rendimiento. Se han diferencias en la concentración de creatina en los distintos tipos somatotipos corporales así como diferencias en las concentraciones de distintos marcadores metabólicos. [5][6]  Cronología de descubrimientos genéticos notables Adaptaciones genéticas Los cambios genéticos pueden dotar a las especies de rasgos complejos que les permita expandirse y ocupar nuevos nichos. Estos cambios son claves para la especiación y diversificación. Por ejemplo, para adaptarse a la vida en las alturas de los árboles, diferentes especies de ranas han adquirido evolutivamente rasgos complejos para escalar y planear.  "
biologia,"La selección natural es un proceso evolutivo que fue descrito por Charles Darwin en su libro El origen de las especies e inspirado en las ideas del Ensayo sobre el principio de la población de Thomas Malthus que establece la supervivencia del más apto o la preponderancia de la ley del más fuerte en un medio natural sin intervención externa, por lo que los individuos menos aptos o más débiles perecen y sus rasgos no se transmiten a las generaciones siguientes al no reproducirse, en contraposición al concepto de selección artificial donde sí existe una intervención directa, por el humano, con el propósito de mejorar los rasgos de los individuos manipulándolos a voluntad. Estrictamente hablando, se define como la supervivencia y reproducción diferencial de los fenotipos de una población biológica. La formulación clásica de la selección natural establece que las condiciones de un medio ambiente favorecen o dificultan, es decir, seleccionan la reproducción de los organismos vivos según sean sus peculiaridades. La selección natural fue propuesta por Darwin como medio para explicar la evolución biológica. Esta explicación parte de tres premisas; la primera de ellas es que el rasgo sujeto a selección debe ser heredable. La segunda sostiene que debe existir variabilidad del rasgo entre los individuos de una población. La tercera premisa plantea que la variabilidad del rasgo debe dar lugar a diferencias en la supervivencia o éxito reproductor, haciendo que algunas características de nueva aparición se puedan extender en la población. La acumulación de estos cambios a lo largo de las generaciones produciría todos los fenómenos evolutivos.  En su formulación inicial, la teoría de la evolución por selección natural constituye el gran aporte[1] de Charles Darwin (e, independientemente, por Alfred Russel Wallace), y es un pilar fundamental del darwinismo, posteriormente reformulado en la actual teoría de la evolución conocida como neodarwinismo o síntesis evolutiva moderna. En biología evolutiva, el proceso de selección natural se considera la principal causa del origen de las especies y de su adaptación al medio.   La selección natural puede ser expresada con la siguiente ley general, tomada de la conclusión de El origen de las especies: El resultado de la repetición de este esquema a lo largo del tiempo origina la evolución de las especies.  En la teoría moderna En la teoría sintética la selección natural no es la única causa de evolución, aunque sí la que tiene un papel más destacado. El concepto de selección natural se define ahora de un modo más preciso: como la reproducción diferencial de los fenotipos en una población. Desde el momento en que existen diferencias en éxito reproductivo de las distintas variantes (con o sin una base genética), existe la selección natural. Por ejemplo: si los individuos más verdosos en una población de insectos-hoja aportan unos tres descendientes a la siguiente generación, y los individuos marrones aportan como media 1,5 descendientes, está habiendo selección a favor de los verdes. Las diferencias en éxito reproductivo pueden ocurrir por diversas causas (diferente fertilidad, riesgo de muerte por depredadores, atractivo sexual, capacidad para explotar los recursos alimenticios, etc.).  Figuras importantes del síntesis, y los tres fundadores de la genética de las poblaciones, fueron Ronald Fisher, quien escribió The Genetical Theory of Natural Selection en 1930, J.B.S. Haldane, quien introdujo el concepto del «costo» de la selección natural,[2] y Sewall Wright, quien elucidó sobre la selección y la adaptación.[3]   Generalmente, existe una correlación entre la eficacia reproductiva de los portadores de un genotipo y la adaptación al medio que este les otorga. Por tanto, los rasgos que confieren ventajas adaptativas comúnmente son seleccionados a favor y propagados en las poblaciones (en algunos casos, un genotipo podría otorgar éxito reproductivo sin aportar mayor adaptación al medio, y sería seleccionado igualmente). La teoría de la selección natural aportó por primera vez una explicación científica satisfactoria para múltiples enigmas científicos del mundo biológico, especialmente el de la ""apariencia de diseño"" que existe en los seres vivos. Permitió, por tanto, que la Biología pudiera prescindir de los elementos divinos y sobrenaturales y se convirtiera así en una auténtica ciencia.  Hoy en día, la evolución por selección natural se estudia en diversos tipos de organismos, mediante experimentos de laboratorio y de campo, y se desarrollan métodos para averiguar qué genes han estado recientemente sometidos a la acción de la selección natural y con qué intensidad.  Aptitud El concepto de aptitud es clave en la selección natural. A grandes rasgos, los individuos que son más aptos tienen mayor potencial de supervivencia, similar a la popular frase «supervivencia del más apto». Sin embargo, y como ocurre con el término selección natural, el significado preciso es más sutil. Richard Dawkins lo evita totalmente en sus últimos libros, aunque dedica un capítulo de su libro El fenotipo extendido a discutir sobre los distintos sentidos en que el término se usa. La teoría evolutiva moderna define la aptitud no sobre la base de cuánto vive el organismo, sino sobre la base de cuánto se reproduce. Si un organismo vive la mitad que otros de su especie, pero en comparación con el resto, el doble de sus descendientes llegan a la edad adulta; entonces sus genes sobrevivirán y se propagarán a la siguiente generación.  Aunque la selección natural opera sobre los individuos, los efectos del azar hacen que la aptitud solo pueda ser definida en promedio para los individuos de una población. La aptitud de un determinado genotipo corresponde al efecto medio sobre todos los individuos con ese genotipo. Los genotipos de muy baja aptitud causan que sus portadores tengan muy poca —o ninguna— descendencia en promedio. Se pueden citar como ejemplos muchas enfermedades genéticas humanas, como la fibrosis quística.  Como la aptitud es una cantidad promediada, es también posible que una mutación favorable que se dé en un individuo no llega a propagarse al grupo si el individuo fallece antes de la edad adulta por otros motivos. La aptitud depende totalmente del entorno. Condiciones como la anemia de células falciformes son muy ineptas en la población humana general. Sin embargo, la anemia de células falciformes confiere al portador inmunidad a la malaria por lo que su aptitud en entornos con altas tasas de infección de malaria es muy alta.  Tipos de selección natural La selección natural puede actuar sobre cualquier rasgo fenotípico heredable y cualquier aspecto del entorno puede producir presión selectiva, esto incluye la selección sexual y la competencia con miembros tanto de la misma como de otra especie. Sin embargo, esto no implica que la selección natural siga siempre una dirección y que resulte en evolución adaptativa. La selección natural produce a menudo el mantenimiento del statu quo mediante la eliminación de las variantes menos aptas.  La unidad de selección puede ser el individuo u otro nivel dentro de la jerarquía de organización biológica como los genes, las células y los grupos familiares. La cuestión sobre si la selección natural actual a nivel de grupo (o especie) para producir adaptaciones que benefician a un grupo mayor, sin vínculos familiares, suscita aún un tenue debate. Así mismo, existe un cierto debate sobre si la selección a nivel molecular anterior a mutaciones genéticas y a la fertilización del zigoto debe considerarse selección natural convencional puesto que tradicionalmente se ha llamado selección natural a una fuerza exterior y ambiental que actúa sobre un fenotipo después del nacimiento. Algunas revistas científicas distinguen entre selección natural y selección genética llamando informalmente a la selección de mutaciones como preselección.[4]  La selección a otros niveles, como el gen, puede resultar en una mejora para el gen y al mismo tiempo en un perjuicio para el individuo portador del gen. Este proceso se denomina conflicto intragenómico. En conjunto, el efecto combinado de todas las presiones a los distintos niveles (gen, individuo, grupo) es lo que determina la aptitud de un individuo y por tanto el resultado de la selección natural.  La selección natural ocurre en cada etapa de la vida de un individuo. Un organismo ha de sobrevivir hasta la edad adulta para poder reproducirse. La selección de aquellos que alcanzan la etapa adulta es llamada selección de viabilidad. En muchas especies los adultos han de competir entre sí para conseguir parejas sexuales. Este mecanismo se denomina selección sexual y el éxito en la misma determina quienes serán los padres de la siguiente generación. Cuando los individuos pueden reproducirse en más de una ocasión, la supervivencia en la edad adulta aumenta la descendencia. A este proceso se le llama selección de supervivencia.  La fecundidad, tanto de machos como de hembras, puede verse limitada por la ""selección de fecundidad"". Así, la viabilidad de los gametos producidos variara. Los conflictos intragenómicos derivan en selección genética. Finalmente, la unión de algunas combinaciones de óvulos y esperma será estadísticamente más compatible que otras. A esto se le llama selección por compatibilidad.  Existen cuatro tipos —a veces considerados tres— de selección natural, clasificados según los individuos que sobreviven en cada tipo de selección, es decir, según cuántos sobrevivan:  Selección sexual Es útil diferenciar entre selección ecológica y selección sexual. La selección ecológica se refiere a cualquier mecanismo de selección como resultado del entorno. Por otra parte, la selección sexual se refiere específicamente a la competencia por la pareja sexual.[5]  La selección sexual puede ser intrasexual, que es el caso de competición entre individuos del mismo sexo en una población, o intersexual, que es cuando un sexo controla el acceso a la reproducción mediante la elección de pareja dentro de la población. Normalmente, la selección intrasexual se da en forma de competición entre machos y la intersexual como elección por parte de las hembras de los mejores machos, debido al mayor coste que para las hembras generalmente conlleva la cría. Sin embargo, algunas especies presentan los papeles sexuales cambiados y es el macho el que se muestra más selectivo a la hora de escoger pareja. El ejemplo más conocido es el de ciertos peces de la familia Syngnathidae. También se han encontrado ejemplos similares en anfibios y pájaros.[6]  Algunas características presentes solo en uno de los dos sexos en especies concretas se pueden explicar a través de la presión ejercida por el otro sexo en su elección de pareja. Por ejemplo, el extravagante plumaje de algunos pájaros macho como el pavo real. Así mismo, la agresión entre miembros del mismo sexo se asocia en ocasiones con características muy distintivas como las cuernas del ciervo, que sirven para pelear con otros ciervos. En general, la selección intrasexual se asocia con el dimorfismo sexual, que incluye diferencias en el tamaño del cuerpo de los machos y las hembras.[7]  Ejemplos de selección natural Un ejemplo muy conocido de selección natural es el desarrollo de resistencia a antibióticos en microorganismos. Desde el descubrimiento de la penicilina en 1928 por Alexander Fleming, los antibióticos se han usado para combatir las enfermedades de origen bacteriano. Las poblaciones naturales de bacterias contienen una gran variación en su acervo génico, principalmente como resultado de mutaciones. Cuando se enfrentan a un antibiótico, la mayoría mueren enseguida. Sin embargo, algunas tienen mutaciones que las hace menos débiles a ese antibiótico concreto. Si el enfrentamiento con el antibiótico es corto, algunos de estos individuos sobrevivirán al tratamiento. Esta selección eliminadora de individuos poco aptos de una población es la selección natural.  Las bacterias supervivientes se reproducirán formando la siguiente generación. Debido a la eliminación de los individuos mal adaptados en la generación pasada, la población contendrá más bacterias que tienen cierto grado de resistencia antibiótica. Al mismo tiempo, surgen nuevas mutaciones de las cuales algunas pueden añadir más resistencia a la bacteria portadora del gen mutante. Las mutaciones espontáneas son poco frecuentes y las ventajosas son aún más infrecuentes. "
biologia,"En taxonomía, se denomina especie (del latín species) a la unidad básica de clasificación biológica. Una especie es un conjunto de organismos o poblaciones naturales capaces de entrecruzarse y producir descendencia fértil, aunque —en principio—  no con miembros de poblaciones pertenecientes a otras especies. En muchos casos los individuos que se separan de la población original y se aíslan del resto pueden alcanzar una diferenciación suficiente como para convertirse en una nueva especie, por lo tanto, el aislamiento reproductivo respecto de otras poblaciones es crucial. En definitiva, una especie es un grupo de organismos reproductivamente homogéneo, aunque muy cambiante a lo largo del tiempo y del espacio.  Mientras que en muchos casos esta definición es adecuada, es a menudo difícil demostrar si dos poblaciones pueden cruzarse y dar descendientes fértiles (por ejemplo, muchos organismos no pueden mantenerse en laboratorio suficiente tiempo). Además, es imposible aplicarla a organismos que no se reproducen sexualmente (como las bacterias) ni a organismos extintos conocidos solo por sus fósiles. En la actualidad suelen aplicarse técnicas moleculares, como las basadas en la semejanza del ADN.  Los nombres comunes de las plantas y los animales corresponden algunas veces con su respectiva especie biológica (por ejemplo, «león», «morsa» y «árbol del alcanfor»), pero con mucha frecuencia no es así: por ejemplo, la palabra pato se refiere a una veintena de especies de diversos géneros, como el pato doméstico. Por ello, para la denominación de las especies se utiliza la nomenclatura binomial, mediante la cual cada especie queda inequívocamente definida con dos palabras —por ejemplo Homo sapiens, la especie humana—. En esta nomenclatura, el primer término corresponde al género: el rango taxonómico superior en el que se pueden agrupar las especies.  Determinación de los límites La determinación de los límites de una especie es puramente subjetiva y, por tanto, expuesta a la interpretación personal. Algunos conceptos usuales son antiquísimos, muy anteriores al establecimiento científico de esta categoría taxonómica. Por el contrario, existen otros de límites muy vagos, en los cuales los sistemáticos están en completo desacuerdo. Si las especies fueran inmutables, se podría definir fácilmente cada una de ellas diciendo que es el conjunto de individuos (que fueron, que son y que serán, de no extinguirse) de caracteres cualitativamente idénticos. Una entidad así determinada no es realmente una especie, sino lo que usualmente se llama una línea pura o un clon.  La delimitación de especies afecta directamente a aspectos tan importantes y actuales como la Biología de la conservación, o a campos aplicados como la modelización de distribuciones, de las que se puede obtener información muy valiosa.  El número de especies presentes en algún territorio es una forma de estimar la riqueza, complejidad y aportación al patrimonio natural de sus habitantes.[1]  Niveles de clasificación Subdividiendo, se encuentran las siguientes categorías:  Historia del concepto de especie El término especie se refiere a tres conceptos distintos, aunque relacionados. El rango especie, que es el nivel más básico de la taxonomía de Linneo; los taxones especie, que son un grupo de organismos descritos y asignados a la categoría especie, y las especies biológicas, que son entes capaces de evolucionar. Respecto a esto, debemos decir que la idea de evolución ya se encontraba en la antigüedad clásica. Anaximandro afirmó que las primeras criaturas habían surgido del agua para pasar a tierra; mientras que Empédocles aseguró que eran partes separadas que, en un momento dado, llegaron a juntarse, lo que nos recuerda la teoría de la simbiogénesis.[2][3] Cicerón escogió el término latino species, 'aspecto característico, forma', para traducir el término griego ἰδέα (idéa) 'aspecto, apariencia, forma',  relacionado con εῖδος, (eîdos) ‘vista, visión, aspecto’, término platónico cargado de connotaciones dialécticas y lógicas.  Para el biólogo Ernst Mayr, Platón fue ""el gran antihéroe del evolucionismo"" por causa de su creencia en el Mundo de las ideas.[4] Aristóteles, en cambio, se mantuvo en una posición ambigua: aportó lo que él consideraba ""pruebas"" de una generación espontánea, pero habló de una ""causa final"" de toda especie,[5] su entelequia, y rechazó explícitamente la idea de Empédocles que decía que las criaturas vivientes se habrían originado por casualidad.[6] Zenón de Citio, según Cicerón, siguió en esta línea.  De los datos que nos aporta Lucrecio, extraemos que Epicuro se anticipó a la ley de la selección natural.[7] El degradacionista Agustín de Hipona dice, en cambio, que el Génesis debe interpretarse y que no hemos de suponer que Dios creara las especies que vemos ahora con sus imperfecciones. Este hecho y el proceso contra Galileo fue clave, siglos más tarde, para la rápida aceptación por la Iglesia católica de la teoría de la evolución. Durante la Edad Media, en una época de indeterminación fomentada por la inestabilidad política, se confundieron frecuentemente los términos ""especie"" y ""género"". Esto podría justificarse con base en el texto de la Vulgata:  Se dejó abierta la posibilidad de que hubiera especies y géneros no creados por Dios o no descubiertos por el hombre europeo. El nominalismo tuvo sus raíces en el siglo XIV con Guillermo de Ockham. Esta doctrina señalaba que no existía ninguna entidad entre el término y los individuos a los que este se refería, es decir, solo existían los individuos. Según esta doctrina, las especies son fruto de nuestra razón y el concepto especie se utiliza solo con el fin de agruparlos por su parecido y darles un nombre. En pocas palabras, el nominalismo no reconoce a las especies como entidades reales.  Linneo y John Ray, por su parte, afianzaron la idea del carácter discreto y de la posesión de atributos objetivos de las especies que permitían su delimitación; es decir, la realidad de las especies. A partir de la publicación de El origen de las especies de Charles Darwin en 1859, se comenzó a considerar a la especie como un agregado de poblaciones morfológicamente variables y con la capacidad de evolucionar. El concepto aristotélico-linneano fue gradualmente reemplazado por una concepción evolutiva basada en la selección natural y en el aislamiento reproductivo.  John Ray definió a la especie como un grupo de individuos semejantes, con antepasados comunes. Igualmente, expresó que ""una especie nunca nace de la semilla de otra especie"", es decir, los conejos no nacen de monos, ni las arvejas dan rosas.  A mediados del siglo XX se plantearon dos posturas respecto a las especies: el realismo evolutivo y el nominalismo. El último sostuvo que en la naturaleza solo existen los organismos individuales y, según los taxónomos evolutivos, las especies son entidades reales de la naturaleza y constituyen unidades de evolución. A partir de la década de 1980, se afianzó la postura realista con respecto a las especies biológicas, conjuntamente con el enfoque filogenético de la clasificación.[8]  De acuerdo con Häuser (1987), los atributos generales del concepto especie deben ser universalidad, aplicabilidad práctica y criterio decisivo.[9] La mayoría de los biólogos que se ocupan de la sistemática de plantas y animales usan el CBE en conjunto con la descripción de la morfoespecie (King, 1993).  Conceptos de especie Otras definiciones de especie Existen multitud de definiciones de especie:  Nomenclatura Los nombres de las especies son binominales, es decir, formados por dos palabras, que deben escribirse en un tipo de letra distinto del texto general (usualmente en cursiva; de las dos palabras citadas, la primera corresponde al nombre del género al que pertenece y se escribe siempre con la inicial en mayúscula; la segunda palabra es el epíteto específico o nombre específico y debe escribirse enteramente en minúscula y debe concordar gramaticalmente con el nombre genérico). Así, en Mantis religiosa, Mantis es el nombre genérico, religiosa el nombre específico y el binomio Mantis religiosa designa esta especie de insecto.  En el nombre científico asignado a las especies, el nombre específico nunca debe ir aislado del genérico ya que carece de identidad propia y puede coincidir en especies diferentes. Si se ha citado previamente el nombre completo y no cabe ninguna duda de a qué género se refiere, el nombre del género puede abreviarse a su inicial (M. religiosa).  Abreviaturas En los libros y artículos académicos, a veces, no se identifican intencionalmente las especies plenamente y se recurre a utilizar la abreviatura ""sp."" en singular o ""spp."" en plural, en lugar del epíteto específico, por ejemplo: Canis sp. La abreviatura plural ""spp."" se utiliza generalmente para referirse a todas las especies individuales dentro de un género. Para una especie concreta cuyo epíteto específico es desconocido o carece de importancia se utiliza ""sp."".  Esto ocurre comúnmente en los siguientes tipos de situaciones:  En los libros y artículos, los nombres de géneros y especies generalmente se imprimen en letra cursiva. Las abreviaciones como «sp.», «spp.», «subsp.», etc., no deben estar en cursivas.  Número estimado de especies conocidas El número real de especies es muy impreciso y varía notablemente según las fuentes. Algunas estimaciones abarcaban un total de entre 1,5 y 2 millones de especies, aunque un estudio de 2011 aumenta esa cifra hasta una horquilla entre los 7,5 y los 10 millones.[19] Las especies colocadas en la siguiente lista son especies conocidas y existentes actualmente. No se han incluido fósiles (pues se podría añadir gran número, principalmente de artrópodos, invertebrados menores, peces y reptiles).  Cada año se describen alrededor de 10 000 nuevas especies, de las cuales, solo una decena son vertebrados, y, estadísticamente, solo 0,4 anuales son mamíferos.  Tamaño relativo de los grandes grupos de animales.  (Fuente de datos animales: Brusca, R. C. & Brusca, G. J. 1990. Invertebrates. Sinauer Associates, Sunderland.)  Reeder T. et al. 2002. Amer. Mus. Novit.  "
biologia,"La clasificación biológica o clasificación científica en biología es un método mediante el cual los biólogos agrupan y categorizan las especies de organismos (sean especies extintas o vivas) y a sus diferentes conjuntos (taxones). La clasificación biológica es una forma de taxonomía científica que se distingue de la taxonomía popular, que carece de base científica. La moderna clasificación biológica nació con los trabajos de Carlos Linneo (1753), quien agrupó a las especies de acuerdo a sus características físicas compartidas y normalizó su denominación. Esta clasificación ha sido revisada para ajustarla a la idea darwiniana del antepasado común. La mayoría de las más recientes revisiones se basan en análisis moleculares de ADN, que usan como datos secuencias de ADN. La clasificación biológica pertenece a la ciencia de la biología sistemática.  La clasificación científica es una de las tareas de la biología sistemática y, más en particular, de la taxonomía biológica, que no sólo admite una jerarquización de características y funciones (taxonomía), sino que también permite establecer un esquema de parentescos, similitudes y relaciones (sistemática) entre los diferentes organismos.  La utilidad principal de la clasificación es que en un nivel científico haya un consenso general y casi universal para establecer un orden esquemático sobre la enorme diversidad de los organismos.  Primeros sistemas Época clásica y medieval Los actuales sistemas de clasificación de las formas de vida descienden del pensamiento presentado por el filósofo griego Aristóteles, quién publicó en sus trabajos metafísicos y lógicos la primera clasificación conocida y que se cree es anterior a cualquier otra existente. Ésta es la matriz moderna que acuñó palabras como sustancia, especie y género, términos conservados y redefinidos por Carlos Linneo.[1]  Aristóteles también estudió a los animales y los clasificó de acuerdo a su método de reproducción, cosa que luego hizo Linneo con las plantas. La clasificación animal de Aristóteles fue abandonada tan pronto como el nuevo conocimiento adicional se superpuso, y fue olvidada.  La clasificación filosófica es en general como sigue.[2] La substancia primaria es la existencia individual; por ejemplo, Pedro, Pablo, etc. La substancia secundaria es el predicado que acompaña a la substancia primera, o que lo especifica a una categoría, por ejemplo Pedro es un hombre. La propiedad o característica ser hombre es la substancia segunda de la substancia primera que es la existencia de Pedro. La característica no debe estar solamente en lo individual; por ejemplo, ser hábil con la gramática. La habilidad en gramática en su mayor parte está fuera de Pedro y, por tanto, no es una característica suya. Similarmente, la humanidad no está en Pedro; sin embargo, el es un hombre.  Las especies son la substancia secundaria pues son más referidas a la propiedad que a la individualidad. La principiar característica que puede hallarse en Pedro es que Pedro es un hombre. Se ha creado una identidad: ""hombre"", que es igual para todos los individuos que lo son y sólo para aquellos individuos. Los miembros de una especie difieren sólo en número pero son todos del mismo tipo.  El género es una substancia secundaria menos característica y más general que la especie. Por ejemplo, el hombre es un animal. No todos los animales son hombres. Es menester que un género está formado por especies. No existe límite en el número de géneros aristotélicos que se pueden encontrar que contengan especies. Aristóteles no estructuró los géneros en clases, filos, etc, tal y como sí hizo Linneo en su clasificación.  La substancia secundaria que distinguen unas especies de otras dentro de un género es la diferencia específica. Un hombre puede ser comprehendido entonces como la suma de sus diferencias específicas (la ""differentiae"" de biología) en categorías cada vez menos generales. Esta suma es la definición; por ejemplo, un hombre es una substancia animada, sensata y racional. La definición más característica contiene la especie, y la más general el género: el hombre es una animal racional. La definición está entonces basada en el problema de la unidad: las especies son solo si todavía tienen muchas differentiae.  Por encima de los géneros están las categorías. Son diez: una de substancia y nueve de ""accidentes"", universales que deben estar ""en"" una substancia. Las substancias existen por sí mismas; los accidentes están solo en ellas: cantidad, calidad, etc. No existe la categoría más alta, ""ser"", debido al siguiente problema, que no fue resuelto hasta la Edad Media por Tomás de Aquino: una diferencia específica no es característica de su género. Si un hombre es un animal racional, entonces la racionalidad no es una propiedad de los animales. La substancia, por tanto, no es un tipo de ""ser"" porque no tiene diferencias específicas, que no podrían ser no-ser.  El problema del ser ocupó la atención de la escolástica durante la Edad Media. La solución de Santo Tomás, calificó la analogía del ser, estableciendo el campo de la ontología, que recibió la mayor parte de la publicidad y dibujando la línea entre la filosofía y la ciencia experimental. El último apareció durante el Renacimiento de técnica práctica. Linnaeus, un erudito clásico, combinó los dos en el umbral del resurgimiento neo-clásico conocido ahora como el Siglo de las Luces.  Renacimiento El profesor suizo Conrad von Gesner (1516-1565) hizo una aportación importante. El trabajo de Gesner fue una crítica compilación sobre las formas de vida conocidas hasta su época.  La llegada al Nuevo Mundo produjo un gran número de nuevas plantas y animales aún no descritos y clasificados por los colonizadores. Los viejos sistemas tenían dificultades para estudiar y localizar todas esas nuevas especies y a menudo las mismas plantas y animales tenían diferentes nombres simplemente porque había descritas demasiadas especies como para estar al tanto. Se necesitaba un nuevo sistema que pudiera agrupar a estos nuevos especímenes junto a los ya conocidos. El sistema binominal se desarrolló basándose en la morfología de los grupos con características similares. A finales del siglo XVI y comienzos del XVII, comenzó un estudio minucioso de los animales que, comenzando por los tipos conocidos, creció gradualmente hasta formar un cuerpo de conocimiento suficiente como para establecer unas bases anatómicas de clasificación. Los modernos avances en la clasificación de seres vivos se deben a las investigaciones en anatomía médica, de manos de anatomistas tales como Fabricius (1537–1619), Petrus Severinus (1580–1656), William Harvey (1578–1657), y Edward Tyson (1649–1708); así como a los progresos en entomología y los primeros microscopios que posibilitaron los trabajos y avances de Marcello Malpighi (1628–1694), Jan Swammerdam (1637–1680), y Robert Hooke (1635–1702). Lord Monboddo (1714–1799) fue uno de los primeros pensadores abstractos cuyos trabajos ilustraron el conocimiento entre especies relacionadas, que fueron preludio de la teoría de la evolución. Los sucesivos desarrollos en la historia de la entomología pueden seguirse en el sitio web ""Insecta"",[3] haciendo clic en los sucesivos trabajos ordenados cronológicamente.  Primeros metodistas A finales del siglo XV, algunos autores llegaron a preocuparse por el llamado methodus (método). Por método los autores se refieren a la organización de minerales, plantas y animales acorde a los principios de la división lógica. El término metodistas fue acuñado por Carlos Linneo en su Bibliotheca botanica para denotar a los autores preocupados por los principios de la clasificación (en contraste con los llamados coleccionistas que se centraban primordialmente en la descripción de las plantas poniendo poca o nula atención a su clasificación en géneros, etc). Los primeros metodistas importantes fueron el filósofo, físico y botánico italiano Andrea Caesalpino, el naturalista inglés John Ray, el físico y botánico alemán Augustus Quirinus Rivinus, y el físico, botánico y viajero Joseph Pitton de Tournefort.  Andrea Caesalpino (1519–1603) en su De plantis libri XVI (1583) propuso la primera organización metódica de plantas. En las bases de su estructura, con respecto al tronco y la fructificación, él dividió las plantas en cincuenta ""grandes géneros"".  John Ray (1627–1705) fue un naturalista inglés que publicó un importante trabajo sobre plantas, animales y teología natural. El enfoque escogido en la clasificación de las plantas en su De historia plantarum fue un importante paso hacia la taxonomía moderna. Ray rechazó el sistema de división dicotómica por el cual las especies eran clasificadas de acuerdo a un preconcebido sistema del tipo uno/otro, y en vez de ello clasificó las plantas de acuerdo a similitudes y diferencias procedentes de su observación.   Tanto Caesalpino como Ray usaron los nombres tradicionales de las plantas y, de este modo, los nombres de las plantas no reflejaban su posición taxonómica (por ejemplo, aunque sabiendo que la manzana y el melocotón pertenezcan a diferentes ""grades géneros"" del methodus de John Ray, ambos conservaron sus nombres tradicionales,Malus and Malus Persica respectivamente. Un importante paso fue dado por Rivinus and Pitton de Tournefort quienes hicieron del género un rango distinto en la jerarquía taxonómica e introdujo la práctica de nombrar a las plantas de acuerdo a su género.  Augustus Quirinus Rivinus (1652–1723), en su clasificación de plantas basada en las características de sus flores, introdujo la categoría de orden (correspondiente al ""gran género"" de John Ray y Andrea Caesalpino). Él fue el primero en abolir la antigua división de las plantas en hierba y árbol e insistió en el método más certero de división, basado solamente en las partes de su fructificación. Rivinus usó extensivamente claves dicotómicas para definir tanto a los órdenes cómo a los géneros. Su método de nombramiento de plantas y especies se parece al de Joseph Pitton de Tournefort. Los nombres de todas las plantas pertenecientes al mismo género deben empezar con la misma palabra (nombre genérico). Cada género contiene varias especies y todas con el mismo y único nombre genérico, mientras que el segundo nombre de la especie era uno elegido conforme a sus diferencias específicas (differentia specifica).  Joseph Pitton de Tournefort (1656–1708) introdujo una jerarquía aún más sofisticada de clases, secciones, géneros y especies. Él fue el primero en usar consistentemente la composición uniforme de los nombres de las especies consistentes en un nombre genérico y una frase diagnóstico differentia specifica. A diferencia de Rivinus, él usó differentiae con todas las especies de géneros politípicos.  Sistemas modernos Características de los sistemas de clasificación Independientemente de la escuela que la defina, el fin último de la taxonomía es presentar un sistema de clasificación que agrupe a toda la diversidad de organismos en unidades discretas dentro de un sistema estable, sobre las que les sea posible trabajar a los investigadores.   Los sistemas de clasificación están compuestos por taxones (del griego ταξα, taxa) ubicados en sus respectivas categorías taxonómicas. La decisión de qué clados deberían convertirse en taxones, y la decisión de en qué categorías taxonómicas debería estar cada taxón, son un poco arbitrarias, pero hay ciertas reglas no escritas que los investigadores utilizan para que el sistema de clasificación sea ""útil"". Para que un sistema de clasificación resulte útil debe ser manejable, y para ello debe organizar la información de la forma en que sea más fácil de recordar. Judd y colaboradores (2002) coinciden en que:  Una vez decidido qué clados convertir en taxones, los sistemáticos deben decidir en qué categorías taxonómicas ubicarlos, lo cual es arbitrario. Por razones históricas se utilizan las categorías linneanas de clasificación: reino, filo o división, clase, orden, familia, género y especie (ver en la sección de historia de la taxonomía). Los mismos criterios utilizados para saber si nombrar un taxón pueden ser utilizados para saber en qué categoría taxonómica ubicarlo,[4] en especial el de la estabilidad en la nomenclatura.  Los sistemas de clasificación que nacen como resultado de la taxonomía tienen dos utilidades:  Linneano Dos años después de la muerte de John Ray, nació Carolus Linnaeus, conocido también como Carlos Linneo (1707–1778). "
biologia,"Un biólogo es un profesional y científico que cuenta con conocimientos del campo de la Biología, entendiendo los mecanismos que rigen el funcionamiento de los sistemas biológicos en campos como la salud, la tecnología, la producción y el medio ambiente; Este profesional puede trabajar en hospitales, universidades, clínicas, laboratorios clínicos, laboratorios de investigación, industrias farmacéuticas, agricultura, zoológicos, es decir, cualquier lugar donde haya vida, valorando el bienestar, la salud y la integridad de individuos y el medio ambiente.    En el área de la investigación, la misión del biólogo es investigar los organismos vivos, entendiendo y buscando soluciones a nivel genético, molecular, fisiológico, celular, ecológico, etc. Los biólogos investigadores realizan investigaciones científicas sobre el origen, las interacciones, la distribución, la composición actual, el aprovechamiento y la conservación de la diversidad biológica haciendo uso del método científico, para probar la validez de una teoría o hipótesis de manera racional, imparcial y reproducible. Además, custodia las colecciones biológicas y comunica este conocimiento entre la sociedad, con el propósito de contribuir a la comprensión y conservación de la vida en distintas regiones del mundo.   Los biólogos se especializan en diferentes áreas de la biología. Están los biólogos teóricos que utilizan métodos matemáticos o estadísticos  desarrollando modelos para comprender los fenómenos e idealmente predecir los resultados experimentales futuros, mientras que los biólogos experimentales conciben experimentos para probar esas predicciones.[1][2][3] Algunos biólogos trabajan con microorganismos, mientras que otros estudian organismos multicelulares (incluidos los humanos). Algunos investigan la nano o microescala, otros propiedades emergentes como las interacciones ecológicas o la cognición.   Existe una gran superposición entre los diferentes campos de la biología (por ejemplo: zoología, microbiología, genética y biología evolutiva) y debido a la naturaleza interdisciplinaria del campo, a menudo es difícil clasificar a un científico de la vida como solo uno de ellos. Muchos biólogos trabajan en investigación y desarrollo. Algunos realizan investigaciones fundamentales para avanzar en el conocimiento humano de la vida. Además, la investigación biológica aplicada a menudo ayuda al desarrollo de soluciones a problemas en áreas como la salud humana, la criminalística  y el medio ambiente natural.  Formación La formación de los biólogos varía de un país a otro, con una formación heterogénea y mixta, sin embargo, la formación más común es a nivel de licenciatura en biología, pudiendo haber países que requieran maestrías y doctorados para trabajar en algunas especialidades.  Un título en biología generalmente implica un conjunto de enseñanzas teóricas y prácticas que se complementa con ciencias básicas auxiliares, en general se imparten asignaturas como física, química, química orgánica, química analítica, matemáticas, bioestadística, biofísica, bioinformática, bioética, bioquímica, genética, biología molecular, biología celular, citología, microbiología, micología, inmunología, histología, fisiología, anatomía, zoología, botánica, entomología, etología, biología del comportamiento, ecología, biología evolutiva, biotecnología, patobiología, biología del desarrollo, biología de la reproducción, farmacología, parasitología, historia de la biología, sistemática, filogenia, biología humana, biología marina, paleontología, conservación ambiental, recursos naturales, legislación y administración, climatología, biogeografía, agronomía, entre otras diversas materias que complementan sus estudios, pudiendo acceder a diversas especialidades y posgrados.       Especialistas En sus comienzos como ciencia la Biología tenía como grandes pilares a dos áreas de estudio, que son la botánica y la zoología, sin embargo con el descubrimiento y desarrollo del microscopio, esta disciplina expandió su conocimiento hacia organismos que no eran visibles para el ojo humano.   Más adelante el redescubrimiento de los trabajos relacionados con la herencia, propuestos originalmente por el monje austriaco Gregor Mendel y la selección natural propuesta por Charles Darwin, desató la apertura de otras dos áreas de estudio que son la genética y la evolución.   Las subcategorías de esta disciplina puede clasificarse dependiendo del tipo de organismos estudiados, así como los métodos utilizados para su estudio. Algunos especialistas son:  Analista clínico: desempeña funciones relacionadas con el diagnóstico de enfermedades mediante técnicas biológicas.  Genetistas:  estudian la genética, la ciencia de los genes, la herencia y la variación de los organismos.  Biólogos moleculares: estudian los mecanismos que involucran el funcionamiento del ADN, las proteínas y el ARN. También desarrollan técnicas modernas de biología molecular en campos como la identificación genética y el diagnóstico molecular.  Neurocientíficos: estudian el sistema nervioso y las bases biológicas de la cognición y la conducta.  Bioquímicos: estudian la composición química de los seres vivos. Analizan las combinaciones químicas complejas y reacciones implicadas en el metabolismo, la reproducción y el crecimiento.   Microbiólogos: investigan el crecimiento y las características de organismos microscópicos, tales como bacterias, algas, virus y hongos, al igual que áreas relacionadas con la inmunología.  Fisiólogos:  estudian funciones de la vida de plantas y animales, en todo el organismo y en el nivel celular o molecular, en condiciones normales y anormales. Los fisiólogos a menudo se especializan en funciones tales como el crecimiento, la reproducción, la fotosíntesis, la respiración o el movimiento, también enfocan la fisiología en un área determinada o algún sistema del organismo.  Bioinformáticos o biólogos computacionales: aplican las técnicas de la informática, las matemáticas aplicadas y las estadísticas para hacer frente a los problemas biológicos. La atención se centra principalmente en el desarrollo de modelos matemáticos y técnicas de simulación computacional. Por estos medios se ocupa de temas de investigación científica con sus preguntas teóricas y experimentales sin un laboratorio.  Embriólogo: busca comprender los mecanismos relacionados con la reproducción. Trabaja principalmente con reproducción asistida humana y animal.  Ecólogos:  estudian las relaciones entre animales, plantas, personas y su entorno. Algunas áreas de su trabajo incluyen la gestión de áreas de conservación, el asesoramiento en la materia de protección del medio ambiente y la participación en proyectos de restauración de suelos contaminados, su trabajo se basa mayoritariamente en terreno con el fin de monitorear los procesos bióticos.  Zoólogos: estudian los animales y los procesos de la vida silvestre, su origen, comportamiento, enfermedades, procesos evolutivos y la conservación de estos mismos. Muchos zoólogos se identifican generalmente por el grupo de animales que estudian. Por ejemplo, los ornitólogos son especialistas en las aves, los mastozoólogos estudian a los mamíferos, los herpetólogos estudian reptiles y anfibios, los ictiólogos estudian a los peces, y los entomólogos a los insectos.   Mientras que por otra parte los Botánicos basan su estudio en las plantas, muchos investigadores estudian todos los aspectos de la vida vegetal, incluidas las algas, líquenes, musgos, helechos, coníferas y plantas con flores; otros se especializan en áreas tales como la identificación y clasificación de las plantas, la estructura y función de partes de plantas, la bioquímica de los procesos de la planta, las causas y curas de enfermedades con las plantas, la interacción de las plantas con otros organismos y el medio ambiente, el registro geológico de plantas y su evolución. Algunas especialidades relacionadas con el área de la botánica son por ejemplo los micólogos que estudian a los hongos.  Condiciones de trabajo Los biólogos generalmente no se exponen a condiciones inseguras o insalubres. Muchos de los que trabajan con organismos peligrosos o sustancias tóxicas en el laboratorio, se rigen siguiendo normas estrictas de seguridad para sus procedimientos, con el fin de evitar y prevenir una posible contaminación. Por otra parte, muchos botánicos, ecólogos y zoólogos requieren un estudio profundo en el campo, por lo cual en ciertas ocasiones demandan tener una actividad física acondicionada a tales exigencias, adaptándose a condiciones adversas, como lo son trabajar en climas cálidos o fríos, altitudes elevadas, desiertos, selvas, pantanos, o muchas veces en lugares que resultan inaccesibles para el común de las personas.  Los biólogos marinos se encuentran con una variedad de condiciones de trabajo. Algunos trabajan en laboratorios; otros trabajan en buques de investigación, y los que trabajan bajo el agua deben practicar buceo seguro mientras se trabaja en torno afilados de arrecifes de coral y la vida marina peligrosa. Aunque algunos biólogos marinos obtienen sus muestras del mar, muchos todavía pasan una buena parte de su tiempo en los laboratorios y oficinas, realizando pruebas, experimentos, registros de resultados o recopilaciones de datos.  La mayoría de los científicos en la actualidad dependen de subvención para financiar su investigación. Las cuales muchas veces pueden estar bajo presión para cumplir con los plazos y  especificaciones regidas en la escritura de la subvención, para ello deben organizar propuestas con el fin de buscar fondos nuevos o la extensión de esta misma.  La gran mayoría de los biólogos suelen trabajar las horas regulares. Si bien la semana laboral de 40 horas es común, más horas no son poco comunes. Los investigadores pueden ser obligados a trabajar horas impares en laboratorios o en otros lugares (especialmente mientras realizan trabajos de campo), dependiendo de la naturaleza de su investigación.  Especialidades de la biología Toxicología Bioinformática Biología cuántica Neurobiología  Biomedicina Biología clínica  Bioseguridad Biodefensa Control de plagas Epidemiología Seguridad alimentaria Terapias biológicas  Biología forense Medio ambiente Aerobiologia Etc etc, y cualquier campo de la vida que estudie organismos vivos o ancestrales así como sus vestigios  Día del biólogo Argentina En Argentina, el Día del Biólogo se celebra el 27 de junio; en 1812 se emitió la circular con la que se llevaría a cabo el primer acto oficial relacionado en ese país con esta profesión: el Museo de Historia Natural, que comenzaría a funcionar en 1826.[4]  México En México, el Día del Biólogo se celebra el 25 de enero: el 25 de enero de 1961 se fundó el Colegio de Biólogos de México.[5] Ese mismo día del año 2012 fallece el biólogo mexicano Julio Cicero.  Perú En Perú, el Día del Biólogo se celebra el 27 de noviembre. Se eligió ese día en honor del botánico, geógrafo y naturalista alemán Augusto Weberbauer, nacido el 26 de noviembre de 1871 en Breslau, Alemania, y fallecido en Lima el 16 de enero de 1948, que visitó, estudió, describió y analizó la fauna de Perú: creó un mapa fitogeográfico, dictó clases sobre botánica y botánica farmacéutica, y publicó El mundo vegetal de los Andes peruanos.[6]  Panamá En Panamá[7] se celebra el día del Biólogo los 28 de agosto.  Se conmemora en esa fecha la fundación del Colegio de Biólogos de Panamá (COBIOPA), desde su creación en 1982.  Oficialmente fue decretado como tal por los miembros de la Junta Directiva del COBIOPA en 2011, aunque durante más de veinte años se realizaban actividades para conmemorar esta fecha. Los miembros de la Junta de Facultad de Ciencias Naturales, Exactas y Tecnología de la Universidad de Panamá también emitió una aprobación respaldando esta declaratoria y se está a la espera de que el Consejo Académico de la Universidad de Panamá haga lo mismo y declarándolo conmemorativo. COBIOPA es una organización gremialista sin fines de lucro y que propulsamos desde la nuestra fundación la defensa del ejercicio profesional en el país.  A raíz de ello, y luego de muchos esfuerzos, logramos obtener la publicación de la Ley No. 17 de 2009 que regula el ejercicio profesional en la República de Panamá. Posteriormente, el 1 de junio de 2010 se logra la conformación del primero Consejo Técnico de las Ciencias Biológicas de Panamá, quien aprueba en esa fecha el Reglamento Interno de funcionamiento del Consejo Técnico y el Código de ética profesional de los profesionales de las Ciencias Biológicas en la República de Panamá.  En la actualidad, desde el 11 de enero de 2013 se ha iniciado la entrega de idoneidades a nivel nacional, un proceso que certifica y entrega la autorización oficial estatal para ejercer Ciencias Biológicas en el país. COBIOPA organiza las Olimpiadas de Biología en nuestra República en asocio a la Universidad de Panamá.  El biólogo en la cultura y la ficción Biólogos en películas Biólogos en series Honores y premios Quizás uno de los más altos honores otorgado a los biólogos, es el Premio Nobel de Fisiología o Medicina, otorgado desde 1901, por la Real Academia Sueca de Ciencias. Otro premio importante es el Premio Crafoord; establecido en 1980 "
matematicas,"Un matemático (del latín mathēmāticus, y este a su vez del griego μαθηματικός mathēmatikós) es una persona cuya área primaria de estudio e investigación es la matemática, es decir que contribuye con nuevo conocimiento en este campo de estudio. En sentido estricto, un matemático es un investigador en el área de las matemáticas. El término recubre una gran gama de competencias y de prácticas muy diferentes, que comparten un vocabulario común y un formalismo específico, así como una exigencia de rigor propia de esta disciplina. También se conoce por matemáticos a aquellos profesionales que han completado  la carrera universitaria en este campo.  El término genérico matemático puede decantarse en dominios más restringidos, como por ejemplo: geómetra, algebrista, analista, topólogo, estadístico, etc.  Distinto uso  del término matemático Existen principalmente dos interpretaciones, por un lado, se le llama matemático a aquella persona que trabaja activamente en la investigación matemática,[1] lo cual, en la actualidad, la mayoría de las veces se acompaña con publicaciones en revistas especializadas en el tema; a esta clasificación pertenecen Henri Poincaré o Andrew Wiles, por ejemplo. Por otro lado, matemático puede designar a una persona con conocimiento especiales en matemática,[2] o que trabajó en un campo conexo como la enseñanza o la vulgarización; como por ejemplo Aurelio Baldor o Martin Gardner.  La unión de matemática Internacional publica un anuario mundial de matemáticos,[3] la definición retenida es:  Se le llama matemático activo a toda persona que haya publicado, durante los 4 últimos años, al menos 2 artículos referenciados en las tres bases de datos bibliográficos: la Zentralblatt MATH, la Mathematical Reviews y la Referativny Zhurnal.  Suele hacerse a veces la distinción entre matemáticas puras y matemáticas aplicadas[4] para diferenciar la investigación en matemática, de la investigación en áreas relacionadas (industria, ingeniería, tecnología) o interdisciplinas (ciencias cognitivas), en ciencias afines (estadística, informática) o incluso en ciencias sociales (filosofía, historia). Esta distinción, sin embargo, no es aceptada unánimemente, como tampoco la clasificación de un matemático como ""científico"".[5]  Matemático puro Matemáticos del siglo XXI Alexander Grothendieck.  Saharon Shelah.  Andrew Wiles.  Grigori Perelmán.  Mujeres matemáticas Como consecuencia de las enormes dificultades e impedimentos con los que las mujeres han tenido que enfrentarse, a lo largo de la historia y en todos los lugares del mundo, para poder llevar a cabo una labor de estudio o investigación en matemáticas (y en la ciencia, en general), la mayoría de las personas que han sobresalido en el área de las matemáticas y han alcanzado renombre universal han sido hombres. A pesar de estos inconvenientes, ha habido mujeres que, gracias a una indomable voluntad, una posición social alta y, generalmente, a la ayuda de algún mecenas masculino, han dejado una huella imborrable en las matemáticas. Y no solo porque sus historias de superación sean un ejemplo, sino porque sus contribuciones científicas han tenido una notable repercusión y relevancia. Entre las mujeres matemáticas más prominentes nacidas antes del siglo XX podemos citar a: Téano de Crotona (siglo VI  a. C.), Hipatia de Alejandría (alrededor del 400), Ada Lovelace (1815-1852), Maria Gaetana Agnesi (1718-1799), Sophie Germain (1776-1831), Sofia Kovalévskaya (1850-1891), Alicia Boole Stott (1860-1940), Émilie du Châtelet (1706-1749), Carolina Herschel (1750-1848), Mary Somerville (1780-1872) y Florence Nightingale (1820-1910).  Los profundos cambios demográficos y sociales acontecidos principalmente desde el final de la Segunda Guerra Mundial han favorecido la integración de las mujeres en el ámbito laboral y la paulatina reducción de las diferencias de oportunidades con los hombres. Por tanto, la lista de grandes mujeres matemáticas del siglo XX es extensa y entre sus figuras más destacadas cabe mencionar a Mileva Marić (1875-1948), Emmy Noether (1882-1935), Mary Lucy Cartwright (1900-1998), Rózsa Péter (1905-1977), Grace Murray Hopper (1906-1992), Olga Taussky-Todd (1906-1995), Julia Robinson (1919-1985), Emma Castelnuovo, (1913-2014), María Wonenburger (1927-), Ingrid Daubechies (1954-)..  No obstante, la presencia de las mujeres en los puestos académicos y científicos de responsabilidad es escasa. Por ello, y como ocurre en los demás ámbitos del conocimiento,  en diversos países existen asociaciones de mujeres matemáticas con una fuerte implicación social en la búsqueda de la igualdad de oportunidades en el marco de la investigación y la docencia en matemáticas. Este es el caso de la Asociación Mujeres y Matemáticas,[12] la European Women in Mathematics (EWM)[13] o la Comisión Mujeres y Matemáticas de la Real Sociedad Matemática Española,[14] así como algunas asociaciones latinoamericanas de mujeres matemáticas.   Cabe citar a Roswitha, monja de un convento sajón del siglo X, de mejor y mayor trabajo en literatura y filosofía que en la ciencia de los números. No obstante lució buen conocimiento de la Aritmética de Boecio y menciona cuestiones ligadas a números defectivos y perfectos, señalando entre ellos a 6, 28, 496, y 8128.[15]  En una publicación sobre matemática recreativa de Rodríguez Vidal y Rodríguez Rigual, también figuran los nombres que siguen, como cultoras de matemática.  Premios y distinciones Nota: No existe premio Nobel de Matemáticas; el Premio Abel o la Medalla Fields se consideran por lo general su equivalente.  Premios internacionales de Matemáticas -- Estructura del informe --  Se puede elegir el orden de la lista, pulsando en el Premio, Sede, etc.    "
matematicas,"La aritmética (del lat. arithmetĭcus, derivado del gr. ἀριθμητικός,[1] a partir de ἀριθμός, «número») es la rama de la matemática cuyo objeto de estudio son los números y las operaciones elementales hechas con ellos: adición, sustracción, multiplicación y división.  Al igual que en otras áreas de la Matemática, como el Álgebra o la Geometría, el sentido de la «Aritmética» ha ido evolucionando con el amplio y diversificado desarrollo de las ciencias. Originalmente, la Aritmética se desarrolló de manera formal en la Antigua Grecia, con el refinamiento del rigor matemático y las demostraciones, y su extensión a las distintas disciplinas de las «Ciencias Naturales».[2] En la actualidad, puede referirse a la Aritmética Elemental, enfocada a la enseñanza de la Matemática Básica; también al conjunto que reúne el Cálculo Aritmético y las Operaciones Matemáticas, específicamente, las cuatro Operaciones Básicas aplicadas, ya sea a números (números naturales, números enteros, números fraccionarios y números decimales, etc.) como a entidades matemáticas más abstractas (matrices, operadores, etc.); también a la así llamada alta aritmética,[3] conocida como Teoría de Números  Operaciones aritméticas Las cuatro operaciones básicas (o elementales) de la aritmética son:  En el sentido de la definición propuesta, el sustantivo «aritmética», en los primeros grados de enseñanza escolar, suele designarse simplemente como «matemática». La distinción comienza a precisarse con la introducción del álgebra y la consiguiente implementación de ""letras"" para representar ""variables"" e ""incógnitas"", así como las definiciones de las propiedades algebraicas tales como conmutatividad, asociatividad o distributividad, que son propias del Álgebra Elemental.[4]  De manera más general, el cómputo numérico incluye, además de las operaciones básicas: el cálculo de congruencias, la factorización, el cálculo de potencias y la extracción de raíces.[5] En este sentido, el término aritmética se aplica para designar operaciones realizadas sobre entidades que no son números enteros solamente, sino que pueden ser decimales, racionales, reales, etc., o incluso objetos matemáticos con características completamente diferentes. El término «aritmética» es utilizado también como adjetivo, como por ejemplo en una progresión aritmética.  La aritmética sirvió de base para los sistemas de potencias. Se llama potencia a una expresión de la forma a^n, donde “a” es la base y “n” es el exponente. Su definición varía según el conjunto numérico al que pertenezca el exponente. Es una manera muy útil de expresar número en grandes cantidades de una manera más práctica y simplificada.  También de la Aritmética surgieron más símbolos y expresiones a fin de simplificar números, las más conocidas son las raíces cúbicas y cuadradas, las cuales les dan a un número una versión simplificada del mismo, son ideales para expresar números complicados de leer, al resolver problemas matemáticos.  Las fracciones y los porcentajes son también raíces surgidas directamente de los primeros símbolos aritméticos.  Instrumentos de cálculo Los utensilios para facilitar las cuentas numéricas y el conteo han sido utilizados a través de miles de años, por ejemplo contar con los dedos, estableciendo una correspondencia uno a uno con los dedos de la mano. El primer objeto para contar fue probablemente un «palo de conteo». Registros posteriores, a lo largo del Creciente Fértil incluyen cálculos (esferas de barro, conos, etc.) que representan cuentas de objetos, posiblemente granos.[6] La numeración con varillas es otro ejemplo.  Cálculo mental  Contar con los dedos  Palos de conteo  Numeración china con varillas  Numeración maya  Tablilla babilónica  Ábaco inca  Regla de cálculo  Ábaco  Máquina de sumar  Calculadora de bolsillo  Historia Origen Los orígenes de la aritmética se pueden rastrear hasta los comienzos de la matemática misma, y de la ciencia en general. Los registros más antiguos datan de la Edad de Piedra: huesos, palos, piedras talladas y escarbadas con muescas, presumiblemente con fines de conteo, de representación numérica y calendarios.  Edad Antigua Hay evidencias de que los babilonios tenían sólidos conocimientos de casi todos los aspectos de la aritmética elemental hacia 1800 a. C., gracias a transcripciones de caracteres cuneiformes sobre tablillas de barro cocido, referidas a problemas de geometría y astronomía. Solo se puede especular sobre los métodos utilizados para generar los resultados aritméticos, tal y como se muestra, por ejemplo, en la tablilla de arcilla Plimpton 322, que parece ser una lista de ternas pitagóricas, pero sin mostrar cómo se generó la lista.  Los antiguos textos Shulba-sutras (datados ca. 800 a. C. y 200 a. C.) recopilan los conocimientos matemáticos de la India durante el período védico; constan de datos geométricos relacionados con la construcción de altares de fuego, e incluyen el problema de la cuadratura del círculo.  Otras civilizaciones mesopotámicas, como sirios y fenicios, alcanzaron grados de desarrollo matemático similar y lo utilizaron tanto para el comercio como para la resolución de ecuaciones algebraicas.  El sistema de numeración egipcio, basado en fracciones unitarias, permitía efectuar cuentas aritméticas avanzadas, como se muestra en papiros conservados como el Papiro de Moscú o el Papiro de Ahmes (que data de ca. 1650 a. C., aunque es una copia de un antiguo texto de ca. 1850 a. C.) que muestra sumas, restas, multiplicaciones y divisiones, utilizando un sistema de fracciones, así como los problemas de determinar el volumen de una esfera o el volumen de una pírámide truncada. El papiro de Ahmes es el primer texto egipcio que menciona los 365 días del calendario egipcio, además de ser el primer calendario solar conocido.  Aritmética formal en la Antigua Grecia La aritmética en la Grecia Antigua era considerada como el estudio de las propiedades de los números, y no incluía cálculos prácticos; los métodos operatorios eran considerados una ciencia aparte. Esta particularidad fue heredada a los europeos durante la Edad Media, y no fue hasta el Renacimiento que la teoría de números y los métodos de cálculo comenzaron a considerarse «aritméticos».  La matemática griega hace una aguda diferencia entre el concepto de número y el de magnitud o conmensurabilidad. Para los antiguos griegos, número significaba lo que hoy se conoce por número natural, además de diferenciar entre «número» y «magnitud geométrica». Los libros 7–9 de Los elementos de Euclides tratan de la aritmética exclusivamente en este sentido.  Nicómaco de Gerasa (ca. 60 - 120 d. C.), en su Introducción a la Aritmética, resume la filosofía de Pitágoras y de Platón enfocada a los números y sus relaciones fundamentales. Nicómaco hace por primera vez la diferencia explícita entre Música, Astronomía, Geometría y Aritmética, y le da a esta última un sentido más «moderno», es decir, referido a los números enteros y sus propiedades fundamentales.[7] El quadrivium (lat. ""cuatro caminos"") agrupaba estas cuatro disciplinas científicas relacionadas con la matemática proveniente de la escuela pitagórica.  Diofanto de Alejandría (siglo III d. C.), es el autor de Arithmetica, una serie de libros sobre ecuaciones algebraicas, donde por primera vez se reconoce a las fracciones como números y se utilizan símbolos y variables como parte de la notación matemática; redescubierto por Pierre de Fermat en el siglo XVII. Las hoy llamadas ecuaciones diofánticas condujeron a un gran avance en la teoría de números.  Edad Media y Renacimiento europeo El mayor progreso matemático de los griegos se dio entre los años 300 a. C. y el 200 d. C. Después de esto, los avances continuaron en regiones islámicas. La matemática floreció en particular en Irán, Siria e India. Si bien los descubrimientos no fueron tan sustanciales como los llevados a cabo por la ciencia griega, sí contribuyeron en gran medida a preservar sus obras originales. A partir del siglo XI, Adelardo de Bath y más adelante Fibonacci, introducen nuevamente en Europa esta matemática islámica y sus traducciones del griego.[8]  De las siete artes liberales en que se organizaban los estudios formales en la Antigüedad y la Edad Media, la aritmética era parte de las enseñanzas escolásticas y universitarias.[9] En 1202, Fibonacci, en su tratado Liber Abaci, introduce el sistema de numeración decimal con números arábigos. Las operaciones aritméticas, aún las más básicas, realizadas hasta entonces con numerales romanos resultaban muy complicadas; la importancia práctica en contabilidad hizo que las nuevas técnicas aritméticas se popularizaran enseguida en Europa. Fibonacci llegó a escribir que «comparado con este nuevo método, todos los demás habían sido erróneos».  Civilizaciones precolombinas Al igual que otras civilizaciones mesoamericanas, los mayas utilizaban un sistema de numeración de base vigesimal (base aritmética 20) para medir el tiempo y participar del comercio a larga distancia. Los mayas preclásicos desarrollaron independientemente el concepto del cero alrededor del año 36 a. C.[10] Aunque poseían sistema de numeración, la ciencia maya y azteca estaba más enfocada en predecir el paso del tiempo, elaborar calendarios y pronosticar eventos astronómicos. Las culturas andinas, que no poseían sistema de escritura, sí parecen haber desarrollado más el cálculo aritmético. Algunas inscripciones fijan con gran precisión el año solar real en 365 días. Fueron las primeras civilizaciones en inventar el cero, aunque con algunas peculiaridades que le privaron de posibilidad operatoria.[11]  Los incas se destacaron principalmente por su capacidad de cálculo para fines económicos y comerciales. Los quipus y yupanas fueron señal de la importancia que tuvo la administración incaica. Esto dotó a los incas de una aritmética sencilla pero efectiva para fines contables; basada en un sistema decimal, conocieron el cero y dominaron la suma, la resta, la multiplicación y la división.  Aritmética en China La matemática china temprana es tan diferente a la de otras partes del mundo, que es razonable suponer que se desarrolló independientemente. El texto de matemáticas más antiguo que se conserva es el Chou Pei Suan Ching (literalmente: La Aritmética Clásica del Gnomon y los Senderos Circulares del Cielo), datado del 300 a. C.[12]  De particular notoriedad es el uso de un sistema decimal posicional, la así llamada numeración con varillas, utilizada muchos siglos antes del sistema indoarábigo de numeración.[12] El sistema de numeración con varillas permitía representar cantidades arbitrariamente grandes, y facilitaba el cálculo matemático con suanpan (o ábaco chino). La fecha de invención del ""suan pan"" es incierta, pero los registros escritos más antiguos que lo mencionan datan del año 190 a. C., en las «Notas Suplementarias en el arte de las Figuras», de Xu Yue.  Los nueve capítulos sobre el arte matemático, contiene problemas de agricultura, comercio, geometría e ingeniería, así como trabajos con triángulos rectángulos y aproximaciones al número π. El matemático chino Zu Chongzhi calculó el valor de π hasta siete decimales.[12]  Aritmética en la India: el cero y la notación posicional La matemática hindú alcanzó su madurez durante los siglos I al VIII, con el invento trascendental de la notación posicional, empleando la cifra cero como valor nulo. Utilizaron, como en Occidente, un sistema de numeración de base 10 (con diez dígitos). Egipcios, griegos y romanos, aunque utilizaban un sistema decimal, este no era posicional, ni poseía el cero, el cual fue transmitido a occidente mucho más tarde por los árabes, que le llamaban hesab, a través de la España e Italia medievales.  El sistema de numeración decimal aparece ya en el Süryasiddhanta, pequeño tratado que data probablemente del siglo VI. Los trabajos matemáticos de los hindúes se incorporaron en general a las obras astronómicas. Este es el caso de Aryabhata, nacido hacia 476, y de Brahmagupta, nacido hacia 598. Hacia 1150, Bhaskara escribió un tratado de aritmética en el que exponía el procedimiento del cálculo de raíces cuadradas. Se trata de una teoría de las ecuaciones de primer y segundo grado, no en forma geométrica, como lo hacían los griegos, sino en una forma que se puede llamar algebraica.  En el siglo VII, el obispo sirio Severo Sebhokt menciona este método con admiración, indicando no obstante que el método indio iba más allá de esa descripción. Las múltiples ventajas prácticas y teóricas del sistema de «notación posicional con cero» dieron el impulso definitivo a todo el desarrollo ulterior de la matemática. Los modernos algoritmos de cálculo fueron posibles gracias a la introducción de los números árabes y la notación decimal posicional.  "
matematicas,"Una ecuación es una igualdad matemática entre dos expresiones, denominadas miembros y separadas por el signo igual, en las que aparecen elementos conocidos y datos desconocidos o incógnitas, relacionados mediante operaciones matemáticas. Los valores conocidos pueden ser números, coeficientes o constantes, también variables o incluso objetos complejos como funciones o vectores; los elementos desconocidos pueden ser establecidos mediante otras ecuaciones de un sistema o algún otro procedimiento de resolución de ecuaciones.[nota 1]  Las incógnitas, representadas generalmente por letras, constituyen los valores que se pretende hallar (en ecuaciones complejas en lugar de valores numéricos podría tratarse de elementos de un cierto conjunto abstracto, como sucede en las ecuaciones diferenciales). Por ejemplo, en la ecuación algebraica siguiente:          3 x − 1  ⏞    primer miembro   =     9 + x  ⏞    segundo miembro     {\displaystyle \overbrace {3x-1} ^{\text{primer miembro}}=\overbrace {9+x} ^{\text{segundo miembro}}}    la variable     x   {\displaystyle x}   representa la incógnita, mientras que el coeficiente 3 y los números 1 y 9 son constantes conocidas. La igualdad planteada por una ecuación será cierta o falsa dependiendo de los valores numéricos que tomen las incógnitas; se puede afirmar entonces que una ecuación es una igualdad condicional, en la que solo ciertos valores de las variables (incógnitas) la hacen cierta.  Se llama solución de una ecuación a cualquier valor individual de dichas variables que la satisface. Para el caso dado, la solución es:      x = 5   {\displaystyle x=5}    En el caso de que todo valor posible de la incógnita haga cumplir la igualdad, la expresión se llama identidad. Si en lugar de una igualdad se trata de una desigualdad entre dos expresiones matemáticas, se denominará inecuación.  El símbolo «=», que aparece en cada ecuación, fue inventado en 1557 por Robert Recorde, quien consideró que no había nada más igual que dos líneas rectas paralelas de la misma longitud.[1]  Una ecuación se escribe como dos expresiones, conectadas por un signo igual (""="").[2][3][4] Las expresiones en los dos lados del signo igual se denominan ""lado izquierdo"" y ""lado derecho"" de la ecuación. Muy a menudo se supone que el lado derecho de una ecuación es cero. Suponiendo que esto no reduce la generalidad, ya que esto se puede realizar restando el lado derecho de ambos lados.  El tipo más común de ecuación es una ecuación polinomial (comúnmente llamada también ecuación algebraica ) en la que los dos lados son polinomios. Los lados de una ecuación polinomial contienen uno o más términos . Por ejemplo, la ecuación  tiene el lado izquierdo     A  x  2   + B x + C − y   {\displaystyle Ax^{2}+Bx+C-y}  , que tiene cuatro términos, y el lado derecho     0   {\displaystyle 0}  , que consta de un solo término. Los nombres de las variables sugieren que x ∧ y son incógnitas, y que A, B, y C son parámetros, pero esto es normalmente fijado por el contexto (en algunos contextos, y puede ser un parámetro, o A, B, y C pueden ser variables ordinarias).  Una ecuación es análoga a una balanza en la que se colocan pesos. Cuando se colocan pesos iguales de algo (por ejemplo, grano) en los dos platillos, los dos pesos hacen que la balanza esté en equilibrio y se dice que son iguales. Si se retira una cantidad de grano de uno de los platillos de la balanza, debe retirarse una cantidad igual de grano del otro platillo para que la balanza siga en equilibrio. Más generalmente, una ecuación permanece en equilibrio si se realiza la misma operación en sus dos lados.  En geometría cartesiana las ecuaciones se utilizan para describir figuras geométricas. Puesto que las ecuaciones que se plantean, como las ecuaciones implícitas o las ecuaciones paramétricas, tienen infinitas soluciones, el objetivo es ahora diferente: en lugar de dar las soluciones explícitamente o contarlas, lo que es imposible, se utilizan las ecuaciones para estudiar las propiedades de las figuras. Esta es la idea de partida de la geometría algebraica, una importante área de las matemáticas.  El Álgebra estudia dos grandes familias de ecuaciones: ecuaciones polinómicas y, entre ellas, el caso especial de las ecuaciones lineales. Cuando hay una sola variable, las ecuaciones polinómicas tienen la forma P(x) = 0, donde P es un polinomio, y las ecuaciones lineales tienen la forma ax + b = 0, donde a y b son parámetros. Para resolver ecuaciones de cualquiera de las dos familias, se utilizan técnicas algorítmicas o geométricas que provienen del álgebra lineal o del análisis matemático. El álgebra también estudia las ecuaciones diofantinas en las que los coeficientes y las soluciones son números enteros. Las técnicas utilizadas son diferentes y provienen de la teoría de números. Estas ecuaciones son difíciles en general; a menudo se busca sólo encontrar la existencia o ausencia de una solución y, si existe una o varias, hallar el número de soluciones.  Las ecuaciones diferenciales son ecuaciones que involucran una o más funciones y sus derivadas. Se resuelven encontrando una expresión para la función que no implique derivadas. Las ecuaciones diferenciales se utilizan para modelar procesos que implican las tasas de cambio de la variable,y se utilizan en áreas como la física, la química, la biología y la economía.  Introducción Ilustraciónes análogas Cada lado de la ecuación corresponde a un lado de una balanza. En cada lado se pueden colocar cantidades diferentes: si los pesos de los dos lados son iguales, la balanza se equilibra, y por analogía, la igualdad que representa la balanza también se equilibra (si no, la falta de equilibrio corresponde a una desigualdad representada por una inecuación).  En la ilustración, x, y y z son cantidades diferentes (en este caso números reales) representadas como pesos circulares, y cada una de x, y y z tiene un peso diferente. La suma corresponde a añadir peso, mientras que la resta corresponde a quitar peso del que ya hay. Cuando la igualdad se mantiene, el peso total de cada lado es el mismo.  Parámetros e incógnitas Las ecuaciones a menudo contienen términos distintos de las incógnitas. Estos otros términos, que se suponen conocidos, suelen llamarse constantes, coeficientes o parámetros.  Un ejemplo de una ecuación que implica x e y como incógnitas y el parámetro R es  Cuando se elige que R tenga el valor de 2 (R = 2), esta ecuación se reconocería en coordenadas cartesianas como la ecuación del círculo de radio 2 alrededor del origen. Por lo tanto, la ecuación con R sin especificar es la ecuación general del círculo.  Normalmente, las incógnitas se denotan con letras del final del alfabeto, x, y, z, w, ...,[2] mientras que los coeficientes (parámetros) se denotan con letras del principio, a, b, c, d, ... . Por ejemplo, la ecuación cuadrática general se suele escribir ax2 + bx + c = 0.  El proceso de encontrar las soluciones o, en el caso de los parámetros, de expresar las incógnitas en términos de los parámetros, se llama resolución de la ecuación. Tales expresiones de las soluciones en términos de los parámetros también se llaman soluciones.  Un sistema de ecuaciones es un conjunto de ecuaciones simultáneas, normalmente en varias incógnitas, para las que se buscan las soluciones comunes. Así, una solución del sistema es un conjunto de valores para cada una de las incógnitas que juntos forman una solución para cada ecuación del sistema. Por ejemplo, el sistema  tiene como única solución x = -1, y = 1.  Uso de ecuaciones La ciencia utiliza ecuaciones para enunciar leyes de forma precisa; estas ecuaciones expresan relaciones entre variables. Así, en física, la ecuación de la dinámica de Newton relaciona las variables fuerza F, aceleración a y masa m: F = ma. Los valores que son solución de la ecuación anterior cumplen la primera ley de la mecánica de Newton. Por ejemplo, si se considera una masa m = 1 kg y una aceleración a = 1 m/s^2, la única solución de la ecuación es F = 1 kg·m/s^2 = 1 newton, que es el único valor para la fuerza permitida por esta ley.  Ejemplos:  El campo de aplicación de las ecuaciones es inmenso, y por ello hay una gran cantidad de investigadores dedicados a su estudio.  Según autores como Ian Stewart, ""el poder de las ecuaciones (...) recae en la correspondencia filosóficamente difícil entre las matemáticas —una creación colectiva de mentes humanas— y una realidad física externa.""[5]  Identidades Una identidad es una expresión matemática que es verdadera para todos los valores posibles de la(s) variable(s) que contiene. Se conocen muchas identidades en álgebra y cálculo. En el proceso de resolver una ecuación, una identidad se utiliza a menudo para simplificar una ecuación, haciéndola más fácil de resolver.  En álgebra, un ejemplo de identidad es la diferencia de dos cuadrados:  que es verdadera para todas las x e y.  La Trigonometría es un área donde existen muchas identidades; éstas son útiles para manipular o resolver ecuaciones trigonométricas. Dos de las muchas que involucran las funciones seno y coseno son:  y  que son ambas verdaderas para todos los valores de θ.  Por ejemplo, para resolver el valor de θ que satisface la ecuación:  donde θ se limita a entre 0 y 45 grados, se puede utilizar la identidad anterior para el producto para dar:  dando la siguiente solución para 'θ:  Como la función seno es una función periódica, hay infinitas soluciones si no hay restricciones en θ. En este ejemplo, restringir θ para que esté entre 0 y 45 grados restringiría la solución a un solo número.  Historia Antigüedad Ya en el siglo XVI a. C., los egipcios resolvían problemas cotidianos que tenían que ver con la repartición de víveres, de cosechas y de materiales que equivalían a resolver ecuaciones algebraicas simples de primer grado; como la notación algebraica no existía, usaban un método iterativo aproximado, llamado «método de la falsa posición».  Los matemáticos chinos de principios de nuestra era escribieron el libro Los nueve capítulos sobre el arte matemático, en el que plantearon diversos métodos para resolver ecuaciones algebraicas de primero y segundo grado, así como sistemas de dos ecuaciones con dos incógnitas.  El matemático griego Diofanto de Alejandría publicó su Arithmetica en el siglo III tratando las ecuaciones de primer y segundo grado; fue uno de los primeros en utilizar símbolos para representar las ecuaciones. También planteó las ecuaciones con soluciones enteras, llamadas en su honor ecuaciones diofánticas.[6]  Siglos XV-XVI En la Edad Moderna, el estudio de las ecuaciones algebraicas experimenta un gran impulso. En el siglo XV estaban a la orden del día los desafíos matemáticos públicos, con premios al vencedor; así, un desafío famoso enfrentó a dos matemáticos a resolver ecuaciones de tercer grado, el vencedor fue Niccolò Fontana Tartaglia, experto algebrista.  Hacia mediados del siglo XVI los matemáticos italianos Girolamo Cardano y Rafael Bombelli descubrieron que para poder resolver todas las ecuaciones de segundo, tercero y cuarto grado, el uso de los números imaginarios era indispensable. Cardano, enemigo acérrimo de Tartaglia, también halló métodos de resolución de ecuaciones de cuarto grado.  En el mismo siglo, el matemático francés René Descartes popularizó la notación algebraica moderna, en la cual las constantes están representadas por las primeras letras del alfabeto, a, b, c, … y las variables o incógnitas por las últimas, x, y, z.  En esta época se enuncian problemas de ecuaciones que solo han sido resueltos actualmente, algunos recientemente; entre ellos el último teorema de Fermat, uno de los teoremas más famosos de la matemática, que no fue demostrado hasta 1995 por Andrew Wiles y Richard Taylor.  Siglos XVII-XVIII En el siglo XVII, Isaac Newton y Gottfried Leibniz publicaron los primeros métodos de resolución de las ecuaciones diferenciales que aparecen en los problemas de la dinámica. Probablemente el primer libro sobre estas ecuaciones fue Sobre las construcciones de ecuaciones diferenciales de primer grado, de Gabriele Manfredi (1707). Durante el siglo XVIII, matemáticos ilustres como Leonhard Euler, Daniel Bernoulli, Joseph-Louis Lagrange y Pierre Simon Laplace publicaron resultados sobre ecuaciones diferenciales ordinarias y ecuaciones en derivadas parciales"
matematicas,"Un número es un concepto abstracto que se emplea para contar (cantidades), medir (magnitudes) y etiquetar. Los números más sencillos, que utilizamos todos en la vida cotidiana, son los números naturales: 1, 2, 3, etc. Se denotan mediante      N    {\displaystyle \mathbb {N} }   y sirven también como ordinales, para establecer un orden (primero, segundo,...). En ocasiones usamos el término número para hablar de lo que en realidad es un numeral o cifra (por ejemplo, nuestros números arábigos). Desde un punto de vista totalmente general un número es cualquier elemento de una estructura lógico-matemática conocida como sistema numérico.  Los números desempeñan un papel fundamental en las ciencias empíricas; no solo los naturales, sino muchos otros tipos de números que contemplan las matemáticas. El conjunto de números enteros (representados por      Z    {\displaystyle \mathbb {Z} }  ) es una ampliación de los naturales, incluyendo los negativos (que utilizamos para representar deudas, y en los termómetros para las temperaturas bajo cero). Si incluimos los números fraccionarios (1/3, 0,75, -3,25, etc.) se obtiene el conjunto de los números racionales, cuyo símbolo es      Q    {\displaystyle \mathbb {Q} }  . Ya en la antigüedad se descubrió que existen números no racionales: la diagonal de un cuadrado de lado 1 mide raíz de dos, un número que no puede representarse como número entero ni como fracción; es irracional. Los racionales junto con los irracionales forman el conjunto de los números reales, (ℝ). Posteriormente, se han ido agregando otros tipos de números: imaginarios, trascendentes, irreales, complejos,...  Nótese que la teoría de números es una rama de las matemáticas que se ocupa de los enteros (no de números en general).  Tipos de números Los números más conocidos son los números naturales. Denotados mediante      N    {\displaystyle \mathbb {N} }  , son conceptualmente los más simples y los que se usan para contar unidades discretas. Estos, conjuntamente con los números «negativos», conforman el conjunto de los enteros, denotados mediante      Z    {\displaystyle \mathbb {Z} }   (del alemán Zahlen, ‘números’). Los números naturales negativos permiten representar formalmente deudas, y generalizar la resta de cualesquiera dos números naturales. Es decir, ya tenemos solución para operaciones como, por ejemplo, 3-2 = 1.  Otro tipo de números ampliamente usados son números fraccionarios, los cuales representan tanto cantidades inferiores a una unidad, como números mixtos (un conjunto de unidades más una parte inferior a la unidad). Los números fraccionarios pueden ser expresados siempre como cocientes de enteros. El conjunto de todos los números fraccionarios es el conjunto de los números racionales (que usualmente se define para que incluya tanto a los racionales positivos, como a los racionales negativos y el cero). Este conjunto de números se designa como      Q    {\displaystyle \mathbb {Q} }  . Al igual que con los números enteros se puede calcular el resultado de cualquier resta, con los racionales es posible efectuar divisiones que no tienen resultado entero, como 15/2 = 7,5 o 7½.  Los números racionales permiten resolver gran cantidad de problemas prácticos, pero desde los antiguos griegos se conoce que ciertas relaciones geométricas (la diagonal de un cuadrado de lado unidad) son números no enteros que tampoco son racionales. Igualmente, la solución numérica de una ecuación polinómica cuyos coeficientes son números racionales, usualmente es un número no racional. Puede demostrarse que cualquier número irracional puede representarse como una sucesión de Cauchy de números racionales que se aproximan a un límite numérico. El conjunto de todos los números racionales y los irracionales (obtenidos como límites de sucesiones de Cauchy de números racionales) es el conjunto de los números reales      R    {\displaystyle \mathbb {R} }  . Durante un tiempo se pensó que toda magnitud física existente podía ser expresada en términos de números reales exclusivamente. Entre los reales, existen números que no son soluciones de una ecuación polinomial o algebraica, y estos reciben el nombre de transcendentales. Los más conocidos de estos números son el número π (Pi) y el número e (este último base de los logaritmos naturales), los cuales están relacionados entre sí por la identidad de Euler.  Uno de los problemas de los números reales es que no forman un cuerpo algebraicamente cerrado, por lo que ciertos problemas no tienen solución planteados en términos de números reales. Esa es una de las razones por las cuales se introdujeron los números complejos      C    {\displaystyle \mathbb {C} }  , que son el mínimo cuerpo algebraicamente cerrado que contiene a los números reales. Además, en algunas aplicaciones prácticas, así como en las formulaciones estándar de la mecánica cuántica se considera útil introducir los números complejos. Al parecer, la estructura matemática de los números complejos refleja estructuras existentes en problemas físicos, por lo que en física teórica y en diversas aplicaciones los números complejos se usan en pie de igualdad con los números reales, a pesar de que inicialmente fueron considerados únicamente como un artificio matemático sin relación con la realidad física. Todos los conjuntos de números      N  ,  Z  ,  Q  ,  R  ,  C    {\displaystyle \mathbb {N} ,\mathbb {Z} ,\mathbb {Q} ,\mathbb {R} ,\mathbb {C} }   fueron de alguna manera «descubiertos» o sugeridos en conexión con problemas planteados en problemas físicos o en el seno de la matemática elemental y todos ellos parecen tener importantes conexiones con la realidad física.  Al margen de los números reales y complejos, claramente conectados con problemas de las ciencias naturales, existen otros tipos de números que generalizan aún más y extienden el concepto de número de una manera más abstracta y responden más a creaciones deliberadas de matemáticos. La mayoría de estas generalizaciones del concepto de número se usan solo en matemáticas, aunque algunos de ellos han encontrado aplicaciones para resolver ciertos problemas físicos. Entre ellos están los números hipercomplejos, que incluyen a los cuaterniones, útiles para representar rotaciones en un espacio de tres dimensiones, y generalizaciones de estos, como octoniones y los sedeniones.  A un nivel un poco más abstracto también se han ideado conjuntos de números capaces de tratar con cantidades infinitas e infinitesimales, como los hiperreales         ∗    R    {\displaystyle {}^{*}\mathbb {R} }   y los transfinitos. E igualmente los números racionales pueden extenderse de otras maneras, como por ejempo para formar los números p-ádicos, entre los cuales se encuentran los enteros p-ádicos       Z   p     {\displaystyle \mathbb {Z} _{p}}  , los racionales p-ádicos       Q   p     {\displaystyle \mathbb {Q} _{p}}   o los complejos p-ádicos     Ω =   C   p     {\displaystyle \Omega =\mathbb {C} _{p}}  , que satisfacen       Z   p   ⊂   Q   p   ⊂   C   p     {\displaystyle \mathbb {Z} _{p}\subset \mathbb {Q} _{p}\subset \mathbb {C} _{p}}  .  Lista de los tipos de números existentes La teoría de los números trata básicamente de las propiedades de los números naturales y los enteros, mientras que las operaciones del álgebra y el cálculo permiten definir la mayor parte de los sistemas numéricos, entre los cuales están:  Estructura de los sistemas numéricos En álgebra abstracta y análisis matemático un sistema numérico se caracteriza por una:   Otra propiedad interesante de muchos conjuntos numéricos es que son representables mediante diagramas de Hasse, diagramas de Euler y diagramas de Venn, pudiéndose tomar una combinación de ambos en un diagrama de Euler-Venn con la forma característica de cuadrilátero y además pudiéndose representar internamente un diagrama de Hasse (es una recta). Tanto históricamente como conceptualmente, los diversos conjuntos numéricos, desde el más simple de los números naturales, hasta extensiones transcendentes de los números reales y complejos, elaboradas mediante la teoría de modelos durante el siglo XX, se construyen desde una estructura más simple hasta otra más compleja.[2]  Números naturales especiales El estudio de ciertas propiedades que cumplen los números ha producido una enorme cantidad de tipos de números, la mayoría sin un interés matemático específico. Se pueden encuadrar dentro de la matemática recreativa. A continuación se indican algunos:  Una vez entendido el problema de la naturaleza y la clasificación de los números, surge otro, más práctico, pero que condiciona todo lo que se va a hacer con ellos: la manera de escribirlos. El sistema que se ha impuesto universalmente es la numeración posicional, gracias al invento del cero, con una base constante.   Más formalmente, en Los fundamentos de la aritmética, Gottlob Frege realiza una definición de «número», la cual fue tomada como referencia por muchos matemáticos (entre ellos Bertrand Russell, cocreador de Principia mathematica):  Véase también que Frege, tanto como cualquier otro matemático, se ve inhabilitado para definir al número como la expresión de una cantidad, porque la simbología matemática no hace referencia necesaria a la numerabilidad, y el hecho de «cantidad» referiría a algo numerable, mientras que números se adoptan para definir la cardinalidad de, por ejemplo, los elementos que se encuentran en el intervalo abierto (0, 1), que contiene innumerables elementos (el continuo).  Peano, antes de establecer sus cinco proposiciones sobre los números naturales, explícita que supone sabida una definición (quizás debido a su «obviedad») de las palabras o conceptos cero, sucesor y número. De esta manera postula:  Sin embargo, si uno define el concepto cero como el número 100, y el concepto número como los números mayores a 100, entonces las cinco proposiciones mencionadas anteriormente aplican, no a la idea que Peano habría querido comunicar, sino a su formalización.  La definición de número se encuentra por ende no totalmente formalizada, aunque se encuentre un acuerdo mayoritario en adoptar la definición enunciada por Frege.  Historia del concepto de número Cognitivamente el concepto de número está asociado a la habilidad de contar y comparar cuál de dos conjuntos de entidades similares tiene mayor cantidad de elementos. Las primeras sociedades humanas se encontraron muy pronto con el problema de determinar cuál de dos conjuntos era «mayor» que otro, o de conocer con precisión cuántos elementos formaban una colección de cosas. Esos problemas podían ser resueltos simplemente contando. La habilidad de contar del ser humano, no es un fenómeno simple, aunque la mayoría de culturas tienen sistemas de cuenta que llegan como mínimo a centenares, algunos pueblos con una cultura material simple, solo disponen de términos para los números 1, 2 y 3 y usualmente usan el término «muchos» para cantidades mayores, aunque cuando es necesario usan recursivamente expresiones traducibles como «3 más 3 y otros 3».  El conteo se debió iniciar mediante el uso de objetos físicos (tales como montones de piedras) y de marcas de cuenta, como las encontradas en huesos tallados: el de Lebombo, con 29 muescas grabadas en un hueso de babuino, tiene unos 37 000 años de antigüedad y otro hueso de lobo encontrado en la antigua Checoslovaquia, con 57 marcas dispuestas en cinco grupos de 11 y dos sueltas, se ha estimado en unos 30 000 años de antigüedad. Ambos casos constituyen una de las más antiguas marcas de cuenta conocidas habiéndose sugerido que pudieran estar relacionadas con registros de fases lunares.[3] En cuanto al origen ordinal algunas teorías lo sitúan en rituales religiosos. Los sistemas numerales de la mayoría de familias lingüísticas reflejan que la operación de contar estuvo asociado al conteo de dedos (razón por la cual los sistemas de base decimal y vigesimal son los más abundantes), aunque está testimoniado el empleo de otras bases numéricas.  El paso hacia los símbolos numerales, al igual que la escritura, se ha asociado a la aparición de sociedades complejas con instituciones centralizadas constituyendo artificios burocráticos de contabilidad en registros impositivos y de propiedades.  "
matematicas,"Número compuesto es cualquier número natural no  primo, a excepción del 1. Es decir, tiene uno o más divisores distintos a 1 y a sí mismo. También se utiliza el término divisible para referirse a estos números.  Los setenta y tres primeros números compuestos antes del cien son: 4, 6, 8, 9, 10, 12, 14, 15, 16, 18, 20, 21, 22, 24, 25, 26, 27, 28, 30, 32, 33, 34, 35, 36, 38, 39, 40, 42, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 58, 60, 62, 63, 64, 65, 66, 68, 69, 70, 72, 74, 75, 76, 77, 78, 80, 81, 82, 84, 85, 86, 87, 88, 90, 91, 92, 93, 94 , 95 ,96 98, 99.  Características Una característica es que cada uno puede escribirse como producto de dos números naturales menores que él. Así, el número 20 es compuesto porque puede expresarse como 4×5; y también el 87 ya que se expresa como 3×29. Sin embargo, no es posible hacer lo mismo con el 17 o el 23 porque son números primos. Cada número compuesto se puede expresar como multiplicación de dos (o más) números primos específicos, cuyo proceso se conoce como factorización. El número compuesto más pequeño es el 4.  La forma más sencilla para probar que un número n es compuesto, es encontrar un divisor d comprendido entre 1 y n (1 < d < n). Por ejemplo, 219 es compuesto porque tiene a 3 por divisor. Y también 371 porque tiene a 7 por divisor. Una buena alternativa es utilizar entonces el pequeño teorema de Fermat, o mejor la generalización de este teorema debida al matemático suizo Leonhard Euler.  Como los números primos y compuestos están entremezclados unos con otros es lógico preguntarse si existirán secuencias de números compuestos consecutivos de longitud arbitraria. La secuencia 32, 33, 34, 35 y 36 es un ejemplo de longitud 5, y 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125 y 126 un ejemplo de longitud 13. La respuesta es que podemos conseguir una secuencia de números compuestos tan larga como se desee. Si deseamos una secuencia de longitud 20, basta tomar los números 21!+2, 21!+3, 21!+4, ... , 21!+21, ya que el primero es divisible por 2, el segundo por 3, etcétera.   Un teorema de Fermat afirma que si p es primo de la forma 4n+1, entonces se da un caso de exclusión simple, que puede expresarse de forma única como suma de dos cuadrados. Si un número de la forma 4n+1 puede expresarse como suma de dos cuadrados de dos formas diferentes al menos, entonces el número es compuesto. Euler halló un método de factorización a partir de este hecho. Por ejemplo, si 221 = 112 + 102 = 142 + 52, entonces, 142 - 112 = 102 - 52. Tomando mcd(14+11, 10+5) = mcd(25,15) = 5, y después 25/5 = 5 y 15/5 = 3, y por último 52 + 32 = 25 + 9 = 34, entonces mcd(221, 34) = 17 nos da el factor que buscamos. El 1 y el 0 son casos especiales y no se consideran ni primos ni compuestos.  "
matematicas,"La geometría analítica es una rama de las matemáticas que estudia las figuras, sus distancias, sus áreas, puntos de intersección, ángulos de inclinación, puntos de división, volúmenes, etcétera. Analiza con detalle los datos de las figuras geométricas mediante técnicas básicas del análisis matemático y del álgebra en un determinado sistema de coordenadas. Su desarrollo histórico comienza con la geometría cartesiana, continúa con la aparición de la geometría diferencial de Carl Friedrich Gauss y más tarde con el desarrollo de la geometría algebraica. Tiene múltiples aplicaciones, más allá de las matemáticas y la ingeniería, pues forma parte ahora del trabajo de administradores para la planeación de estrategias y logística en la toma de decisiones.  Las dos cuestiones fundamentales de la geometría analítica son:  La geometría analítica representa las figuras geométricas mediante la ecuación     y = f ( x )   {\displaystyle y=f(x)}  , donde     f   {\displaystyle f}   es una función u otro tipo. Así, las rectas se expresan mediante la ecuación general     a x + b y = c   {\displaystyle ax+by=c}  , las circunferencias y el resto de cónicas como ecuaciones polinómicas de grado 2 (la circunferencia,      x  2   +  y  2   = 4   {\displaystyle x^{2}+y^{2}=4}  ; la hipérbola,     x y = 1   {\displaystyle xy=1}  ).  Historia Antigua Grecia El matemático griego Menecmo resolvió problemas y demostró teoremas utilizando un método que tenía un gran parecido con el uso de coordenadas y, en ocasiones, se ha sostenido que había introducido la geometría analítica.[1]  Apolonio de Perge, en Sobre la sección determinada, trató los problemas de una manera que puede llamarse geometría analítica de una dimensión; con la cuestión de encontrar puntos en una recta que estuvieran en proporción a los demás.[2] Apolonio en las Cónicas desarrolló además un método que es tan similar a la geometría analítica que a veces se piensa que su trabajo se anticipó al trabajo de Descartes por unos 1800 años. Su aplicación de líneas de referencia, un diámetro y una tangente no es esencialmente diferente de nuestro uso moderno de un marco de coordenadas, donde las distancias medidas a lo largo del diámetro desde el punto de tangencia son las abscisas, y los segmentos paralelos a la tangente e interceptados entre el eje y la curva son las ordenadas. Desarrolló además relaciones entre las abscisas y las ordenadas correspondientes que son equivalentes a ecuaciones retóricas (expresadas en palabras) de curvas. Sin embargo, aunque Apolonio estuvo cerca de desarrollar la geometría analítica, no lo logró ya que no tuvo en cuenta las magnitudes negativas y en todos los casos el sistema de coordenadas se superpuso a una curva dada a posteriori en lugar de a priori. Es decir, las ecuaciones estaban determinadas por curvas, pero las curvas no estaban determinadas por ecuaciones. Las coordenadas, las variables y las ecuaciones eran nociones subsidiarias aplicadas a una situación geométrica específica.[3]  Persia El matemático persa del siglo XI Omar Jayam vio una fuerte relación entre la geometría y el álgebra y se estaba moviendo en la dirección correcta cuando ayudó a cerrar la brecha entre el álgebra numérica[4] y geométrica con su solución geométrica de las ecuaciones cúbicas generales,[5] pero el paso decisivo vino después con Descartes. A Omar Jayam se le atribuye la identificación de los fundamentos de la geometría algebraica, y su libro Tratado sobre demostraciones de problemas de álgebra (1070), que estableció los principios de la geometría analítica, es parte del cuerpo de matemáticas persas que finalmente se transmitió a Europa.[6] Debido a su enfoque geométrico completo de las ecuaciones algebraicas, Jayampuede considerarse un precursor de Descartes en la invención de la geometría analítica.[7]: 248   Europa Occidental La geometría analítica fue inventada de forma independiente por René Descartes y Pierre de Fermat,[8][9] aunque a Descartes a veces se le da el crédito exclusivo.[10][11] La geometría cartesiana, el término alternativo utilizado para la geometría analítica, lleva el nombre de Descartes.  Descartes hizo un progreso significativo con los métodos en un ensayo titulado La Géométrie (La Geometría), uno de los tres ensayos adjuntos (apéndices) publicados en 1637 junto con su Discurso sobre el método para dirigir correctamente la razón y buscar la verdad en las ciencias, comúnmente denominado Discurso del método. La Géométrie, escrita en su lengua materna francesa, y sus principios filosóficos, sirvieron de base para el cálculo en Europa. Inicialmente, el trabajo no fue bien recibido debido, en parte, a las muchas lagunas en los argumentos y ecuaciones complicadas. Solo después de la traducción al latín y la adición de comentarios por Frans van Schooten en 1649 (y otros trabajos posteriores) hizo que la obra maestra de Descartes recibiera el debido reconocimiento.[12]  Pierre de Fermat también fue pionero en el desarrollo de la geometría analítica. Aunque no se publicó durante su vida, una forma manuscrita de Ad locos planos et solidos isagoge (Introducción a los lugares planos y sólidos) circulaba en París en 1637, justo antes de la publicación del Discurso de Descartes.[13][14][15] Claramente escrita y bien recibida, la Introducción También sentó las bases para la geometría analítica. La diferencia clave entre los tratamientos de Fermat y Descartes es una cuestión de punto de vista: Fermat siempre comenzaba con una ecuación algebraica y luego describía la curva geométrica que la satisfacía, mientras que Descartes comenzaba con curvas geométricas y producía sus ecuaciones como una de varias propiedades de las curvas.[12] Como consecuencia de este enfoque, Descartes tuvo que lidiar con ecuaciones más complicadas y tuvo que desarrollar los métodos para trabajar con ecuaciones polinómicas de mayor grado. Fue Leonhard Euler quien aplicó por primera vez el método de coordenadas en un estudio sistemático de superficies y curvas espaciales.  Construcciones fundamentales En un sistema de coordenadas cartesianas, un punto del plano queda determinado por dos números, llamados abscisa y ordenada del punto. Mediante ese procedimiento a todo punto del plano corresponden siempre dos números reales ordenados (abscisa y ordenada), y recíprocamente, a un par ordenado de números corresponde un único punto del plano. Consecuentemente el sistema cartesiano establece una correspondencia biunívoca entre un concepto geométrico como es el de los puntos del plano y un concepto algebraico como son los pares ordenados de números. Esta correspondencia constituye el fundamento de la geometría analítica.  Con la geometría analítica se puede determinar figuras geométricas planas por medio de ecuaciones e inecuaciones con dos incógnitas. Este es un método alternativo de resolución de problemas, o cuando menos nos proporciona un nuevo punto de vista con el cual poder atacar el problema.  Localización de un punto en el plano cartesiano En un plano (v.g. papel milimetrado) se traza dos rectas orientadas perpendiculares entre sí (ejes) —que por convenio se trazan de manera que una de ellas sea horizontal y la otra vertical—, y cada punto del plano queda unívocamente determinado por las distancias de dicho punto a cada uno de los ejes, siempre y cuando se dé también un criterio para determinar sobre qué semiplano determinado por cada una de las rectas hay que tomar esa distancia, criterio que viene dado por un signo. Ese par de números, las coordenadas, quedará representado por un par ordenado     ( x , y )   {\displaystyle (x,y)}  , siendo     x   {\displaystyle x}   la distancia a uno de los ejes (por convenio será la distancia al eje vertical) e     y   {\displaystyle y}   la distancia al otro eje (al horizontal).  En la coordenada     x   {\displaystyle x}  , el signo positivo (que suele omitirse) significa que la distancia se toma hacia la derecha sobre el eje horizontal (eje de las abscisas), y el signo negativo (nunca se omite) indica que la distancia se toma hacia la izquierda. Para la coordenada     y   {\displaystyle y}  , el signo positivo (también se omite) indica que la distancia se toma hacia arriba sobre el eje vertical (eje de ordenadas), tomándose hacia abajo si el signo es negativo (en ningún caso se omiten los signos negativos).  A la coordenada     x   {\displaystyle x}   se la suele denominar abscisa del punto, mientras que a la     y   {\displaystyle y}   se la denomina ordenada del punto.  Los puntos del eje de abscisas tienen por lo tanto ordenada igual a     0   {\displaystyle 0}  , así que serán de la forma     ( x , 0 )   {\displaystyle (x,0)}  , mientras que los del eje de ordenadas tendrán abscisa igual a     0   {\displaystyle 0}  , por lo que serán de la forma     ( 0 , y )   {\displaystyle (0,y)}  .  El punto donde ambos ejes se cruzan tendrá por lo tanto distancia     0   {\displaystyle 0}   a cada uno de los ejes, luego su abscisa será     0   {\displaystyle 0}   y su ordenada también será     0   {\displaystyle 0}  . A este punto —el     ( 0 , 0 )   {\displaystyle (0,0)}  — se le denomina origen de coordenadas.  Se consideran dos rectas orientadas, (ejes), perpendiculares entre sí, ""x"" e ""y"", con un origen común, el punto O de intersección de ambas rectas.  Teniendo un punto a, al cual se desea determinar las coordenadas, se procede de la siguiente forma:  Por el punto P se trazan rectas perpendiculares a los ejes, éstas determinan en la intersección con los mismos dos puntos, P' (el punto ubicado sobre el eje x) y el punto P'' ( el punto ubicado sobre el eje y).  Dichos puntos son las proyecciones ortogonales sobre los ejes x e y del punto P.  A los Puntos P' y P'' le corresponden por número la distancia desde ellos al origen, teniendo en cuenta que si el punto P' se encuentra a la izquierda de O, dicho número será negativo, y si el punto P'' se encuentra por debajo del punto O, dicho número será negativo.  Los números relacionados con P' y P'', en ese orden son los valores de las coordenadas del punto P.  Ejemplo 1: P' se encuentra a la derecha de O una distancia igual a 2 unidades. P'' se encuentra hacia arriba de O, una distancia igual a 3 unidades. Por lo que las coordenadas de P son (2, 3).  Ejemplo 2: P' se encuentra a la derecha de O una distancia igual a 4 unidades. P'' se encuentra hacia abajo de O, una distancia igual a 5 unidades. Por lo que las coordenadas de P son (4, -5).  Ejemplo 3: P' se encuentra a la izquierda de O una distancia igual a 3 unidades. P'' se encuentra hacia abajo de O, una distancia igual a 2 unidades. Por lo que las coordenadas de P son (-3, -2).  Ejemplo 4: P' se encuentra a la izquierda de O una distancia igual a 6 unidades. P'' se encuentra hacia arriba de O, una distancia igual a 4 unidades. Por lo que las coordenadas de P son (-6, 4).  Ecuaciones de la recta en el plano Una recta es el lugar geométrico de todos los puntos en el plano tales que, tomados dos cualesquiera de ellos, el cálculo de la pendiente resulta siempre igual a una constante.  La ecuación general de la recta es de la forma:      A x + B y + C = 0    {\displaystyle Ax+By+C=0\,}    cuya pendiente es m = -A/B y cuya ordenada al origen es b = -C/B.  Una recta en el plano se representa con la función lineal de la forma:      y = m x + b    {\displaystyle y=mx+b\,}    Como expresión general, ésta es conocida con el nombre de ecuación pendiente-ordenada al origen y podemos distinguir dos casos particulares. Si una recta no corta a uno de los ejes, será porque es paralela a él. Como los dos ejes son perpendiculares, si no corta a uno de ellos forzosamente ha de cortar al otro (siempre y cuando la función sea continua para todos los reales). Tenemos pues tres casos:  Secciones cónicas "
matematicas,"En matemática, una matriz es un conjunto bidimensional de números. Dado que puede definirse tanto la suma como el producto de matrices, en mayor generalidad se dice que son elementos de un anillo. Una matriz se representa por medio de una letra mayúscula (A,B, …) y sus elementos con la misma letra en minúscula (a,b, …), con un doble subíndice donde el primero indica la fila y el segundo la columna a la que pertenece.      A =   (     a  11      a  12     ⋯    a  1 n        a  21      a  22     ⋯    a  2 n       ⋮   ⋮   ⋱   ⋮      a  m 1      a  m 2     ⋯    a  m n      )     {\displaystyle A={\begin{pmatrix}a_{11}&a_{12}&\cdots &a_{1n}\\a_{21}&a_{22}&\cdots &a_{2n}\\\vdots &\vdots &\ddots &\vdots \\a_{m1}&a_{m2}&\cdots &a_{mn}\\\end{pmatrix}}}    Los elementos individuales de una matriz     m   {\displaystyle m}   x     n   {\displaystyle n}  , se denotan a menudo por      a  i j     {\displaystyle a_{ij}}  , donde el máximo valor de     i   {\displaystyle i}   es     m   {\displaystyle m}  , y el máximo valor de     j   {\displaystyle j}   es     n   {\displaystyle n}  . Siempre que la matriz tenga el mismo número de filas y de columnas que otra matriz, estas se pueden sumar o restar elemento por elemento.  Pueden sumarse, multiplicarse y descomponerse de varias formas, lo que también las hace un concepto clave en el campo del álgebra lineal.  Historia El origen de las matrices es muy antiguo. Los cuadrados latinos y los cuadrados mágicos se estudiaron desde hace mucho tiempo. Un cuadrado mágico, 3 por 3, se registra en la literatura china hacia el 650 a. C.[2]  Es larga la historia del uso de las matrices para resolver ecuaciones lineales. Un importante texto matemático chino que proviene del año 300 a. C. a 200 a. C., Nueve capítulos sobre el Arte de las matemáticas (Jiu Zhang Suan Shu), es el primer ejemplo conocido de uso del método de matrices para resolver un sistema de ecuaciones simultáneas.[3] En el capítulo séptimo, ""Ni mucho ni poco"", el concepto de determinante apareció por primera vez, dos mil años antes de su publicación por el matemático japonés Seki Kōwa en 1683 y el matemático alemán Gottfried Leibniz en 1693.  Los «cuadrados mágicos» eran conocidos por los matemáticos árabes, posiblemente desde comienzos del s. VII d. C., quienes a su vez pudieron tomarlos de los matemáticos y astrónomos de la India, junto con otros aspectos de la matemática combinatoria. Todo esto sugiere que la idea provino de China. Los primeros «cuadrados mágicos» de orden 5 y 6 aparecieron en Bagdad en el año 983, en la Enciclopedia de la Hermandad de Pureza (Rasa'il Ihkwan al-Safa).[2]  Después del desarrollo de la teoría de determinantes por Seki Kowa y Leibniz para facilitar la resolución de ecuaciones lineales, a finales del siglo XVII, Gabriel Cramer presentó en 1750 la ahora denominada regla de Cramer. Carl Friedrich Gauss y Wilhelm Jordan desarrollaron la eliminación de Gauss-Jordan en el siglo XIX  Fue James Joseph Sylvester quien utilizó por primera vez el término «matriz» en 1848/1850.  En 1853, William Rowan Hamilton hizo algunos aportes a la teoría de matrices. Cayley introdujo en 1858 la notación matricial, como forma abreviada de escribir un sistema de m ecuaciones lineales con n incógnitas.  Cayley, Hamilton, Hermann Grassmann, Ferdinand Georg Frobenius, Olga Taussky-Todd y John von Neumann cuentan entre los matemáticos famosos que trabajaron sobre la teoría de las matrices. En 1925, Werner Heisenberg redescubre el cálculo matricial fundando una primera formulación de lo que iba a pasar a ser la mecánica cuántica. Se le considera a este respecto como uno de los padres de la mecánica cuántica.  Olga Taussky-Todd (1906-1995), durante la II Guerra Mundial, usó la teoría de matrices para investigar el fenómeno de   Introducción Definición Una matriz es un conjunto p-dimensional de números (elementos de la matriz) ordenados en filas (o renglones) y columnas, donde una fila es cada una de las líneas horizontales de la matriz y una columna es cada una de las líneas verticales. A una matriz con     m   {\displaystyle m}   filas y     n   {\displaystyle n}   columnas se le denomina «matriz     m   {\displaystyle m}   por     n   {\displaystyle n}   » (escrito     m × n   {\displaystyle m\times n}  ) donde     m , n ∈  N  − { 0 }   {\displaystyle m,n\in \mathbb {N} -\{0\}}  . El conjunto de las matrices de tamaño     m × n   {\displaystyle m\times n}   se representa como        M    m × n   (  K  )   {\displaystyle {\mathcal {M}}_{m\times n}(\mathbb {K} )}  , donde      K    {\displaystyle \mathbb {K} }   es el cuerpo al cual pertenecen los elementos de la matriz. El tamaño de una matriz siempre se da con el número de filas primero y el número de columnas después.   Se dice que dos matrices son iguales si tienen el mismo tamaño (dimensión u orden)  y los mismos elementos en las mismas posiciones. El elemento de una matriz que se encuentra en la fila     i −     {\displaystyle i-\,\!}  ésima y la columna     j −     {\displaystyle j-\,\!}  ésima se le llama elemento     i , j     {\displaystyle i,j\,\!}   o elemento     ( i , j )     {\displaystyle (i,j)\,\!}  -ésimo de la matriz. En estas expresiones también se consideran primero las filas y después las columnas.  Dos matrices     A , B ∈    M    m × n   (  K  )   {\displaystyle A,B\in {\mathcal {M}}_{m\times n}(\mathbb {K} )}   son iguales si los elementos correspondientes son iguales, es decir,      a  i j   =  b  i j   , 1 ≤ i ≤ m , 1 ≤ j ≤ n   {\displaystyle a_{ij}=b_{ij},1\leq i\leq m,1\leq j\leq n}  .  Para definir el concepto de matriz, el término ""conjunto bidimensional"" es útil, aunque poco formal, pero puede formalizarse usando el concepto de función. De este modo, una matriz de m filas y n columnas con entradas en un cuerpo      K    {\displaystyle \mathbb {K} }   es una función cuyo dominio es el conjunto de los pares ordenados     ( i , j )     {\displaystyle (i,j)\,\!}  , donde     1 ≤ i ≤ m   {\displaystyle 1\leq i\leq m}   y     1 ≤ j ≤ n   {\displaystyle 1\leq j\leq n}  ,  y cuyo codominio es      K    {\displaystyle \mathbb {K} }  . Con esta definición, la entrada      i , j     {\displaystyle i,j\,\!}   es el valor de la función en el par ordenado      ( i , j )     {\displaystyle (i,j)\,\!}  .  Se denota a las matrices con letra mayúscula, mientras que se utiliza la correspondiente letra en minúsculas para denotar a las entradas de las mismas, con subíndices que refieren al número de fila y columna del elemento.[4] Por ejemplo, al elemento de una matriz     A   {\displaystyle A}   de tamaño     m × n   {\displaystyle m\times n}   que se encuentra en la fila     i −     {\displaystyle i-\,\!}  ésima y la columna     j −     {\displaystyle j-\,\!}  ésima se le denota como      a  i j       {\displaystyle a_{ij}\,\!}  , donde     1 ≤ i ≤ m   {\displaystyle 1\leq i\leq m}   y     1 ≤ j ≤ n   {\displaystyle 1\leq j\leq n}  .  Cuando se va a representar explícitamente una entrada la cual está indexada con un     i     {\displaystyle i\,\!}   o un     j     {\displaystyle j\,\!}   con dos cifras se introduce una coma entre el índice de filas y de columnas. Así por ejemplo, la entrada que está en la primera fila y la segunda columna de la matriz     A     {\displaystyle A\,\!}   de tamaño     50 × 100   {\displaystyle 50\times 100}   se representa como      a  1 , 2       {\displaystyle a_{1,2}\,\!}   mientras que la entrada que está en la fila número 23 y la columna 100 se representa como      a  23 , 100       {\displaystyle a_{23,100}\,\!}  .  Además de utilizar letras mayúsculas para representar matrices, numerosos autores representan a las matrices con fuentes en negrita para distinguirlas de otros objetos matemáticos.[cita requerida] Así      A    {\displaystyle \mathbf {A} }   es una matriz, mientras que     A     {\displaystyle A\,\!}   es un escalar en esa notación. Sin embargo esta notación generalmente se deja para libros y publicaciones, donde es posible hacer esta distinción tipográfica con facilidad. En otras notaciones se considera que el contexto es lo suficientemente claro como para no usar negritas.  Otra notación, en sí un abuso de notación, representa a la matriz por sus entradas, i.e.     A := (  a  i j   )     {\displaystyle A:=(a_{ij})\,\!}   o incluso     A :=  a  i j       {\displaystyle A:=a_{ij}\,\!}  .  Como caso particular de matriz, se definen los vectores fila y los vectores columna. Un vector fila o vector renglón es cualquier matriz de tamaño     "
matematicas,"En general el término cálculo (del latín calculus, piedrecita, usado para contar o como ayuda al calcular)[1] hace referencia al resultado correspondiente a la acción de calcular. Calcular, por su parte, consiste en realizar las operaciones necesarias para prever el resultado de una acción previamente concebida, o conocer las consecuencias que se pueden derivar de unos datos previamente conocidos.  No obstante, el uso más común del término «cálculo» es el lógico-matemático. Desde esta perspectiva, el cálculo consiste en un  procedimiento mecánico o algoritmo, mediante el cual podemos conocer las consecuencias que se derivan de las variables previamente conocidas debidamente formalizadas y simbolizadas.  Cálculo como razonamiento y cálculo lógico-matemático Las dos acepciones del cálculo (la general y la restringida) arriba definidas están íntimamente ligadas. El cálculo es una actividad natural y primordial en el hombre, que comienza en el mismo momento en que empieza a relacionar unas cosas con otras en un pensamiento o discurso. El cálculo lógico natural como razonamiento es el primer cálculo elemental del ser humano. El cálculo en sentido lógico-matemático aparece cuando se toma conciencia de esta capacidad de razonar y trata de formalizarse.  Por lo tanto, podemos distinguir dos tipos de operaciones:  Historia del cálculo De la Antigüedad El término «cálculo» procede del latín calculus, piedrecita que se mete en el calzado y que produce molestia. Precisamente, tales piedrecitas ensartadas en tiras constituían el ábaco romano que, junto con el suanpan chino, constituyen las primeras máquinas de calcular en el sentido de contar.  Los antecedentes de procedimiento de cálculo, como algoritmo, se encuentran en los que utilizaron los geómetras griegos, Eudoxo en particular, en el sentido de llegar por aproximación de restos cada vez más pequeños, a una medida de figuras curvas; así como Diofanto precursor del álgebra.  Se considera que Arquímedes fue uno de los matemáticos más grandes de la antigüedad y, en general, de toda la historia.[2][3] Usó el método exhaustivo para calcular el área bajo el arco de una parábola con el sumatorio de una serie infinita, y dio una aproximación extremadamente precisa del número Pi.[4] También definió la espiral que lleva su nombre, fórmulas para los volúmenes de las superficies de revolución y un ingenioso sistema para expresar números muy largos.  La consideración del cálculo como una forma de razonamiento abstracto aplicado en todos los ámbitos del conocimiento se debe a Aristóteles, quien en sus escritos lógicos fue el primero en formalizar y simbolizar los tipos de razonamientos categóricos (silogismos). Este trabajo sería completado más tarde por los estoicos, los megáricos, la Escolástica.  Los algoritmos actuales del cálculo aritmético, utilizados universalmente, son fruto de un largo proceso histórico. De vital importancia son las aportaciones de Muhammad ibn al-Juarismi en el siglo IX;[5]  En el siglo XIII, Fibonacci introduce en Europa la representación de los números arábigos del sistema decimal. Se introdujo el 0, ya de antiguo conocido en la India y se construye definitivamente el sistema decimal de diez cifras con valor posicional. La escritura antigua de números en Babilonia, en Egipto, en Grecia o en Roma, hacía muy difícil un procedimiento mecánico de cálculo.[6]  El sistema decimal fue muy importante para el desarrollo de la contabilidad de los comerciantes de la Baja Edad Media, en los inicios del capitalismo.  El concepto de función por tablas ya era practicado de antiguo pero adquirió especial importancia en la Universidad de Oxford en el siglo XIV.[7] La idea de un lenguaje o algoritmo capaz de determinar todas las verdades, incluidas las de la fe, aparecen en el intento de Raimundo Lulio en su Ars Magna  A fin de lograr una operatividad mecánica se confeccionaban unas tablas a partir de las cuales se podía generar un algoritmo prácticamente mecánico. Este sistema de tablas ha perdurado en algunas operaciones durante siglos, como las tablas de logaritmos, o las funciones trigonométricas; las tablas venían a ser como la calculadora de hoy día; un instrumento imprescindible de cálculo. Las amortizaciones de los créditos en los bancos, por ejemplo, se calculaban a partir de tablas elementales hasta que se produjo la aplicación de la informática en el tercer tercio del siglo XX.  A finales de la Edad Media la discusión entre los partidarios del ábaco y los partidarios del algoritmo se decantó claramente por estos últimos.[8] De especial importancia es la creación del sistema contable por partida doble recomendado por Luca Pacioli fundamental para el progreso del capitalismo en el Renacimiento.[9]  Renacimiento El sistema que usamos actualmente fue introducido por Luca Pacioli en 1494, el cual fue creado y desarrollado para responder a la necesidad de la contabilidad en los negocios de la burguesía renacentista.  El desarrollo del álgebra (con la introducción de un sistema de símbolos por un lado, y la resolución de problemas por medio de las ecuaciones) vino de la mano de los grandes matemáticos de la época renacentista como Tartaglia, Stevin, Cardano o Vieta y fue esencial para el planteamiento y solución de los más diversos problemas que surgieron en la época, que dieron como consecuencia los grandes descubrimientos que hicieron posible el progreso científico que surgiría en el siglo XVII.[10]  Siglos XVII y XVIII En el siglo XVII el cálculo conoció un enorme desarrollo siendo los autores más destacados Descartes,[11] Pascal[12] y, finalmente, Leibniz y Newton[13] con el cálculo infinitesimal que en muchas ocasiones ha recibido simplemente, por absorción, el nombre de cálculo.  El concepto de cálculo formal en el sentido de algoritmo reglado para el desarrollo de un razonamiento y su aplicación al mundo de lo real,[14] adquiere una importancia y desarrollo enorme respondiendo a una necesidad de establecer relaciones matemáticas entre diversas medidas, esencial para el progreso de la ciencia física que, debido a esto, es tomada como nuevo modelo de Ciencia frente a la especulación tradicional filosófica, por el rigor y seguridad que ofrece el cálculo matemático. Cambia así el sentido tradicional de la Física como filosofía de la naturaleza y toma el sentido de ciencia que estudia los cuerpos materiales, en cuanto materiales.  A partir de entonces el propio sistema de cálculo permite establecer modelos sobre la realidad física, cuya comprobación experimental[15] supone la confirmación de la teoría como sistema. Es el momento de la consolidación del llamado método científico cuyo mejor exponente es en aquel momento la Teoría de la Gravitación Universal y las leyes de la Mecánica de Newton.[16]  Siglos XIX y XX Durante el siglo XIX y XX el desarrollo científico y la creación de modelos teóricos fundados en sistemas de cálculo aplicables tanto en mecánica como en electromagnetismo y radioactividad, etc., así como en astronomía fue impresionante. Las geometrías no euclidianas encuentran aplicación en modelos teóricos de astronomía y física. El mundo deja de ser un conjunto de infinitas partículas que se mueven en un espacio-tiempo absoluto y se convierte en un espacio de configuración o espacio de fases de     n   {\displaystyle n}   dimensiones que físicamente se hacen consistentes en la teoría de la relatividad, la mecánica cuántica, la teoría de cuerdas, etc., que cambia por completo la imagen del mundo físico.  La lógica asimismo sufrió una transformación radical.[17] La formalización simbólica fue capaz de integrar las leyes lógicas en un cálculo matemático, hasta el punto que la distinción entre razonamiento lógico-formal y cálculo matemático viene a considerarse como meramente utilitaria.  En la segunda mitad del siglo XIX y primer tercio del XX, a partir del intento de formalización de todo el sistema matemático, Frege, y de matematización de la lógica, (Bolzano, Boole, Whitehead, Russell) fue posible la generalización del concepto como cálculo lógico. Se lograron métodos muy potentes de cálculo, sobre todo a partir de la posibilidad de tratar como «objeto» conjuntos de infinitos elementos, dando lugar a los números transfinitos de Cantor.  Mediante el cálculo la lógica encuentra nuevos desarrollos como lógicas modales y lógicas polivalentes.  Los intentos de axiomatizar el cálculo como cálculo perfecto por parte de Hilbert y Poincaré, llevaron, como consecuencia de diversas paradojas (Cantor, Russell, etc.) a nuevos intentos de axiomatización, Axiomas de Zermelo-Fraenkel y a la demostración de Gödel de la imposibilidad de un sistema de cálculo perfecto: consistente, decidible y completo en 1931, de grandes implicaciones lógicas, matemáticas y científicas.  Actualidad En la actualidad, el cálculo en su sentido más general, en tanto que cálculo lógico interpretado matemáticamente como sistema binario, y físicamente hecho material mediante la lógica de circuitos electrónicos, ha adquirido una dimensión y desarrollo impresionante por la potencia de cálculo conseguida por los ordenadores, propiamente máquinas computadoras. La capacidad y velocidad de cálculo de estas máquinas hace lo que humanamente sería imposible: millones de operaciones por segundo.  El cálculo así utilizado se convierte en un instrumento fundamental de la investigación científica por las posibilidades que ofrece para la modelización de las teorías científicas, adquiriendo especial relevancia en ello el cálculo numérico.  Cálculo infinitesimal: breve reseña El cálculo infinitesimal, llamado por brevedad «cálculo», tiene su origen en la antigua geometría griega. Demócrito calculó el volumen de pirámides y conos considerándolos formados por un número infinito de secciones de grosor infinitesimal (infinitamente pequeño). Eudoxo y Arquímedes utilizaron el «método de agotamiento» o exhaución para encontrar el área de un círculo con la exactitud finita requerida mediante el uso de polígonos regulares inscritos de cada vez mayor número de lados. En el periodo tardío de Grecia, el neoplatónico Pappus de Alejandría hizo contribuciones sobresalientes en este ámbito. Sin embargo, las dificultades para trabajar con números irracionales y las paradojas de Zenón de Elea impidieron formular una teoría sistemática del cálculo en el periodo antiguo.   En el siglo XVII, Cavalieri y Torricelli ampliaron el uso de los infinitesimales, Descartes y Fermat utilizaron el álgebra para encontrar el área y las tangentes (integración y derivación en términos modernos). Fermat e Isaac Barrow tenían la certeza de que ambos cálculos estaban relacionados, aunque fueron Newton (hacia 1660), en Inglaterra y Leibniz en Alemania (hacia 1670) quienes demostraron que los problemas del área y la tangente son inversos, lo que se conoce como teorema fundamental del cálculo. Leibniz es el creador del simbolismo de la derivada, diferencial y la ∫ estilizada para la integración, en vez de la I de Bernoulli. Usó el nombre de cálculo diferencial y el nombre de cálculo integral propuso Juan Bernoulli, que sustituyó al nombre de 'cálculo sumatorio' de Leibniz. La simbología de Leibniz impulsó el avance del cálculo en Europa continental.[18]  El descubrimiento de Newton, a partir de su teoría de la gravitación universal, fue anterior al de Leibniz, pero el retraso en su publicación aún provoca controversias sobre quién de los dos fue el primero. Newton utilizó el cálculo en mecánica en el marco de su tratado «Principios matemáticos de filosofía natural», obra científica por excelencia, llamando a su método de «fluxiones». Leibniz utilizó el cálculo en el problema de la tangente a una curva en un punto, como límite de aproximaciones sucesivas, dando un carácter más filosófico a su discurso. Sin embargo, terminó por adoptarse la notación de Leibniz por su versatilidad.   En el siglo XVIII aumentó considerablemente el número de aplicaciones del cálculo, pero el uso impreciso de las cantidades infinitas e infinitesimales, así como la intuición geométrica, causaban todavía confusión y duda sobre sus fundamentos. De hecho, la noción de límite, central en el estudio del cálculo, era aún vaga e imprecisa en ese entonces. Uno de sus críticos más notables fue el filósofo George Berkeley.   "
matematicas,"En matemática, se dice que una magnitud es función de otra si el valor de la primera depende del valor de la segunda. Por ejemplo, el área A de un círculo es función de su radio r (el valor del área es proporcional al cuadrado del radio, A = π·r2). Del mismo modo, la duración T de un viaje en tren entre dos ciudades separadas por una distancia d depende de la velocidad v a la que se desplace el tren (a saber, la duración es inversamente proporcional a la velocidad, T = d / v). A la primera magnitud (el área, la duración) se la denomina variable dependiente, y la magnitud de la que depende (el radio y la velocidad) es la variable independiente.  En análisis matemático, el concepto general de función,  se refiere a una regla que asigna a cada elemento de un primer conjunto un único elemento de un segundo conjunto. Las funciones son relaciones entre los elementos de dos conjuntos. Por ejemplo, cada número entero posee un único cuadrado, que resulta ser un número natural (incluyendo el cero):[1]  Esta asignación constituye una función entre el conjunto de los números enteros Z y el conjunto de los números naturales N. Aunque las funciones que manipulan números son las más conocidas, no son el único ejemplo: puede imaginarse una función que a cada palabra del español le asigne su letra inicial:  Esta es una función entre el conjunto de las palabras del español y el conjunto de las letras del alfabeto español.   La manera habitual de denotar una función f es:  donde A es el dominio de la función f; su primer conjunto, o conjunto de partida, y B es el codominio de f; su segundo conjunto, o conjunto de llegada. Por f(a) se denota la regla o algoritmo para obtener la imagen de un cierto objeto arbitrario a del dominio A, es decir, el (único) objeto de B que le corresponde. En ocasiones esta expresión es suficiente para especificar la función por completo, infiriendo el dominio y codominio por el contexto. En el ejemplo anterior, las funciones «cuadrado» e «inicial», llámeseles     f   {\displaystyle f}   y     g   {\displaystyle g}  , se denotarían entonces como:  si se conviene V = {Palabras del español} y A = {Alfabeto español}.   Una función puede representarse de diversas formas: mediante el citado algoritmo o ecuaciones para obtener la imagen de cada elemento, mediante una tabla de valores que empareje cada valor de la variable independiente con su imagen —como las mostradas arriba—, o como una gráfica que dé una imagen de la función.  Historia El concepto de función como un objeto matemático independiente, susceptible de ser estudiado por sí solo, no apareció hasta los inicios del cálculo en el siglo XVII.[2] René Descartes, Isaac Newton y Gottfried Leibniz establecieron la idea de función como dependencia entre dos cantidades variables. Leibniz en particular acuñó los términos «función», «variable», «constante» y «parámetro». La notación f(x) fue utilizada por primera vez por el francés Alexis Claude Clairaut, y por el suizo Leonhard Euler en su obra Commentarii de San petersburgo en 1736.[3][4][5]  Inicialmente, una función se identificaba, a efectos prácticos, con una expresión analítica que permitía calcular sus valores. Sin embargo, esta definición tenía algunas limitaciones: expresiones distintas pueden arrojar los mismos valores, y no todas las «dependencias» entre dos cantidades pueden expresarse de esta manera. En 1837, el matemático alemán Johann Peter Gustav Lejeune Dirichlet propuso la definición moderna de función numérica como una correspondencia cualquiera entre dos conjuntos de números, que asocia a cada número en el primer conjunto un único número del segundo.  La intuición sobre el concepto de función también evolucionó. Inicialmente la dependencia entre dos cantidades se imaginaba como un proceso físico, de modo que su expresión algebraica capturaba la ley física que correspondía a este. La tendencia a una mayor abstracción se vio reforzada a medida que se encontraron ejemplos de funciones sin expresión analítica o representación geométrica sencillas, o sin relación con ningún fenómeno natural; y por los ejemplos «patológicos» como funciones continuas sin derivada en ningún punto.  Durante el siglo XIX los matemáticos alemanes Julius Wilhelm Richard Dedekind, Karl Weierstrass y Georg Cantor, partiendo de un estudio profundo de los números reales, desarrollaron la teoría de funciones, siendo esta teoría independiente del sistema de numeración empleado.[cita requerida] Con el desarrollo de la teoría de conjuntos, en los siglos XIX y XX surgió la definición actual de función, como una correspondencia entre dos conjuntos de objetos cualesquiera, no necesariamente numéricos.[6] También se asoció con otros conceptos vinculados como el de relación binaria.  Introducción Una función es un objeto matemático que se utiliza para expresar la dependencia entre dos magnitudes, y puede presentarse a través de varios aspectos complementarios. Un ejemplo habitual de función numérica es la relación entre la posición y el tiempo en el movimiento de un cuerpo.   Un móvil que se desplaza con una aceleración de 0,66 m/s2 recorre una distancia d que está en función del tiempo transcurrido t. Se dice que d es la variable dependiente y t la variable independiente. Estas magnitudes, calculadas a priori o medidas en un experimento, pueden consignarse de varias maneras. (Se supone que el cuerpo parte en un instante en el que se conviene que el tiempo es t = 0 s.)  Los valores de las variables pueden recogerse en una tabla, anotando la distancia recorrida d en un cierto instante t, para varios momentos distintos:  La gráfica en la imagen es una manera equivalente de presentar la misma información. Cada punto de la curva roja representa una pareja de datos tiempo-distancia, utilizando la correspondencia entre puntos y coordenadas del plano cartesiano. También puede utilizarse una regla o algoritmo que dicte como se ha de calcular d a partir de t. En este caso, la distancia que recorre un cuerpo con esta aceleración está dada por la expresión:  donde las magnitudes se expresan unidades del SI. De estos tres modos se refleja que existe una dependencia entre ambas magnitudes.  Una función también puede reflejar la relación de una variable dependiente con varias variables independientes. Si el cuerpo del ejemplo se mueve con una aceleración constante pero indeterminada a, la distancia recorrida es una función entonces de a y t; en particular,     d =    a ×  t  2    2     {\displaystyle d={\frac {a\times t^{2}}{2}}}  . Las funciones también se utilizan para expresar la dependencia entre otros objetos cualesquiera, no solo los números. Por ejemplo, existe una función que a cada polígono le asigna su número de lados; o una función que a cada día de la semana le asigna el siguiente:  Definición La definición general de función hace referencia a la dependencia entre los elementos de dos conjuntos dados.  Dados dos conjuntos A y B, una función (también aplicación o mapeo) entre ellos es una asociación[7] f que a cada elemento de A le asigna un único elemento de B.  Se dice entonces que A es el dominio (también conjunto de partida o conjunto inicial) de f y que B es su  codominio (también conjunto de llegada o conjunto final).  Un objeto o valor genérico a en el dominio A se denomina la variable independiente; y un objeto genérico b del codominio B es la variable dependiente. También se les llama valores de entrada y de salida, respectivamente. Esta definición es precisa, aunque en matemática se utiliza una definición formal más rigurosa, que construye las funciones como un objeto concreto a partir de la idea de pares ordenados. Es decir, una función es un conjunto de pares ordenados en el cual el primer elemento de cada par no se repite.  Ejemplos Funciones con múltiples variables Existen muchos ejemplos de funciones que «necesitan dos valores» para ser calculadas, como la función «tiempo de viaje» T, que viene dada por el cociente entre la distancia d y la velocidad media v: cada pareja de números reales positivos (una distancia y una velocidad) tiene asociada un número real positivo (el tiempo de viaje). Por tanto, una función puede tener dos (o más) variables independientes.  La noción de función de múltiples variables independientes no necesita de una definición específica separada de la de función «ordinaria». La generalidad de la definición anterior, en la que se contempla que el dominio sea un conjunto de objetos matemáticos arbitrarios, permite omitir la especificación de dos (o más) conjuntos de variables independientes, A1 y A2, por ejemplo. En lugar de ello, el dominio se toma como el conjunto de las parejas (a1, a2), con primera componente en A1 y segunda componente en A2. Este conjunto se denomina el producto cartesiano de A1 y A2, y se denota por A1 × A2.   De este modo las dos variables independientes quedan reunidas en un solo objeto. Por ejemplo, en el caso de la función T, su dominio es el conjunto       R   +     {\displaystyle \mathbb {R} ^{+}}   ×       R   +     {\displaystyle \mathbb {R} ^{+}}  , el conjunto de parejas de números reales positivos. En el caso de más de dos variables, la definición es la misma, usando un conjunto ordenado de múltiples objetos, (a1,..., an), una n-tupla. También el caso de múltiples variables dependientes se contempla de esta manera. Por ejemplo, una función división puede tomar dos números naturales como valores de entrada (dividendo y divisor) y arrojar dos números naturales como valores de salida (cociente y resto). Se dice entonces que esta función tiene como dominio y codominio el conjunto      N  ×  N    {\displaystyle \mathbb {N} \times \mathbb {N} }  .  Notación y Nomenclatura La notación habitual para presentar una función f con dominio A y codominio B es:          f :   A   ⟶   B      a   ↦   b = f ( a )       {\displaystyle {\begin{array}{rrcl}f:&A&\longrightarrow &B\\&a&\mapsto &b=f(a)\end{array}}}    También se dice que f es una función «de A a B» o «entre A y B». El dominio de una función f se denota también por dom(f), D(f), Df, etc. Por f(a) se resume la operación o regla que permite obtener el elemento de B asociado a un cierto a ∈ A, denominado la imagen de a.[7]  La notación utilizada puede ser un poco más laxa, como por ejemplo     f ( n ) =   n     {\displaystyle f(n)={\sqrt {n}}}  . En dicha expresión, no se especifica que conjuntos se toman como dominio y codominio. En general, estos vendrán dados por el contexto en el que se especifique dicha función. En el caso de funciones de varias variables (dos, por ejemplo), la imagen del par     (  a  1   ,  a  2   )   {\displaystyle (a_{1},a_{2})}   no se denota por     f ( (  a  1   ,  a  2   ) )   {\displaystyle f((a_{1},a_{2}))}  , sino por     f (  a  1   ,  a  2   )   {\displaystyle f(a_{1},a_{2})}  , y similarmente para más variables.  Existen además terminologías diversas en distintas ramas de la matemática para referirse a funciones con determinados dominios y codominios:  También las sucesiones infinitas de elementos tales como a, b, c, ... son funciones, cuyo dominio en este caso son los números naturales. Las palabras «función», «aplicación», «mapeo», u otras como «operador», «funcional», etc., pueden designar tipos concretos de función según el contexto. Adicionalmente, algunos autores restringen la palabra «función» para el caso en el que los elementos del conjunto inicial y final son números.[8]  Imagen e imagen inversa Los elementos del codominio B asociados con algún elemento del dominio A constituyen la imagen de la función.  Dada una función f : A → B, el elemento de B que corresponde a un cierto elemento a del dominio A se denomina la imagen de a, f(a).   El conjunto de las imágenes de cada elemento del dominio es la imagen de la función f (también rango o recorrido de f). El conjunto de las imágenes de un subconjunto cualquiera del dominio, X ⊆ A, se denomina la imagen de X.  "
matematicas,"El porcentaje  es  un punto símbolo matemático que representa una cantidad dada como una fracción en 100 partes iguales. También se le llama comúnmente tanto por ciento, donde por ciento significa «de cada cien unidades». Se usa para definir las relaciones entre dos cantidades, de forma que el tanto por ciento de una cantidad, que es un número, se refiere a la parte proporcional que ese número de unidades representa por cada cien de esa cantidad.  El porcentaje se denota utilizando el símbolo «%»,[1] que matemáticamente equivale al factor 0,01 y que se debe escribir después del número al que se refiere, dejando un espacio de separación.[2][3] Por ejemplo, «treinta y dos por ciento» se representa escribiendo 32 %, y significa ‘treinta y dos de cada cien’. También puede representarse:  y, operando:  El 32 % de 2000 significa la parte proporcional a 32 unidades de cada 100 de esas 2000, es decir:  640 unidades en total.  El porcentaje se usa para comparar una fracción (que indica la relación entre dos cantidades) con otra, expresándolas mediante porcentajes para usar 100 como denominador común. Por ejemplo, si en un país hay 500 000 enfermos de gripe de un total de 10 millones de personas, y en otro hay 150 000 enfermos de un total de un millón de personas, resulta más claro expresar que en el primer país hay un 5 % (5 por ciento) de personas con gripe, y en el segundo hay un 15 % (15 por ciento), lo que da como resultado una proporción mayor en el segundo país.  Ejemplos Por ejemplo, 45 % (léase ""cuarenta y cinco por ciento"") es igual a la fracción 45/100, la proporción 45:55 (o 45:100 cuando se compara con el total en lugar de la otra parte), o 0,45. Los porcentajes se utilizan a menudo para expresar una parte proporcional de un total.  (Del mismo modo, uno también puede expresar un número como una fracción de 1 000, usando el término ""por mil"" o el símbolo ""‰"".)  Ejemplo 1 Si el 50 % del número total de alumnos de la clase son hombres, eso significa que 50 de cada 100 alumnos son hombres. Si hay 500 estudiantes, entonces 250 de ellos son hombres.  Ejemplo 2 Un aumento de 0,15 € sobre un precio de 2,50 € es un aumento de una fracción de 0,15/2,50 = 0,06. Expresado como un porcentaje, esto es un aumento del 6 %.  Si bien muchos valores porcentuales están entre 0 y 100, no existe una restricción matemática y los porcentajes pueden tomar otros valores.[4] Por ejemplo, es común referirse a 111 % o −35 %, especialmente para cambios porcentuales y comparaciones.  Idea y origen Ya era una herramienta de análisis en el siglo XV que tenía aplicación a la hora de calcular impuestos e intereses; sin embargo, el uso de este solo proviene de la abreviatura de una idea que databa desde hace mucho. En el antiguo imperio romano, el emperador Augusto estableció un sistema de impuestos en el que se dictaba que había que pagar el       1 100     {\textstyle {\frac {1}{100}}}   sobre los bienes vendidos en subastas. Ya entonces, para facilitar los cálculos, utilizaban fracciones simplificadas a las centenas.   A medida que crecían las denominaciones de dinero en la Edad Media, los cálculos con un denominador de 100 se volvieron cada vez más estándar, de modo que desde finales del siglo XV hasta principios del siglo XVI, se hizo común que los textos aritméticos incluyeran tales cálculos. Muchos de estos textos aplicaron estos métodos a pérdidas y ganancias, tipos de interés y la regla de tres. En el siglo XVII, era estándar cotizar las tasas de interés en centésimas.[5]  Evolución La idea de «por ciento» surge de la necesidad de abreviar el uso de las fracciones en la cotidianidad, pues resultaba tener mayor complejidad hacer referencia al       2 3     {\textstyle {\frac {2}{3}}}   de una cantidad que al 66 %, por lo que, con el tiempo, era más común que se hablase únicamente de fracciones reducidas a las centenas, y progresivamente se fue actualizando la referencia hablada hasta llegar al «por ciento». Al hacerlo, nació la necesidad de plasmar la nueva abreviación, lo que generó, con el tiempo, el uso de varios símbolos. El primero provino de un manuscrito anónimo de 1425 en el que el autor hacía referencia al «por ciento» que se solía utilizar en la época con un símbolo que dio evolución al actual «%».  Cálculos El valor porcentual se calcula multiplicando el valor numérico de la razón por 100. Por ejemplo, para hallar 50 manzanas como porcentaje de 1250 manzanas, primero se calcula la razón 50/1250 = 0,04, y luego se multiplica por 100 para obtener 4 %. El valor porcentual también se puede encontrar multiplicando primero en lugar de después, por lo que en este ejemplo, el 50 se multiplicaría por 100 para dar 5000, y este resultado se dividiría por 1250 para dar 4 %.  Para calcular un porcentaje de un porcentaje, se convierte ambos porcentajes a fracciones de 100, o a decimales, y se multiplican. Por ejemplo, 50 % de 40 % es:  No es correcto dividir por 100 y usar el signo de porcentaje al mismo tiempo; literalmente implicaría una división por 10.000. Por ejemplo, 25% = 25/100 = 0,25, no 25%/100, que en realidad es 25⁄100/100 = 0,0025. Un término como 100/100 % también sería incorrecto, ya que se leería como 1 por ciento, incluso si la intención fuera decir 100 %.  Siempre que se comunique sobre un porcentaje, es importante especificar a qué se refiere (es decir, cuál es el total que corresponde al 100 %). El siguiente problema ilustra este punto.  Se nos pide que calculemos la proporción de mujeres que se especializan en ciencias de la computación con respecto a todas las estudiantes de ciencias de la computación. Sabemos que el 60 % de todos los estudiantes son mujeres, y entre estos el 5 % son estudiantes de informática, por lo que concluimos que 60/100 × 5/100 = 3/100 o el 3 % de todos los estudiantes son mujeres con especialización en informática. Dividiendo esto por el 10 % de todos los estudiantes que son estudiantes de informática, llegamos a la respuesta: 3%/10% = 30/100 o el 30 % de todos los estudiantes de informática son mujeres.  Este ejemplo está estrechamente relacionado con el concepto de probabilidad condicional.  Símbolo Muchos creen que el símbolo «%» ha evolucionado a partir de la expresión matemática       x 100   .   {\displaystyle {\frac {x}{100}}.}    Símbolo en el siglo XV  Símbolo en el siglo XVII  Símbolo desde el siglo XVIII  El símbolo % es una forma estilizada de los dos ceros. Evolucionó a partir de un símbolo similar, sólo que presentaba una línea horizontal en lugar de diagonal (c. 1650), que a su vez proviene de un símbolo que representaba «P cento» (c. 1425), (per cento).[6][7] El ""per"" a menudo se abreviaba como ""p"", y finalmente desapareció por completo. El ""cento"" se contrajo a dos círculos separados por una línea horizontal, de la que se deriva el símbolo moderno ""%"".[8]   Otros símbolos relacionados incluyen ‰ (por mil) y ‱ (por diez mil, también conocido como un punto básico), que indican que un número se divide por mil o diez mil, respectivamente.  Representación Tanto por ciento como fracción El tanto por ciento se divide entre 100 y se simplifica la fracción.  Ejemplo:  Para saber cómo se representa el 10 % en fracción, se divide y luego se simplifica:  Tanto por ciento como multiplicación La fracción común se multiplica por el número que sea necesario para que el denominador sea 100 y se toma el numerador, que será el porcentaje.  Ejemplo:  Para representar 1/10 como un porcentaje se hace la siguiente operación:  Obtener un tanto por ciento de un número Para obtener un tanto por ciento de un número, simplemente se multiplica por 0,01, es decir, 1/100. Por ejemplo, el 25 % de 150 es     25 ⋅ 0 , 01 ⋅ 150 = 37 , 5   {\displaystyle 25\cdot 0,01\cdot 150=37,5}  . Una forma equivalente de tratar esta operación es considerar que se multiplica por la cifra y se divide por cien (pues 0,01 = 1/100).  Alternativamente, en un método muy habitual antaño, se construye una regla de tres simple directa. Así, para calcular el 25 % de 150, se hace la regla de tres: simplemente se multiplica cruzado y divide por el que queda solo o en conjunción con el restado.  Por tanto: 37,5 es el 25 % de 150.  Porcentaje de aumento y disminución Debido al uso inconsistente, no siempre está claro en el contexto a qué se refiere un porcentaje. Cuando se habla de una ""subida del 10 %"" o de una ""bajada del 10 %"" en una cantidad, la interpretación habitual es que es relativa al ""valor inicial"" de esa cantidad. Por ejemplo, si un artículo tiene un precio inicial de $200 y el precio aumenta un 10 % (un aumento de $20), el nuevo precio será de $220. Tenga en cuenta que este precio final es el 110 % del precio inicial (100 % + 10 % = 110 %).  Algunos otros ejemplos de cambios porcentuales:  En general, un cambio de x por ciento en una cantidad da como resultado una cantidad final que es 100 + x por ciento de la cantidad original (equivalentemente, (1 + 0.01x) veces la cantidad original).  Porcentajes compuestos Los cambios porcentuales aplicados secuencialmente no suman de la forma habitual. Por ejemplo, si el aumento del 10 % en el precio considerado anteriormente (en el artículo de 200€, elevando su precio a 220€) es seguido por una disminución del 10 % en el precio (una disminución de 22€), entonces el precio final será de 198€—no el precio original de 200€. La razón de esta aparente discrepancia es que los cambios del dos por ciento (+10 % y −10 %) se miden en relación con cantidades ""diferentes"" (200€ y 220€, respectivamente) y, por lo tanto, no se ""cancelan"".  En general, si un aumento de x por ciento es seguido por una disminución de x por ciento, y la cantidad inicial era p, la cantidad final es p(1 + 0,01x)(1 − 0.01x) = p(1 − (0.01x)Plantilla:Sup); por lo tanto, el cambio neto es una disminución general de x por ciento de x porcentaje (el cuadrado del cambio porcentual original cuando se expresa como un número decimal). Así, en el ejemplo anterior, después de un aumento y disminución de x = 10 por ciento, el monto final, 198€, fue 10 % de 10 %, o 1 %, menos que el monto inicial de 200€. El cambio neto es el mismo para una disminución de x por ciento, seguido de un aumento de x por ciento; la cantidad final es p(1 - 0,01x)(1 + 0,01x) = p(1 − (0,01x)Plantilla:Sup).  Esto se puede ampliar para un caso en el que uno no tenga el mismo cambio porcentual. Si la cantidad inicial p conduce a un cambio porcentual x, y el segundo cambio porcentual es y, entonces la cantidad final es p(1 + 0,01x)(1 + 0,01y).  Para cambiar el ejemplo anterior, después de un aumento de x = 10 por ciento y disminución de y = −5 percent, la cantidad final, 209€, es 4,5 % más que la cantidad inicial de 200€.  Como se muestra arriba, los cambios porcentuales se pueden aplicar en cualquier orden y tienen el mismo efecto.  En el caso de las tasas de interés, una forma muy común pero ambigua de decir que una tasa de interés subió del "
matematicas,"El lenguaje, también llamada lógica simbólica, lógica teorética, lógica formal o logística,[1] es el estudio formal y simbólico de la lógica y su aplicación a algunas áreas de la matemática y la ciencia. Comprende la aplicación de las técnicas de la lógica formal a la construcción y el desarrollo de las matemáticas y el razonamiento matemático, y conversamente la aplicación de técnicas matemáticas a la representación y el análisis de la lógica formal. La investigación en lógica matemática ha jugado un papel crucial en el estudio de los fundamentos de las matemáticas.  La lógica matemática estudia la inferencia mediante la construcción de sistemas formales como la lógica proposicional, la lógica de primer orden o la lógica modal. Estos sistemas capturan las características esenciales de las inferencias válidas en los lenguajes naturales, pero al ser estructuras formales susceptibles de análisis matemático, permiten realizar demostraciones rigurosas sobre ellas.  La lógica matemática se suele dividir en cuatro áreas: teoría de modelos, teoría de la demostración, teoría de conjuntos y teoría de la computabilidad. La teoría de la demostración y la teoría de modelos fueron el fundamento de la lógica matemática. La teoría de conjuntos se originó en el estudio del infinito por Georg Cantor y ha sido la fuente de muchos de los temas más desafiantes e importantes de la lógica matemática, desde el teorema de Cantor, el axioma de elección y la cuestión de la independencia de la hipótesis del continuo, al debate moderno sobre grandes axiomas cardinales. La lógica matemática tiene estrechas conexiones con las ciencias de la computación. La teoría de la computabilidad captura la idea de la computación en términos lógicos y aritméticos. Sus logros más clásicos son la indecidibilidad del Entscheidungsproblem de Alan Turing y su presentación de la tesis de Church-Turing. Hoy en día, la teoría de la computabilidad se ocupa principalmente del problema más refinado de las clases de complejidad (¿cuándo es un problema eficientemente solucionable?) y de la clasificación de los grados de insolubilidad.  La lógica matemática también estudia las definiciones de nociones y objetos matemáticos básicos como conjuntos, números, demostraciones y algoritmos. La lógica matemática estudia las reglas de deducción formales, las capacidades expresivas de los diferentes lenguajes formales y las propiedades metalógicas de los mismos.  En un nivel elemental, la lógica proporciona reglas y técnicas para determinar si es o no válido un argumento dado dentro de un determinado sistema formal. En un nivel avanzado, la lógica matemática se ocupa de la posibilidad de axiomatizar las teorías matemáticas, de clasificar su capacidad expresiva, y desarrollar métodos computacionales útiles en sistemas formales. La teoría de la demostración y la matemática inversa son dos de los razonamientos más recientes de la lógica matemática abstracta. Debe señalarse que la lógica matemática se ocupa de sistemas formales que pueden no ser equivalentes en todos sus aspectos, por lo que la lógica matemática no es un método para descubrir verdades del mundo físico real, sino solo una fuente posible de modelos lógicos aplicables a teorías científicas, muy especialmente a la matemática convencional.   Por otra parte, la lógica matemática no estudia el concepto de razonamiento humano general o el proceso creativo de construcción de demostraciones matemáticas mediante argumentos rigurosos pero con lenguaje informal con algunos signos o diagramas, sino solo de demostraciones y razonamientos que se pueden formalizar por completo.  Áreas La Mathematics Subject Classification divide la lógica matemática en las siguientes áreas:  En algunos casos hay conjunción de intereses con la informática teórica, pues muchos pioneros de la informática, como Alan Turing, fueron matemáticos y lógicos. Así, el estudio de la semántica de los lenguajes de programación procede de la teoría de modelos, así como también la verificación de programas y el caso particular de la técnica del model checking. También el isomorfismo de Churry-Howard entre pruebas y programas se corresponde con la teoría de pruebas, donde la lógica intuicionista y la lógica lineal son especialmente significativas.  Algunos sistemas formales como el cálculo lambda y la lógica combinatoria entre otras han devenido en auténticos lenguajes de programación, creando nuevos paradigmas como son la programación funcional y la programación lógica.  Sistemas formales Un sistema formal o sistema lógico es un sistema abstracto compuesto por un lenguaje formal, axiomas, reglas de inferencia y a veces una semántica formal, que se utiliza para deducir o demostrar teoremas y dar una definición rigurosa del concepto de demostración. Un sistema formal es una formalización rigurosa y completa del concepto de sistema axiomático, los cuales se pueden expresar en lenguaje formal o en lenguaje natural formalizado. Al crear un sistema formal se pretende capturar y abstraer la esencia de determinadas características del mundo real, en un modelo conceptual expresado en un determinado lenguaje formal. Algunos de los sistemas formales más conocidos son la lógica proposicional, la lógica de primer orden y la lógica modal.  En la teoría de la demostración, las demostraciones formales se pueden expresar en el lenguaje de los sistemas formales, consistentes en axiomas y reglas de inferencia. Los teoremas pueden ser obtenidos por medio de demostraciones formales. Este punto de vista de las matemáticas ha sido denominado formalista; aunque en muchas ocasiones este término conlleva una acepción peyorativa. En ese sentido, David Hilbert creó la metamatemática para estudiar los sistemas formales, entendiendo que el lenguaje utilizado para ello, denominado metalenguaje era distinto del lenguaje del sistema formal que se pretendía estudiar, al que se llama lenguaje objeto.  Un sistema así es la reducción de un lenguaje formalizado a meros símbolos, lenguaje formalizado y simbolizado sin contenido material alguno; un lenguaje reducido a mera forma que se expresa mediante fórmulas que reflejan las relaciones sintácticas entre los símbolos y las reglas de formación y transformación que permiten construir las fórmulas del sistema y pasar de una fórmula a otra.[2]  Una teoría axiomática es un conjunto de fórmulas en un determinado lenguaje formal y todas las fórmulas deducibles de dichas expresiones mediante las reglas de inferencia posibles en dicho sistema formal. El objetivo de las teorías axiomáticas es construir sistemas formales que representen las características esenciales de ramas enteras de las matemáticas. Si se selecciona un conjunto más amplio o menos amplio de axiomas el conjunto de teoremas deducibles cambian. El interés de la teoría de modelos es que en un modelo en que satisfagan los axiomas de determinada teoría también se satisfacen los teoremas deducibles de dicha teoría. Es decir, si un teorema es deducible en una cierta teoría, entonces ese teorema es universalmente válido en todos los modelos que satisfacen los axiomas. Esto es interesante porque en principio la clase de modelos que satisface una cierta teoría es difícil de conocer, ya que las teorías matemáticas interesantes en general admiten toda clase infinita de modelos no isomorfos, por lo que su clasificación en general resulta difícilmente abordable si no existe un sistema formal y un conjunto de axiomas que caracterice los diferentes tipos de modelos.  En el siglo XX, Hilbert y otros sostuvieron que la matemática es un sistema formal. Pero en 1931, Kurt Gödel demostró que ningún sistema formal con suficiente poder expresivo para capturar la aritmética de Peano puede ser a la vez consistente y completo. El teorema de la incompletitud de Gödel, junto con la demostración de Alonzo Church de que la matemática tampoco es decidible, terminó con el programa de Hilbert. Sin embargo,  a pesar de sus limitaciones, el enfoque sigue siendo ampliamente usado, básicamente porque no se ha encontrado ninguna alternativa mejor al enfoque formalista de Hilbert y la pretensión de trabajar en el seno de teorías matemáticas explícitamente axiomatizadas, aun con sus limitaciones.  Metalógica Teoría de modelos En matemática, teoría de modelos es el estudio de (clases de) estructuras matemáticas tales como grupos, cuerpos, grafos, o incluso universos de teoría de conjuntos, en relación con las teorías axiomáticas y la lógica matemática. La teoría de modelos permite atribuir una interpretación semántica a las expresiones puramente formales de los lenguajes formales. Además permite estudiar en sí mismos los conjuntos de axiomas, su completitud, consistencia, independencia mutua, y permiten introducir un importante número de cuestiones metalógicas.  Teoría de la computabilidad La teoría de la computabilidad o teoría de la recursión es la parte de la computación que estudia los problemas de decisión que se pueden resolver con un algoritmo o equivalentemente con una máquina de Turing. Las preguntas fundamentales de la teoría de la computabilidad son:  Teoría de conjuntos La teoría de conjuntos es una rama de la lógica matemática que estudia las propiedades y relaciones de los conjuntos: colecciones abstractas de objetos, consideradas como objetos en sí mismas. Los conjuntos y sus operaciones más elementales son una herramienta básica que permite formular de cualquier otra teoría matemática.[5]  La teoría de los conjuntos es lo suficientemente flexible y general como para construir el resto de objetos y estructuras de interés en matemáticas: números, funciones, figuras geométricas, etc; gracias a las herramientas de la lógica, permite estudiar los fundamentos.   Además, la propia teoría de conjuntos es objeto de estudio per se, no solo como herramienta auxiliar, en particular las propiedades y relaciones de los  conjuntos infinitos. En esta disciplina es habitual que se presenten casos de propiedades indemostrables o contradictorias, como la hipótesis del continuo o la existencia de algún cardinal inaccesible. Por esta razón, sus razonamientos y técnicas se apoyan en gran medida en la lógica matemática.[6]  El desarrollo histórico de la teoría de conjuntos se atribuye a Georg Cantor, que comenzó a investigar cuestiones conjuntistas (puras) del infinito en la segunda mitad del siglo XIX, precedido por algunas ideas de Bernhard Bolzano e influido por Richard Dedekind. El descubrimiento de las paradojas de la teoría cantoriana de conjuntos, formalizada por Gottlob Frege, propició los trabajos de Bertrand Russell, Ernst Zermelo y Abraham Fraenkel.[7]  Teoría de la demostración Historia El uso más temprano de matemáticas y de geometría en relación con la lógica y la filosofía se remonta a los griegos antiguos tales como Euclides, Platón, y Aristóteles. Muchos otros filósofos antiguos y medievales aplicaron ideas y métodos matemáticos a sus afirmaciones filosóficas.  En el siglo XVIII se hicieron algunos intentos de tratar las operaciones lógicas formales de una manera simbólica por parte de algunos filósofos matemáticos como Lambert, pero su labor permaneció desconocida y aislada. También por parte de Leibniz  que desarrolló la idea de un calculus ratiocinator, un sistema de reglas para simplificar oraciones compuestas.  Siglo XIX A partir de la segunda mitad del siglo XIX, la lógica sería revolucionada profundamente. En 1847, George Boole publicó un breve tratado titulado El análisis matemático de la lógica, y en 1854 otro más importante titulado Las leyes del pensamiento. La idea de Boole fue construir a la lógica como un cálculo en el que los valores de verdad se representan mediante el F (falsedad) y la V (verdad), y a los que se les aplican operaciones matemáticas como la suma y la multiplicación.  En el último tercio del siglo XIX la lógica va a encontrar su transformación más profunda de la mano de las investigaciones matemáticas y lógicas, junto con el desarrollo de la investigación de las estructuras profundas del lenguaje, la lingüística, convirtiéndose definitivamente en una ciencia formal. Es una ciencia formal, ya que estudia las ideas y constituye una herramienta conceptual para todas las otras ciencias y áreas del conocimiento. y forma parte de un conjunto sistemático de conocimientos racionales y coherentes, que se ocupan del estudio de los procesos lógicos y matemáticos,  Al mismo tiempo, Augustus De Morgan publica en 1847 su obra Lógica formal, "
matematicas,"En cálculo diferencial y análisis matemático, la derivada de una función es la razón de cambio instantánea con la que varía el valor de dicha función matemática, según se modifique el valor de su variable independiente. La derivada de una función es un concepto local, es decir, se calcula como el límite de la rapidez de cambio media de la función en cierto intervalo, cuando el intervalo considerado para la variable independiente se torna cada vez más pequeño.[1] Por eso se habla del valor de la derivada de una función en un punto dado.  Un ejemplo habitual aparece al estudiar el movimiento: si una función representa la posición de un objeto con respecto al tiempo, su derivada es la velocidad de dicho objeto para todos los momentos. Un avión que realice un vuelo transatlántico de 4500 km entre las 12:00 y las 18:00, viaja a una velocidad media de 750 km/h. Sin embargo, puede estar viajando a velocidades mayores o menores en distintos tramos de la ruta. En particular, si entre las 15:00 y las 15:30 recorre 400 km, su velocidad media en ese tramo es de 800 km/h. Para conocer su velocidad instantánea a las 15:20, por ejemplo, es necesario calcular la velocidad media en intervalos de tiempo cada vez menores alrededor de esta hora: entre las 15:15 y las 15:25, entre las 15:19 y las 15:21.  Entonces el valor de la derivada de una función en un punto puede interpretarse geométricamente, ya que se corresponde con la pendiente de la recta tangente a la gráfica de la función en dicho punto. La recta tangente es, a su vez, la gráfica de la mejor aproximación lineal de la función alrededor de dicho punto. La noción de derivada puede generalizarse para el caso de funciones de más de una variable con la derivada parcial y el diferencial.  Historia de la derivada Los problemas típicos que dieron origen al cálculo infinitesimal comenzaron a plantearse en la época clásica de la antigua Grecia (siglo III a. C.), pero no se encontraron métodos sistemáticos de resolución hasta diecinueve siglos después (en el siglo XVII por obra de Isaac Newton y Gottfried Leibniz).  En lo que atañe a las derivadas existen dos conceptos de tipo geométrico que le dieron origen:  En su conjunto dieron origen a lo que actualmente se conoce como cálculo diferencial.  Siglo XVII Los matemáticos perdieron el miedo que los griegos les habían tenido a los infinitesimales: Johannes Kepler y Bonaventura Cavalieri fueron los primeros en usarlos, empezaron a andar un camino que llevaría en medio siglo al descubrimiento del cálculo infinitesimal.  A mediados del siglo XVII las cantidades infinitesimales fueron cada vez más usadas para resolver problemas de cálculos de tangentes, áreas, volúmenes; los primeros darían origen al cálculo diferencial, los otros al integral.  Newton y Leibniz A finales del siglo XVII se sintetizaron en dos conceptos los algoritmos usados por sus predecesores, en lo que hoy llamamos «derivada» e «integral». La historia de la matemática reconoce que Isaac Newton y Gottfried Leibniz son los creadores del cálculo diferencial e integral. Ellos desarrollaron reglas para manipular las derivadas (reglas de derivación) e Isaac Barrow demostró que la derivación y la integración son operaciones inversas.  Newton desarrolló en Cambridge su propio método para el cálculo de tangentes. En 1665 encontró un algoritmo para derivar funciones algebraicas que coincidía con el descubierto por Fermat. A finales de 1665 se dedicó a reestructurar las bases de su cálculo, intentando desligarse de los infinitesimales, e introdujo el concepto de fluxión, que para él era la velocidad con la que una variable «fluye» (varía) con el tiempo.  Gottfried Leibniz, por su parte, formuló y desarrolló el cálculo diferencial en 1675. Fue el primero en publicar los mismos resultados que Isaac Newton descubriera 10 años antes, de manera independiente. En su investigación conservó un carácter geométrico y trató a la derivada como un cociente incremental y no como una velocidad, viendo el sentido de su correspondencia con la pendiente de la recta tangente a la curva en dicho punto.  Leibniz es el inventor de diversos símbolos matemáticos. A él se deben los nombres de: cálculo diferencial y cálculo integral, así como los símbolos de derivada          d  y    d  x       {\displaystyle \textstyle {\frac {\mathrm {d} y}{\mathrm {d} x}}}   y el símbolo de la integral ∫.  Conceptos y aplicaciones El concepto de derivada es uno de los conceptos básicos del análisis matemático. Los otros son los de integral definida e indefinida, sucesión; sobre todo, el concepto de límite. Este es usado para la definición de cualquier tipo de derivada y para la integral de Riemann, sucesión convergente y suma de una serie y la continuidad. Por su importancia, hay un antes y un después de tal concepto que biseca las matemáticas previas, como el álgebra, la trigonometría o la geometría analítica, del cálculo. Según Albert Einstein, el mayor aporte que se obtuvo de la derivadas fue la posibilidad de formular diversos problemas de la física mediante ecuaciones diferenciales [cita requerida].  La derivada es un concepto que tiene variadas aplicaciones. Se aplica en aquellos casos donde es necesario medir la rapidez con que se produce el cambio de una magnitud o situación. Es una herramienta de cálculo fundamental en los estudios de Física, Química y Biología, o en ciencias sociales como la Economía y la Sociología. Por ejemplo, cuando se refiere a la gráfica de dos dimensiones de     f   {\displaystyle f}  , se considera la derivada como la pendiente de la recta tangente del gráfico en el punto     x   {\displaystyle x}  . Se puede aproximar la pendiente de esta tangente como el límite cuando la distancia entre los dos puntos que determinan una recta secante tiende a cero, es decir, se transforma la recta secante en una recta tangente. Con esta interpretación, pueden determinarse muchas propiedades geométricas de los gráficos de funciones, tales como monotonía de una función (si es creciente o decreciente) y la concavidad o convexidad.  Algunas funciones no tienen derivada en todos o en alguno de sus puntos. Por ejemplo, una función no tiene derivada en los puntos en que se tiene una tangente vertical, una discontinuidad o un punto anguloso. Afortunadamente, gran cantidad de las funciones que se consideran en las aplicaciones prácticas son continuas y su gráfica es una curva suave, por lo que es susceptible de derivación.  Las funciones que son diferenciables (derivables si se habla en una sola variable), son aproximables linealmente.  Definiciones de derivada Derivada en un punto a partir de cocientes diferenciales La derivada de una función     f    {\displaystyle f\,}   en el punto     a   {\displaystyle a}   es la pendiente de la recta tangente a la gráfica de     f    {\displaystyle f\,}   en el punto     a    {\displaystyle a\,}  . El valor de esta pendiente será aproximadamente igual a la pendiente de una recta secante a la gráfica que pase por el punto     ( a , f ( a ) )   {\displaystyle (a,f(a))}   y por un punto cercano     ( x , f ( x ) )   {\displaystyle (x,f(x))}  ; por conveniencia suele expresarse     x = a + h   {\displaystyle x=a+h}  , donde     h   {\displaystyle h}   es un número cercano a 0. A partir de estos dos puntos se calcula la pendiente de la recta secante como  (Esta expresión se denomina «cociente diferencial» o «cociente de Newton».[2]) A medida que el número     h   {\displaystyle h}   se acerca a cero, el valor de esta pendiente se aproximará mejor al de la recta tangente. Esto permite definir la derivada de la función     f   {\displaystyle f}   en el punto     a   {\displaystyle a}  , denotada como      f ′  ( a )   {\displaystyle f'(a)}  , como el límite de estos cocientes cuando     h   {\displaystyle h}   tiende a cero:  No obstante, esta definición sólo es válida cuando el límite es un número real: en los puntos     a   {\displaystyle a}   donde el límite no existe, la función     f   {\displaystyle f}   no tiene derivada.  Derivada de una función Dada una función     f   {\displaystyle f}  , se puede definir una nueva función que, en cada punto     x   {\displaystyle x}  , toma el valor de la derivada      f ′  ( x )   {\displaystyle f'(x)}  . Esta función se denota      f ′    {\displaystyle f'}   y se denomina función derivada de     f   {\displaystyle f}   o simplemente derivada de     f   {\displaystyle f}  . Esto es, la derivada de     f   {\displaystyle f}   es la función dada por  Esta función sólo está definida en los puntos del dominio de     f   {\displaystyle f}   donde el límite existe; en otras palabras, el dominio de      f ′    {\displaystyle f'}   está contenido en el de     f   {\displaystyle f}  .  Considere la función cuadrática     f ( x ) =  x  2     {\displaystyle f(x)=x^{2}}   definida para todo     x ∈  R    {\displaystyle x\in \mathbb {R} }  . Se trata de calcular la derivada de esta función aplicando la definición  Continuidad y diferenciabilidad La continuidad es necesaria Para que una función sea derivable en un punto es necesario que también sea continua en ese punto: intuitivamente, si la gráfica de una función está «rota» en un punto, no hay una manera clara de trazar una recta tangente a la gráfica. Más precisamente, esto se debe a que, si una función     f   {\displaystyle f}   no es continua en un punto     a   {\displaystyle a}  , entonces la diferencia entre el valor     f ( a )   {\displaystyle f(a)}   y el valor en un punto cercano     f ( a + h )   {\displaystyle f(a+h)}   no va a tender a 0 a medida que la distancia     h   {\displaystyle h}   entre los dos puntos tiende a 0; de hecho, el límite      lim  h → 0   f ( a + h ) − f ( a )   {\displaystyle \lim _{h\to 0}f(a+h)-f(a)}   no tiene por qué estar bien definido si los dos límites laterales no son iguales. Tanto si este límite no existe como si existe pero es distinto de 0, el cociente diferencial  no tendrá un límite definido.  Como ejemplo de lo que ocurre cuando la función no es continua, se puede considerar la función de Heaviside, definida como  Esta función no es continua en     x = 0   {\displaystyle x=0}  : el valor de la función en este punto es 1, pero en todos los puntos a su izquierda la función vale 0. En este caso, el límite por la izquierda de la diferencia      lim  h →  0  −     f ( 0 + h ) − f ( 0 )   {\displaystyle \lim _{h\to 0^{-}}f(0+h)-f(0)}   es igual a 1, por lo que el cociente diferencial no tendrá un límite bien definido.  La continuidad no es suficiente La relación no funciona a la inversa: el que una función sea continua no garantiza su derivabilidad. Es posible que los límites laterales sean iguales pero las derivadas laterales no; en este caso concreto, la función presenta un punto anguloso en dicho punto.  Un ejemplo es la función valor absoluto     f ( x ) =  |  x  |    {\displaystyle f(x)=|x|}  , que se define como   Esta función es continua en el punto     x = 0   {\displaystyle x=0}  : en este punto la función toma el valor 0, y para valores de     x   {\displaystyle x}   infinitamente cercanos a 0, tanto positivos como negativos, el valor de la función tiende a 0. Sin embargo, no es derivable: la derivada lateral por la derecha de     x = 0   {\displaystyle x=0}   es igual a 1, mientras que por la izquierda la derivada lateral vale -1. Como las derivadas laterales dan resultados diferentes, no existe derivada en     x = 0   {\displaystyle x=0}  , a pesar de que la función sea continua en dicho punto.  De manera informal, si el gráfico de la función tiene puntas agudas, se interrumpe o tiene saltos, no es derivable. Sin embargo, la función f(x)=x|x| es diferenciable para todo x.  Notación Existen diversas formas para nombrar "
matematicas,"En el ámbito de la estadística, la mediana (del latín mediānus 'del medio'[1]) representa el valor de la variable de posición central en un conjunto de datos ordenados. Se le denota mediana.  Si la serie tiene un número par de puntuaciones, la mediana es la media entre las dos puntuaciones centrales.  Ejemplo.  7, 8, 9, 10, 11, 12   Me = 9,5 = (9+10)/2  Conjunto finito de números La mediana de una lista finita de números es el número ""medio"", cuando esos números se enumeran en orden de menor a mayor.  Si el conjunto de datos tiene un número impar de observaciones, se selecciona la del medio. Por ejemplo, la siguiente lista de siete números,  tiene como mediana 6, que es el cuarto valor.  Si el conjunto de datos tiene un número par de observaciones, no hay un valor medio distinto y la mediana suele definirse como la media aritmética de los dos valores medios.[2][3] Por ejemplo, este conjunto de datos de 8 números  tiene un valor mediano de 4.5, es decir     ( 4 + 5 )  /  2   {\displaystyle (4+5)/2}  . (En términos más técnicos, esto interpreta la mediana como el estimador completamente recortado rango medio).   En general, con esta convención, la mediana puede definirse como sigue: Para un conjunto de datos     x   {\displaystyle x}   de     n   {\displaystyle n}   elementos, ordenados de menor a mayor,  Conceptos generales En teoría de la probabilidad, se define la mediana de una variable aleatoria como un número tal que la variable tiene igual probabilidad de tomar valores menores o mayores que él. Finalmente, en inferencia estadística se estudia la mediana poblacional y la mediana muestral.  La mediana se utiliza normalmente para dar un valor ""típico"" que caracteriza un conjunto de datos. En comparación con la media, la propiedad esencial de la mediana es que no se ve afectada si hay un grupo de datos mucho más pequeño o mucho más grandes que las otras, mientras que la media sí que puede quedar distorsionada. Un ejemplo de esta situación se da al analizar el tiempo que los estudiantes universitarios tardan en acabar una carrera, el hecho que haya algunos estudiantes que estén muchos años para acabar la carrera (porque se ponen a trabajar y retardan los estudios, u otros motivos) hace que la media no refleje bien los datos; al contrario, la mediana no es sensible a estos valores extremos, y proporciona un mejor valor representativo de la duración de los estudios.  Definición formal Formalmente, una mediana de una población es cualquier valor tal que al menos la mitad de la población es menor o igual que la mediana propuesta y al menos la mitad es mayor o igual que la mediana propuesta.  Como se ha véase anteriormente, las medianas pueden no ser únicas.  Si cada conjunto contiene más de la mitad de la población, entonces parte de la población es exactamente igual a la mediana única.  La mediana está bien definida para cualquier dato ordenado (unidimensional), y es independiente de cualquier espacio métrico.  Por tanto, la mediana puede aplicarse a clases ordenadas pero no numéricas (por ejemplo, calcular la mediana de una nota cuando los alumnos se califican de A a F), aunque el resultado podría estar a medio camino entre las clases si hay un número par de casos.  Una mediana geométrica, en cambio, se define en cualquier número de dimensiones.  Un concepto relacionado, en el que se fuerza a que el resultado corresponda a un miembro de la muestra, es el medoide.  No existe una notación estándar ampliamente aceptada para la mediana, pero algunos autores representan la mediana de una variable x bien como x͂ o como μ1/2[2] a veces también M. [4][5] En cualquiera de estos casos, el uso de estos u otros símbolos para la mediana debe definirse explícitamente cuando se introducen.  La mediana es un caso especial de otras formas de resumir los valores típicos asociados a una distribución estadística: es el 2º cuartil, el 5º decil y el 50º percentil.  Usos La mediana se puede utilizar como una medida de localización cuando uno concede poca importancia a los valores extremos, normalmente porque una distribución es skewed, los valores extremos no son conocidos, o outliers son poco fiables, es decir, pueden ser errores de medición/transcripción.  Por ejemplo, consideremos el multiconjunto  La mediana es 2 en este caso, al igual que la moda, y podría verse como una mejor indicación de la centro que la media aritmética de 4, que es mayor que todos los valores menos uno.  Sin embargo, la relación empírica ampliamente citada de que la media se desplaza ""más hacia la cola"" de una distribución que la mediana no suele ser cierta.  Como mucho, se puede decir que las dos estadísticas no pueden estar ""demasiado lejos"".[6]  Como la mediana se basa en los datos medios de un conjunto, no es necesario conocer el valor de los resultados extremos para calcularla. Por ejemplo, en una prueba de psicología en la que se investiga el tiempo necesario para resolver un problema, si un pequeño número de personas no consigue resolver el problema en absoluto en el tiempo dado, se puede calcular la mediana.[7]  Debido a que la mediana es sencilla de entender y fácil de calcular, a la vez que una aproximación robusta a la  media, la mediana es una estadística de resumen popular en estadística descriptiva.  En este contexto, hay varias opciones para una medida de variabilidad: el rango, el rango intercuartílico, la desviación media y la desviación absoluta mediana.  A efectos prácticos, las distintas medidas de localización y dispersión suelen compararse en función de lo bien que pueden estimarse los valores poblacionales correspondientes a partir de una muestra de datos. La mediana, estimada a partir de la mediana muestral, tiene buenas propiedades en este sentido. Aunque no suele ser óptima si se supone una distribución poblacional determinada, sus propiedades son siempre razonablemente buenas. Por ejemplo, una comparación de la eficiencia de los estimadores candidatos muestra que la media muestral es más eficiente estadísticamente cuando y sólo cuando- los datos no están contaminados por datos de distribuciones de colas pesadas o de mezclas de distribuciones.  Incluso entonces, la mediana tiene una eficiencia del 64% en comparación con la media de varianza mínima (para muestras normales grandes), lo que equivale a decir que la varianza de la mediana será ~50% mayor que la varianza de la media.[8][9]  Métodos de cálculo Existen dos métodos para el cálculo de la mediana: "
matematicas,"En ingeniería, ciencia, industria y estadística, se denomina exactitud a la capacidad de un instrumento de acercarse al valor de la magnitud real. La exactitud es diferente de la precisión.  La exactitud depende de los errores sistemáticos que intervienen en la medición, denotando la proximidad de una medida al verdadero valor y, en consecuencia, la validez de la medida.[1][2]  Suponiendo varias mediciones, no estamos midiendo el error de cada una, sino la distancia a la que se encuentra la medida real de la media de las mediciones (cuán calibrado está el aparato de medición).  Esta cualidad también se encuentra en instrumentos generadores de magnitudes físicas, siendo en este caso la capacidad del instrumento de acercarse a la magnitud física real.  Exactitud es la cercanía del valor experimental obtenido, con el valor exacto de dicha medida. El valor exacto de una magnitud física es un concepto utópico, ya que es imposible conocerlo sin incertidumbre alguna.  Por ejemplo, si leemos la velocidad del velocímetro de un auto, esta tiene una precisión de 3 cifras significativas y una exactitud de 5 km/h.  Desde la aparición de la Guia para expresar incertidumbres de medida (GUM: JCGM-100 de la BIPM) algunos de los conceptos anteriores han sido revisados y el ""error absoluto"" no se considera medible ni estimable, desaconsejándose el uso de dicho término, pues el mensurando en ninguna manera puede ser conocido. "
matematicas,"En matemáticas, polinomio (del latín: polynomium, y este del griego: πολυς, polys, ‘muchos’ y νόμος, nómos, ‘regla’, ‘prescripción’, ‘distribución’)[1][2][3] es una expresión algebraica formada por la suma de varios monomios o términos, cada uno de los cuales es el producto de:  En cada término, cada variable puede aparecer más de una vez, en tal caso se representa por medio de una potencia, como en      x  3   = x ⋅ x ⋅ x   {\displaystyle x^{3}=x\cdot x\cdot x}  . Cada uno de los términos del polinomio tiene asociado un número natural llamado grado, igual a la suma de los exponentes de sus variables (p.e. el monomio     5 x  y  2     {\displaystyle 5xy^{2}}   tiene grado 3). Se llama grado del polinomio al mayor de los grados de sus términos.  Es frecuente el término polinómico (ocasionalmente también el anglicismo polinomial), como adjetivo, para designar cantidades que se pueden expresar como polinomios de algún parámetro, como por ejemplo: tiempo polinómico, etc.  Los polinomios son objetos muy utilizados en matemáticas y en ciencias. En la práctica, son utilizados en cálculo y análisis matemático para aproximar cualquier función derivable; las ecuaciones polinómicas y las funciones polinómicas tienen aplicaciones en una gran variedad de problemas, desde la matemática elemental y el álgebra hasta áreas como la física, química, economía y las ciencias sociales.  En álgebra abstracta, los polinomios son utilizados para construir los anillos de polinomios, un concepto central en teoría de números algebraicos y geometría algebraica.  Definición algebraica Los polinomios están constituidos por un conjunto finito de variables (llamadas incógnitas) y constantes (llamadas coeficientes), con las operaciones aritméticas de suma, resta y multiplicación, así como también exponentes enteros positivos. Pueden ser de una o de varias variables.[4]  Polinomios de una variable Para      a  0   ,  … ,   a  n     {\displaystyle a_{0},\;\ldots ,\;a_{n}}   constantes en algún anillo A (en particular podemos tomar un cuerpo, como      R    {\displaystyle \mathbb {R} }   o      C    {\displaystyle \mathbb {C} }  , en cuyo caso los coeficientes del polinomio serán números) con an distinto de cero y     n ∈  N    {\displaystyle n\in \mathbb {N} }  , entonces un polinomio      P        {\displaystyle P_{}^{}}   de grado n en la variable x es un objeto de la forma:      P ( x  )      =   {\displaystyle P(x)_{}^{}=}        a  n    x  n   +  a  n − 1    x  n − 1   + ⋯ +  a  1    x  1   +  a  0    x  0   .   {\displaystyle a_{n}x^{n}+a_{n-1}x^{n-1}+\cdots +a_{1}x^{1}+a_{0}x^{0}.}    Un polinomio     P ( x ) ∈ K [ x ]   {\displaystyle P(x)\in K[x]}   no es más que una sucesión matemática finita       {   a  n    }   n     {\displaystyle \left\{{a_{n}}\right\}_{n}}   tal que      a  n   ∈ K   {\displaystyle a_{n}\in K}  . También puede considerarse una sucesión infinita     {  a  n    }  n = 1   ∞     {\displaystyle \{a_{n}\}_{n=1}^{\infty }}    entendiendo que a partir de un cierto término      n  0   ∈  N    {\displaystyle n_{0}\in \mathbb {N} }   podemos considerar      a  n   = 0   {\displaystyle a_{n}=0}   para cada     n ≥  n  0     {\displaystyle n\geq n_{0}}  .[5]    Representado como:      P ( x  )      =  a  0   +  a  1   x +  a  2    x  2   + . . . +  a  n    x  n     {\displaystyle P(x)_{}^{}=a_{0}+a_{1}x+a_{2}x^{2}+...+a_{n}x^{n}}    el polinomio se puede escribir más concisamente usando sumatorios como:      P ( x ) =  ∑  i = 0   n    a  i    x  i   .   {\displaystyle P(x)=\sum _{i=0}^{n}a_{i}x^{i}.}    Las constantes a0, …, an se llaman los coeficientes del polinomio. A a0 se le llama el coeficiente constante (o término independiente) y a an, el coeficiente principal (o coeficiente director). Cuando el coeficiente principal es 1, al polinomio se le llama mónico o normalizado.  Polinomios de varias variables Como ejemplo de polinomios de dos variables, desarrollando los binomios:  (2)      {    ( x + y  )  2   =  x  2   + 2 x y +  y  2       ( x + y  )  3   =  x  3   + 3  x  2   y + 3 x  y  2   +  y  3       ( x + y  )  4   =  x  4   + 4  x  3   y + 6  x  2    y  2   + 4 x  y  3   +  y  4           {\displaystyle {\begin{cases}(x+y)^{2}=x^{2}+2xy+y^{2}\\(x+y)^{3}=x^{3}+3x^{2}y+3xy^{2}+y^{3}\\(x+y)^{4}=x^{4}+4x^{3}y+6x^{2}y^{2}+4xy^{3}+y^{4}\end{cases}}}    Estos polinomios son mónicos, homogéneos, simétricos y sus coeficientes son coeficientes binomiales.  Para obtener la expansión de las potencias de una resta (véase productos notables), basta con tomar -y en lugar de y en el caso anterior. La expresión (2) queda de la siguiente forma:  Los polinomios de varias variables, a diferencia de los de una variable, tienen en total más de una variable. Por ejemplo los monomios:  En detalle el último de ellos     4 x  y    2   z   {\displaystyle 4xy_{}^{2}z}   es un monomio de tres variables (ya que en él aparecen las tres letras x, y y z), el coeficiente es 4, y los exponentes son 1, 2 y 1 de x, y y z respectivamente.  Grado de un polinomio Se define el grado de un monomio como la suma de los  exponentes de las variables que la componen. El grado de un polinomio es el del monomio de mayor grado, y se denota por      gr  ( p )   {\displaystyle {\text{gr}}(p)}  .  Convencionalmente se define el grado del polinomio nulo como      − ∞    {\displaystyle \scriptstyle -\infty }  .  En particular los números son polinomios de grado cero.  Polinomio nulo o cero Es el polinomio que tiene todos sus coeficientes cero y 0, tiene grado      − ∞    {\displaystyle \scriptstyle -\infty }   . Actúa de elemento neutro aditivo: p(x) + 0 = 0+ p(x)= p(x), para cualquier p(x).  Polinomio de grado cero Es aquel que no lleva la indeterminada. Son los elementos no nulos de conjuntos numéricos correspondientes.  Operaciones con polinomios Los polinomios se pueden sumar y restar agrupando los términos y simplificando los términos semejantes. Para multiplicar polinomios se multiplica cada término de un polinomio por cada uno de los términos del otro polinomio y luego se simplifican los términos semejantes.  Sean los polinomios:     P ( x ) = ( 2  x    3   + 4 x + 1 )   {\displaystyle P(x)=(2x_{}^{3}+4x+1)}   y     Q ( x  )      = ( 5  x  2   + 3 )   {\displaystyle Q(x)_{}^{}=(5x^{2}+3)}  , entonces el producto es:  Para poder realizar eficazmente la multiplicación de polinomios, se tiene que ordenar cada polinomio de forma decreciente según el grado de sus términos. Una fórmula analítica que expresa el producto de dos polinomios es la siguiente:      P ( x ) Q ( x  )      =   {\displaystyle P(x)Q(x)_{}^{}=}        (   ∑  i = 0   m    a  i    x  i    )   (   ∑  j = 0   n    b  j    x  j    )  =   {\displaystyle \left(\sum _{i=0}^{m}a_{i}x^{i}\right)\left(\sum _{j=0}^{n}b_{j}x^{j}\right)=}        ∑  k = 0   m + n    (   ∑  p = 0   k    a  p    b  k − p    )   x  k     {\displaystyle \sum _{k=0}^{m+n}\left(\sum _{p=0}^{k}a_{p}b_{k-p}\right)x^{k}}    Aplicando esta fórmula al ejemplo anterior se tiene:      P ( x ) Q ( x  )      =   {\displaystyle P(x)Q(x)_{}^{}=}       ( 2  x    3   + 4 x + 1 ) ( 5  x  2   + 3 ) =   {\displaystyle (2x_{}^{3}+4x+1)(5x^{2}+3)=}       ( 1 ⋅ 3 )  x    0   + ( 4 ⋅ 3 )  x  1   + ( 1 ⋅ 5 )  x  2   + ( 4 ⋅ 5 + 2 ⋅ 3 )  x  3   + ( 0 )  x  4   + ( 5 ⋅ 2 )  x  5   =   {\displaystyle (1\cdot 3)x_{}^{0}+(4\cdot 3)x^{1}+(1\cdot 5)x^{2}+(4\cdot 5+2\cdot 3)x^{3}+(0)x^{4}+(5\cdot 2)x^{5}=}       10  x    5   + 26  x  3   + 5  x  2   + 12 x + 3   {\displaystyle 10x_{}^{5}+26x^{3}+5x^{2}+12x+3}    Puede comprobarse que para polinomios no nulos se satisface la siguiente relación entre el grado de los polinomios      P ( X )    {\displaystyle \scriptstyle P(X)}   y      Q ( X )    {\displaystyle \scriptstyle Q(X)}   y el polinomio producto      P ( X ) Q ( X )    {\displaystyle \scriptstyle P(X)Q(X)}  :  (*)      gr   ( P ( x ) Q ( x ) ) =   gr   ( P ( x ) ) +   gr   ( Q ( x ) )    {\displaystyle {\mbox{gr}}(P(x)Q(x))={\mbox{gr}}(P(x))+{\mbox{gr}}(Q(x))\,}    Puesto que el producto de cualquier polinomio por el polinomio nulo es el propio polinomio nulo, se define convencionalmente que        gr   ( 0 ) = − ∞    {\displaystyle \scriptstyle {\mbox{gr}}(0)=-\infty }   (junto con la operación     ∀ p : − ∞ + p = − ∞   {\displaystyle \forall p:-\infty +p=-\infty }  ) por lo que la expresión puede extenderse también al caso de que alguno de los polinomios sea nulo.  Funciones polinómicas Una función polinómica es una función matemática expresada mediante un polinomio. Dado un polinomio P[x] se puede definir una función polinómica asociada al polinomio dado substituyendo la variable x por un elemento del anillo:       f  P   : A → A ,   a ∈ A ↦  f  P   ( a ) =  a  n    a  n   +  a  n − 1    a  n − 1   + ⋯ +  a  1   a +  a  0   ∈ A   {\displaystyle f_{P}:A\to A,\qquad \qquad a\in A\mapsto f_{P}(a)=a_{n}a^{n}+a_{n-1}a^{n-1}+\dots +a_{1}a+a_{0}\in A}    Las funciones polinómicas reales son funciones suaves, es decir, son infinitamente diferenciables (tienen derivadas de todos los órdenes). Debido a su estructura simple, las funciones polinómicas son muy sencillas de evaluar numéricamente, y se usan ampliamente en análisis numérico para interpolación polinómica o para integrar numéricamente funciones más complejas. Una manera muy eficiente para evaluar polinomios es la utilización de la regla de Horner.  En álgebra lineal el polinomio característico de una matriz cuadrada codifica muchas propiedades importantes de la matriz. En teoría de los grafos el polinomio cromático de un grafo codifica las distintas maneras de colorear los vértices del grafo usando x colores.  Con el desarrollo de la computadora, los polinomios han sido remplazados por funciones spline en muchas áreas del análisis numérico. Las splines se definen a partir de polinomios y tienen mayor flexibilidad que los polinomios ordinarios cuando definen funciones simples y suaves. Estas son usadas en la interpolación spline y en gráficos por computadora.  Ejemplos de funciones polinómicas Note que las gráficas representan a las funciones polinómicas y no a los polinomios en sí, pues un polinomio solo es la suma de varios monomios."
matematicas,"En matemáticas, el teorema de Pitágoras es una relación en geometría euclidiana entre los tres lados de un triángulo rectángulo. Afirma que el área del cuadrado cuyo lado es la hipotenusa (el lado opuesto al ángulo recto) es igual a la suma de las áreas de los cuadrados cuyos lados son los catetos (los otros dos lados que no son la hipotenusa). Este teorema se puede escribir como una ecuación que relaciona las longitudes de los lados 'a', 'b' y 'c'. Es la proposición más conocida entre las que tienen nombre propio en la matemática.[1] El teorema de Pitágoras establece que, en todo triángulo rectángulo, la longitud de la hipotenusa es igual a la raíz cuadrada de la suma del área de los cuadrados de las respectivas longitudes de los catetos.  Si en un triángulo rectángulo hay catetos de longitud     a    {\displaystyle a\,}   y     b    {\displaystyle b\,}  , y la medida de la hipotenusa es     c    {\displaystyle c\,}  , entonces se cumple la siguiente relación:  (1)     a  2   +  b  2   =  c  2      {\displaystyle a^{2}+b^{2}=c^{2}\,}    De esta ecuación se deducen tres corolarios de verificación algebraica y aplicación práctica:   El teorema de Pitágoras se ha demostrado en numerosas ocasiones por muchos métodos diferentes, posiblemente el mayor número de teoremas matemáticos. Las pruebas son diversas, e incluyen tanto pruebas geométricas como algebraicas, y algunas se remontan a miles de años atrás.   El teorema se puede generalizar de varias maneras: a espacios de mayor dimensión, a espacios que no son euclidianos, a objetos que no son triángulos rectos y a objetos que no son triángulos en absoluto, sino sólidos n. El teorema de Pitágoras ha despertado interés fuera de las matemáticas como símbolo de abstracción matemática, mística o poder intelectual; abundan las referencias populares en la literatura, obras de teatro, musicales, canciones, sellos y dibujos animados.  Historia El teorema de Pitágoras fue comprobado en el siglo VI a. C. por el filósofo y matemático griego Pitágoras, pero se estima que pudo haber sido previo a su existencia, o demostrado bajo otra denominación.  Respecto de los babilonios hay esta nota:   Existe un debate sobre si el teorema de Pitágoras se descubrió una vez, o muchas veces en muchos lugares, y la fecha del primer descubrimiento es incierta, al igual que la fecha de la primera demostración. Los historiadores de las matemáticas mesopotámicas han llegado a la conclusión de que la regla pitagórica tuvo un uso generalizado durante el período babilónico antiguo (siglo XX al siglo VI a. C.), más de mil años antes del nacimiento de Pitágoras.[3][4][5][6]   El teorema de Pitágoras tiene este nombre porque su demostración, sobre todo, es esfuerzo de la escuela pitagórica. Anteriormente, en Mesopotamia y el Antiguo Egipto se conocían ternas de valores que se correspondían con los lados de un triángulo rectángulo, y se utilizaban para resolver problemas referentes a los citados triángulos, tal como se indica en algunas tablillas y papiros. Sin embargo, no ha perdurado ningún documento que exponga teóricamente su relación.[7] La pirámide de Kefrén, datada en el siglo XXVI a. C., fue la primera gran pirámide que se construyó basándose en el llamado triángulo sagrado egipcio, de proporciones 3-4-5.  Designaciones convencionales   Demostraciones El teorema de Pitágoras es de los que cuentan con un mayor número de demostraciones diferentes, utilizando métodos muy diversos. Una de las causas de esto es que en la Edad Media se exigía una nueva demostración del teorema para alcanzar el grado de ""Magíster matheseos"".[cita requerida]  Algunos autores proponen hasta más de mil demostraciones. Por ejemplo, el matemático estadounidense E. S. Loomis catalogó 367 pruebas diferentes en su libro de 1927 The Pythagorean Proposition.[cita requerida]  En ese mismo libro, Loomis clasificaría las demostraciones en cuatro grandes grupos: las algebraicas, donde se relacionan los lados y segmentos del triángulo; las geométricas, en las que se realizan comparaciones de áreas; las dinámicas, a través de las propiedades de fuerza, masa, y las cuaterniónicas, mediante el uso de vectores.[cita requerida]  China: El Zhoubi Suanjing y el Jiuzhang Suanshu El Zhoubi Suanjing es una obra matemática de datación discutida en algunos lugares, aunque se acepta mayoritariamente que se escribió entre el 500 y el 300 a. C. Se cree que Pitágoras no conoció esta obra. En cuanto al Jiuzhang Suanshu, parece que es posterior; está fechado en torno al año 250 a. C.  El Zhou Bi demuestra el teorema construyendo un cuadrado de lado (a+b) que se parte en cuatro triángulos de base a y altura b, y un cuadrado de lado c.  Sea el triángulo rectángulo de catetos a y b e hipotenusa c. Se trata de demostrar que el área del cuadrado de lado c es igual a la suma de las áreas de los cuadrados de lado a y lado b. Es decir:   Si añadimos tres triángulos iguales al original dentro del cuadrado de lado c formando la figura mostrada en la imagen, obtenemos un cuadrado de menor tamaño. Se puede observar que el cuadrado resultante tiene efectivamente un lado de b - a. Luego, el área de este cuadrado menor puede expresarse de la siguiente manera:  Ya que     ( b − a  )  2   = ( a − b  )  2      {\displaystyle (b-a)^{2}=(a-b)^{2}\,}  .  Es evidente que el área del cuadrado de lado c es la suma del área de los cuatro triángulos de altura a y base b que están dentro de él más el área del cuadrado menor:  Con lo cual queda demostrado el teorema.  Demostraciones supuestas de Pitágoras Se estima que se demostró el teorema mediante semejanza de triángulos: sus lados homólogos son proporcionales.[8]  Sea el triángulo ABC, rectángulo en C. El segmento CH es la altura relativa a la hipotenusa, en la que determina los segmentos a’ y b’, proyecciones en ella de los catetos a y b, respectivamente.  Los triángulos rectángulos ABC, AHC y BHC tienen sus tres bases iguales: todos tienen dos bases en común, y los ángulos agudos son iguales bien por ser comunes, bien por tener sus lados perpendiculares. En consecuencia, dichos triángulos son semejantes.  y dos triángulos son semejantes si hay dos o más ángulos congruentes.  Los resultados obtenidos son el teorema del cateto.  Sumando:   Pero      (   a ′  +  b ′   )  =   c   {\displaystyle \left(a'+b'\right)=\ c}  , por lo que finalmente resulta:   Pitágoras también pudo haber demostrado el teorema basándose en la relación entre las superficies de figuras semejantes.   Los triángulos PQR y PST son semejantes, de manera que:   siendo r la razón de semejanza entre dichos triángulos. Si ahora buscamos la relación entre sus superficies:  obtenemos después de simplificar que:   pero siendo       r u   =   s v   = r   {\displaystyle {\frac {r}{u}}={\frac {s}{v}}=r}   la razón de semejanza, está claro que:   Es decir, ""la relación entre las superficies de dos figuras semejantes es igual al cuadrado de la razón de semejanza"".  Aplicando ese principio a los triángulos rectángulos semejantes ACH y BCH tenemos que:   que de acuerdo con las propiedades de las proporciones da:   y por la semejanza entre los triángulos ACH y ABC resulta que:   pero según (I)        S  A C H    b  2     =     S  A C H   +  S  B C H      b  2   +  a  2        {\displaystyle {\frac {S_{ACH}}{b^{2}}}={\frac {S_{ACH}+S_{BCH}}{b^{2}+a^{2}}}}  , así que:   y por lo tanto:   quedando demostrado el teorema de Pitágoras.  Es asimismo posible que Pitágoras hubiera obtenido una demostración gráfica del teorema.  Partiendo de la configuración inicial, con el triángulo rectángulo de lados a, b, c, y los cuadrados correspondientes a catetos e hipotenusa –izquierda-, se construyen dos cuadrados diferentes:  Si a cada uno de estos cuadrados les quitamos los triángulos, evidentemente el área del cuadrado gris (     c  2     {\displaystyle c^{2}}  ) equivale a la de los cuadrados amarillo y azul (     b  2   +  a  2     {\displaystyle b^{2}+a^{2}}  ), habiéndose demostrado el teorema de Pitágoras.    Demostración de Euclides: proposición I.47 de Los Elementos El descubrimiento de los números irracionales por Pitágoras y los Pitagóricos supuso un contratiempo muy serio.[11] De pronto, las proporciones dejaron de tener validez universal, no siempre podían aplicarse. La demostración de Pitágoras de su teorema se basaba muy probablemente en proporciones, y una proporción es un número racional. ¿Sería realmente válida como demostración? Ante esto, Euclides elabora una demostración nueva que elude la posibilidad de encontrarse con números irracionales.  El eje de su demostración es la proposición I.47[12] de Los Elementos:   En los triángulos rectángulos el cuadrado del lado opuesto al ángulo recto es igual a la suma de los cuadrados de los lados que comprenden el ángulo recto.   Basándose en la proposición I.41[9] de Los Elementos, que equivale a decir que a igual base y altura, el área del paralelogramo dobla a la del triángulo, (véase Figura Euclides 1).  Se tiene el triángulo ABC, rectángulo en C (véase Figura Euclides 3), y se construye los cuadrados correspondientes a catetos e hipotenusa. La altura CH se prolonga hasta J. Seguidamente se traza cuatro triángulos, iguales dos a dos:  Abundando en las anteriores consideraciones, nótese que un giro con centro en A, y sentido positivo, transforma ABD en ACK. Y un giro con centro en B, y sentido también positivo, transforma ABG en CBI. En la demostración de Leonardo da Vinci se encontrará nuevamente con giros que demuestran la igualdad de figuras.  Véase (en la Figura Euclides 3) que:  Pero siendo ACK=ABD, resulta que el rectángulo AHJK y el cuadrado ADEC tienen áreas equivalentes.  Haciendosé razonamientos similares con los triángulos ABg y CBI, respecto al cuadrado BCFG y al rectángulo HBIJ respectivamente, se concluye que estos últimos tienen asimismo áreas iguales.  A partir de lo anterior, surge de inmediato que: «la suma de las áreas de los cuadrados construidos sobre los catetos, es igual al área del cuadrado construido sobre la hipotenusa».  Demostración de Pappus Unos 625 años después que Euclides, Pappus[13] parece seguir su senda, y desarrolla una demostración del teorema de Pitágoras basada en la proposición I.36[10] de Los Elementos de Euclides:  Partimos del triángulo ABC rectángulo en C, sobre cuyos catetos e hipotenusa hemos construido los cuadrados correspondientes.   Prolongando CH hacia arriba se obtiene el rectángulo CEGI cuya diagonal CG determina en aquel dos triángulos rectángulos iguales al triángulo ABC dado:   En consecuencia los triángulos rectángulos ABC, ICG y EGC tienen sus tres lados iguales.   De 1) y 2) se sigue que las superficies de ACED y AHMN son iguales.  Análogamente:  De dónde se deduce la equivalencia de las superficies de BLMH y de CIKB.  El teorema de Pitágoras queda demostrado.  Demostración de Bhaskara Bhaskara II, el matemático y astrónomo hindú del siglo XII, dio la siguiente demostración del teorema de Pitágoras.  Con cuatro triángulos rectángulos de lados a, b y c se construye el cuadrado de lado c –izquierda-, en cuyo centro se forma otro cuadrado de lado (a-b).   Redistribuyendo los cuatro triángulos y el cuadrado de lado (a-b), construimos la figura de la derecha, cuya superficie resulta ser la suma de la de dos cuadrados: uno de lado a –azul- y otro de lado b -naranja-.  Se ha demostrado gráficamente que      c  2   =  a  2   +  b  2     {\displaystyle c^{2}=a^{2}+b^{2}}    Algebraicamente: el área del cuadrado de lado c es la correspondiente a los cuatro triángulos, más el área del cuadrado central de lado (a-b), es decir:  expresión que desarrollada y simplificada nos da el resultado      c  2   =  a  2   +  b  2     {\displaystyle c^{2}=a^{2}+b^{2}}  , y el teorema queda demostrado.  Demostración de Leonardo Da Vinci En el elenco de inteligencias que abordaron el teorema de Pitágoras no falta el genio del Renacimiento, Leonardo da Vinci.   Partiendo del triángulo rectángulo ABC con los cuadrados de catetos e hipotenusa, Leonardo añade los triángulos ECF y HIJ, iguales al dado, resultando dos polígonos, cuyas superficies va a demostrar que son equivalentes:  "
matematicas,"Un cateto, en geometría, es cualquiera de los dos lados menores de un triángulo rectángulo, los que conforman el ángulo recto. Su nombre proviene del latín cathetus, préstamo del griego κάθετος, káthetos ('vertical, perpendicular').  El lado de mayor medida se denomina hipotenusa, el que es opuesto al ángulo recto. La denominación de catetos e hipotenusa se aplica a los lados de los triángulos rectángulos exclusivamente.  Propiedades de los catetos Teorema de Pitágoras El cuadrado de la longitud de la hipotenusa es igual a la suma del cuadrado de las longitudes de los catetos.  En la figura, los lados a y b son los catetos y c la hipotenusa. Veámoslo con un ejemplo:   Imaginemos que el lado a mide 5 cm y el lado b mide 4 cm y se quiere calcular la hipotenusa (el lado c). Entonces se haría:  El valor de la hipotenusa sería igual a la raíz cuadrada de 41.  Proyecciones ortogonales El cuadrado de la longitud de un cateto es igual al producto de su proyección ortogonal sobre la hipotenusa por la longitud de ésta.  Es decir, la longitud de un cateto a es media proporcional entre las longitudes de su proyección n y la de la hipotenusa c.  En la figura, la hipotenusa es el lado c y los catetos son los lados a y b. La proyección ortogonal de a' es n, y la de b es m.  Razones trigonométricas Mediante razones trigonométricas se puede obtener el valor de los ángulos agudos del triángulo rectángulo. Respecto de un ángulo, un cateto se denomina adyacente o contiguo, si conforma el ángulo junto con la hipotenusa, y opuesto si no forma parte del ángulo dado.  Conocida la longitud de los catetos     b    {\displaystyle b\,}   y     a    {\displaystyle a\,}  , la razón entre ambos es:   por tanto, la función trigonométrica inversa es la siguiente:  siendo     β    {\displaystyle \beta \,}   el valor del ángulo opuesto al cateto     b    {\displaystyle b\,}  .  El ángulo opuesto al cateto     a    {\displaystyle a\,}  , denominado     α    {\displaystyle \alpha \,}  , tendrá el valor:"
matematicas,"En matemática (inicialmente estudiado en geometría elemental y, de forma más rigurosa, en geometría diferencial), la curva (o línea curva) es una línea continua de una dimensión, que varía de dirección paulatinamente. Ejemplos sencillos de curvas cerradas simples son la elipse o la circunferencia o el óvalo, el cicloide; ejemplos de curvas abiertas, la parábola, la hipérbola y la catenaria y una infinidad de curvas estudiadas en la geometría analítica plana. La recta asume el caso límite de una circunferencia de radio de curvatura infinito y de curvatura 0; además, una recta es la imagen homeomorfa de un intervalo abierto.[1] Todas las curvas tienen dimensión topológica igual a 1. La noción curva, conjuntamente con la de superficie, es uno de los objetos primordiales de la geometría diferencial, ciertamente con profusa aplicación de las herramientas del cálculo diferencial.[2]  Historia y definiciones Camille Jordan (1838-1922) propuso una teoría sobre las curvas basada en la definición de una curva en términos de puntos variables (ver teorema de la curva de Jordan). En geometría, una curva en el n-espacio euclidiano es un conjunto       C   ⊂   R   n     {\displaystyle {\mathcal {C}}\subset \mathbb {R} ^{n}}   que es la imagen de un intervalo Ι abierto bajo una aplicación continua      x  :  I  →   R   n     {\displaystyle \mathbf {x} \colon \mathrm {I} \to \mathbb {R} ^{n}}  , es decir:        C   = {  x  ( t ) ∈   R   n   : t ∈  I  }   {\displaystyle {\mathcal {C}}=\{\mathbf {x} (t)\in \mathbb {R} ^{n}\colon t\in \mathrm {I} \}}    donde suele decirse que (     x  ,  I    {\displaystyle \mathbf {x} ,\mathrm {I} }  ) es una representación paramétrica o parametrización de       C     {\displaystyle {\mathcal {C}}}  .  Curva, en el plano o en el espacio tridimensional, es la imagen de un camino γ, que se considera con derivada continua a trozos en el intervalo de definición .[4]   Métodos de expresión de una curva plana     ρ = f ( ϕ )   {\displaystyle \rho =f(\phi )}  … Ejemplo:     ρ = a ⋅ ϕ   {\displaystyle \rho =a\cdot \phi }  . Espiral de Arquímedes [5]  Curva elemental Un conjunto      γ    {\displaystyle \mathbb {\gamma } }   de puntos del espacio se denominará curva elemental si es la imagen obtenida en el espacio por una aplicación topológica [6] de un segmento abierto de recta.[7]  Sea γ una curva elemental y sea a < t < b el segmento abierto del que se obtiene la aplicación f de la curva correspondiente al punto t del segmento. El sistema de igualdades       x =  f  1   ( t ) , y =  f  2   ( t ) , z =  f  3   ( t )   {\displaystyle x=f_{1}(t),y=f_{2}(t),z=f_{3}(t)}    constituyen ecuaciones de la curva      γ    {\displaystyle \mathbb {\gamma } }   en forma paramétrica. [7]  Curva simple La curva, según esta definición, pueden ser muy intrincadas, de muy diverso tipo. Con el objetivo de evitar auto intersecciones, puntos singulares y a los extremos, se define el concepto de curva simple como aquella curva tal que para todo punto p existe un Ω entorno abierto de p para el cual     Ω ∩   C     {\displaystyle \Omega \cap {\mathcal {C}}}   admite una representación de clase      C  k     {\displaystyle C^{k}}   con     k ≥ 1   {\displaystyle k\geq 1}  .  La definición de Jordan ha sido cuestionada a partir del descubrimiento del italiano Giuseppe Peano. Este matemático demostró en 1890 que un cuadrado relleno entra dentro de la definición de Jordan, pues logró representar todos los puntos del mismo utilizando dicha definición: trazó todos los puntos del cuadrado con una única curva. Pero es claro que un cuadrado no es, en el sentido convencional del término, una curva. Debido a ello, y al descubrimiento posterir de otros casos similares a los de Peano, se ha planteado la necesidad de mejorar la definición de la definición de lo que es, matemáticamente, una curva.[3]  Un conjunto      δ    {\displaystyle \mathbb {\delta } }   de puntos del espacio se denominara curva simple si es conjunto conexo y si para todo punto     W   {\displaystyle W}   del mismo existe un entorno tal que la parte de      δ    {\displaystyle \mathbb {\delta } }  , comprendida en él, forma una curva elemental.[7]  Curva plana Una curva plana es aquella que reside en un solo plano y puede ser abierta o cerrada. La representación gráfica de una función real de una variable real es una curva plana.[8]  Curva diferenciable Una curva se llama diferenciable cuando la función      x  : [ a , b ] ⊂  I  →   R   n     {\displaystyle \mathbf {x} \colon [a,b]\subset \mathrm {I} \to \mathbb {R} ^{n}}   es diferenciable. Si además la función anterior es inyectiva en el intervalo     ( a , b )    {\displaystyle (a,b)\,}   entonces la curva admite un vector tangente único en cada punto y es rectificable (lo cual significa que su longitud de arco está bien definida y es posible calcular su longitud. La curva      x  : [ 0 , ∞ ) →   R   n     {\displaystyle \mathbf {x} \colon [0,\infty )\to \mathbb {R} ^{n}}   :       x  ( t ) =   {    ( t , t sin ⁡  (   1 t   )  )   t > 0     ( 0 , 0 )   t = 0         {\displaystyle \mathbf {x} (t)={\begin{cases}(t,t\sin \left({\frac {1}{t}}\right))&t>0\\(0,0)&t=0\end{cases}}}    es continua pero no diferenciable, por lo que su longitud entre el punto (0,0) y cualquier otro punto de la misma no puede calcularse.  Curva cerrada Una curva diferenciable es cerrada cuando      x  : [ a , b ] →   R   n     {\displaystyle \mathbf {x} \colon [a,b]\to \mathbb {R} ^{n}}   cuando      x  ( a ) =  x  ( b )   {\displaystyle \mathbf {x} (a)=\mathbf {x} (b)}  . Si además, la función      x    {\displaystyle \mathbf {x} }   es inyectiva en el intervalo     ( a , b )    {\displaystyle (a,b)\,}   entonces se dice que la curva es una curva cerrada simple. Una curva cerrada simple es homeomorfa al círculo      S  1     {\displaystyle S^{1}}  , es decir, tiene la misma topología de un anillo. La curva      x  : [ 0 , 1 ] →   R   n     {\displaystyle \mathbf {x} \colon [0,1]\to \mathbb {R} ^{n}}   dada por:       x  ( t ) = ( a cos ⁡ ( 2 π t ) , b sin ⁡ ( 2 π t ) )   {\displaystyle \mathbf {x} (t)=(a\cos(2\pi t),b\sin(2\pi t))}    es una curva diferenciable cerrada, de hecho dicha curva resulta ser una elipse de semiejes a y b.  Se llama curva cerrada a aquella curva simple homeomorfa con una circunferencia.[9] Se llama entorno de un punto W de una curva simple δ la parte común de la curva δ y un entorno espacial del punto W. Por tanto , todo punto de una curva simple posee un entorno que conforma una curva elemental.[9]  Curva suave Se le llama curva suave a la curva que no posee puntos angulosos. Un ejemplo puede ser el círculo, la elipse, la parábola, etc. Una curva que no es suave puede ser, por ejemplo, una cicloide.[10]  Formalmente, dada una curva C representada por la ecuación paramétrica:  en un intervalo I cualquiera, es suave si sus derivadas son continuas en el intervalo I y no son simultáneamente nulas, excepto posiblemente en los puntos terminales del intervalo.  Una curva C es suave por partes si es suave en todo intervalo de alguna partición de I, es decir que el intervalo puede dividirse en un número finito de subintervalos, en cada uno de los cuales C es suave.  Geometría diferencial de curvas en R3 La geometría diferencial de curvas propone definiciones y métodos para analizar curvas simples en el espacio euclídeo tridimensional o, más generalmente, curvas contenidas en variedades de Riemann. En particular, en el espacio euclídeo tridimensional       R   3     {\displaystyle \mathbb {R} ^{3}}  , una curva de la que se conoce un punto de paso y el vector tangente en dicho punto, queda totalmente descrita por su curvatura y torsión. Esta curvatura y torsión pueden estudiarse mediante el llamado triedro de Frênet-Serret, que se explica a continuación.  Vectores tangente, normal y binormal Dada una curva parametrizada r(t) según un parámetro cualquiera t se define el llamado vector tangente, binormal y normal como:        Estos tres vectores son unitarios y perpendiculares entre sí, juntos configuran un sistema de referencia móvil conocido como triedro de Frênet-Serret. Es interesante que para una partícula física desplazándose en el espacio, el vector tangente es paralelo a la velocidad, mientras que el vector normal da el cambio dirección por unidad de tiempo de la velocidad o aceleración normal.  Curvas no diferenciables Cuando la función que define la curva es diferenciable se dice que la curva es diferenciable. Una curva diferenciable tiene la propiedad de admitir una recta tangente en cada uno de sus puntos. Una curva con un número finito de puntos donde no es diferenciable es una curva diferenciable a tramos. Cuando el número de puntos no es finito puede darse el caso de una curva continua no sea rectificable en ningún punto, eso significa que la tangente no puede definirse en ningún punto. En esos casos la longitud de la curva no es un número finito y puede darse el caso que la curva tenga una longitud infinita aun cuando ocupe una región finita del espacio. La curva de Koch es un ejemplo de curva no rectificable de longitud infinita, que encierra un área finita. De hecho esta curva es un objeto fractal de dimensión fractal:       D  f   =    ln ⁡ 4   ln ⁡ 3    ≈ 1 , 26186 …   {\displaystyle D_{f}={\frac {\ln 4}{\ln 3}}\approx 1,26186\dots }   "
matematicas,"Una hipérbola (del griego ὑπερβολή) es una curva abierta de dos ramas, obtenida cortando un cono recto mediante un plano no necesariamente paralelo al eje de simetría, y con ángulo menor que el de la generatriz respecto del eje de revolución.[1] En geometría analítica, una hipérbola es el lugar geométrico de los puntos de un plano, tales que el valor absoluto de la diferencia de sus distancias a dos puntos fijos, llamados focos, es igual a la distancia entre los vértices, la cual es una constante positiva. Siendo esta constante menor a la distancia entre los focos.  Etimología Hipérbola proviene de la palabra griega ὑπερβολή (exceso), y es cognado del término literario hipérbole.  Historia Según la tradición, las secciones cónicas fueron descubiertas por el geómetra y matemático griego Menecmo (380 A. C.- 320 A. C.), en su estudio del problema de la duplicación del cubo,[2] mediante el cual demostró la existencia de una solución usando el corte de una parábola con una hipérbola, lo cual es confirmado posteriormente por los también geómetras Proclo y Eratóstenes.[3]  Sin embargo, el primero en usar el término hipérbola fue Apolonio de Perge en su tratado Cónicas,[4] considerada la obra cumbre sobre el tema de las matemáticas griegas, y donde se desarrolla el estudio de las tangentes a las secciones cónicas.  Ecuaciones de la hipérbola Ecuaciones canónicas en coordenadas cartesianas La hipérbola cuyo centro se halla en el origen de coordenadas     O ( 0 , 0 )    {\displaystyle O(0,0)\,}  es representable mediante una de las siguientes ecuaciones denominadas de manera común como ecuación canónica o forma normal de la ecuación de una hipérbola:  (1)       x  2    a  2     −    y  2    b  2     = 1 ⇒  b  2    x  2   −  a  2    y  2   =  a  2    b  2     {\displaystyle {\frac {x^{2}}{a^{2}}}-{\frac {y^{2}}{b^{2}}}=1\Rightarrow b^{2}x^{2}-a^{2}y^{2}=a^{2}b^{2}}    o   (2)       y  2    a  2     −    x  2    b  2     = 1 ⇒  b  2    y  2   −  a  2    x  2   =  a  2    b  2     {\displaystyle {\frac {y^{2}}{a^{2}}}-{\frac {x^{2}}{b^{2}}}=1\Rightarrow b^{2}y^{2}-a^{2}x^{2}=a^{2}b^{2}}    En dichas ecuaciones     a   {\displaystyle a}  ,     b   {\displaystyle b}   y     c   {\displaystyle c}  , representan a los semiejes transverso, conjugado y focal, respectivamente. La ecuación (1) representa a las hipérbolas cuyo eje focal es colineal al eje     x   {\displaystyle x}   y la (2) para aquellas que lo son respecto al eje     y   {\displaystyle y}  . En la primera ecuación, los focos están en     F ( ± c , 0 )   {\displaystyle F(\pm c,0)}   y los vértices en     V ( ± a , 0 )   {\displaystyle V(\pm a,0)}  . En la segunda, los focos están en     F ( 0 , ± c )   {\displaystyle F(0,\pm c)}   y los vértices en     V ( 0 , ± a )   {\displaystyle V(0,\pm a)}  . En cualquier caso, la relación entre los tres semiejes viene dada por la igualdad:  (3)     c  2   =  a  2   +  b  2     {\displaystyle c^{2}=a^{2}+b^{2}}    Sin embargo, se debe advertir que, a diferencia del caso de la elipse, no necesariamente     a > b   {\displaystyle a>b}  .  Ecuaciones de una hipérbola con centro en el punto     C ( h , k )   {\displaystyle C(h,k)}   Como en el caso anterior, la ecuación asume una de las siguientes formas:  (4)       ( x − h  )  2     a  2     −    ( y − k  )  2     b  2     = 1   {\displaystyle {\frac {(x-h)^{2}}{a^{2}}}-{\frac {(y-k)^{2}}{b^{2}}}=1}    o   (5)       ( y − k  )  2     a  2     −    ( x − h  )  2     b  2     = 1   {\displaystyle {\frac {(y-k)^{2}}{a^{2}}}-{\frac {(x-h)^{2}}{b^{2}}}=1}    La ecuación (4) corresponde a hipérbolas cuyo eje focal y mayor son paralelos al eje     x   {\displaystyle x}  , en las cuales el vértice se halla en     V ( h ± a , k )   {\displaystyle V(h\pm a,k)}   y los focos en     F ( h ± c , k )   {\displaystyle F(h\pm c,k)}  . La ecuación (5) es la de las hipérbolas cuyo eje focal y mayor son paralelos respecto al eje     y   {\displaystyle y}   en las cuales los vértices están ubicados en     V ( h , k ± a )   {\displaystyle V(h,k\pm a)}   y los focos en     F ( h , k ± c )   {\displaystyle F(h,k\pm c)}  .  Excentricidad La excentricidad     e   {\displaystyle e}   de una hipérbola es un valor definido como:  donde:      c   {\displaystyle c}   representa la mitad de la distancia del eje focal.     a   {\displaystyle a}   representa la mitad de la distancia del eje mayor.  Ya que     c   {\displaystyle c}   es un valor mayor que     a   {\displaystyle a}  , la excentricidad de una hipérbola es siempre mayor que 1.  Ecuación general de la hipérbola La ecuación general de una hipérbola es la siguiente:  (5)    A  x  2   − C  y  2   + D x + E y + F = 0   {\displaystyle Ax^{2}-Cy^{2}+Dx+Ey+F=0}    Si los coeficientes     A   {\displaystyle A}   y     C   {\displaystyle C}   son de signos diferentes, no nulos y     A > 0   {\displaystyle A>0}   y     C > 0   {\displaystyle C>0}  , entonces (5) representa la ecuación general de una hipérbola cuyos ejes son paralelos o colineales a los ejes coordenados o un par de rectas que se cortan. [5]  Demostración En la ecuación (5) son separadas las variables en     x   {\displaystyle x}   y     y   {\displaystyle y}   convirtiendo a     A   {\displaystyle A}   y     C   {\displaystyle C}   en factores comunes:  (6)    A  (   x  2   +   D A   x  )  − C  (   y  2   −   E C   y  )  = − F   {\displaystyle A\left(x^{2}+{\frac {D}{A}}x\right)-C\left(y^{2}-{\frac {E}{C}}y\right)=-F}    Mediante la completación de cuadrados se reescribe la ecuación anterior como:  (7)    A  (   x  2   +   D A   x +    D  2    4  A  2       )  − C  (   y  2   −   E C   y +    E  2    4  C  2       )  =    D  2    4 A    −    E  2    4 C    − F   {\displaystyle A\left(x^{2}+{\frac {D}{A}}x+{\frac {D^{2}}{4A^{2}}}\right)-C\left(y^{2}-{\frac {E}{C}}y+{\frac {E^{2}}{4C^{2}}}\right)={\frac {D^{2}}{4A}}-{\frac {E^{2}}{4C}}-F}    Ahora se convierten los trinomios de la izquierda en binomios notables:  (8)    A   (  x +   D  2 A     )   2   − C   (  y −   E  2 C     )   2   =    D  2    4 A    −    E  2    4 C    − F   {\displaystyle A\left(x+{\frac {D}{2A}}\right)^{2}-C\left(y-{\frac {E}{2C}}\right)^{2}={\frac {D^{2}}{4A}}-{\frac {E^{2}}{4C}}-F}    Se convierte el término de la derecha a una constante denominada     t   {\displaystyle t}  . De acuerdo al valor de     t   {\displaystyle t}  , se presentan los siguientes casos:  Cualquiera que sea el caso, el centro de la hipérbola o el punto de intersección de las dos rectas es siempre     C  (  −   D  2 A    ,   E  2 C     )    {\displaystyle C\left(-{\frac {D}{2A}},{\frac {E}{2C}}\right)}  .  Ecuación de la hipérbola en su forma compleja Una hipérbola en el plano complejo es el lugar geométrico formado por un conjunto de puntos     z    {\displaystyle z\,}  , en el plano     R e I m    {\displaystyle ReIm\,}  ; tales que, cualquiera de ellos satisface la condición geométrica de que el valor absoluto de la diferencia de sus distancias      |  z −  w  1    |  −  |  z −  w  2    |     {\displaystyle |z-w_{1}|-|z-w_{2}|\,}  , a dos puntos fijos llamados focos      w  1      {\displaystyle w_{1}\,}   y      w  2      {\displaystyle w_{2}\,}  , es una constante positiva igual al doble de la distancia (o sea     2 l    {\displaystyle 2l\,}   ) que existe entre su centro y cualesquiera de sus vértices del eje focal:  Evidentemente esta operación se lleva a cabo en el conjunto de los números complejos.  Ecuaciones en coordenadas polares Hipérbola abierta de derecha a izquierda:   Hipérbola abierta de arriba abajo:  Hipérbola abierta de noreste a suroeste:   Hipérbola abierta de noroeste a sureste:  Hipérbola con origen en el foco derecho:      r ( θ ) =    a (  ε  2   − 1 )   1 − ε cos ⁡ θ      {\displaystyle r(\theta )={\frac {a(\varepsilon ^{2}-1)}{1-\varepsilon \cos \theta }}}    Hipérbola con origen en el foco izquierdo:      r ( θ ) =    a (  ε  2   − 1 )   1 + ε cos ⁡ θ      {\displaystyle r(\theta )={\frac {a(\varepsilon ^{2}-1)}{1+\varepsilon \cos \theta }}}    Ecuaciones paramétricas Hipérbola abierta de derecha a izquierda:  o          x = ± a sec ⁡ t + h     y = b tan ⁡ t + k       t ∈  ] − π  /  2 ,  π  /  2 [    (un signo para cada rama)     {\displaystyle {\begin{matrix}x=\pm a\sec t+h\\y=b\tan t+k\\\end{matrix}}\;\;t\in \,]-\pi /2,\;\pi /2[\;{\mbox{(un signo para cada rama)}}}    Más manejable   es la parametrización      r ( t ) = ( a cosh ⁡ ( t ) ,  ± b sinh ⁡ ( t ) ) ,  t ∈   R    {\displaystyle r(t)=(a\cosh(t),\;\pm b\sinh(t)),\;t\in \,\mathbb {R} }   (un signo para cada rama).   En este caso el vértice en cada rama es     r ( 0 )   {\displaystyle r(0)}   y los puntos     r ( t )   {\displaystyle r(t)}   y     r ( − t )   {\displaystyle r(-t)}   son simétricos respecto al eje focal. En particular podemos graficar una parte de cada rama usando cualquier intervalo:     t ∈  [ − s ,  s ]   {\displaystyle t\in \,[-s,\;s]}  , por ejemplo     t ∈  [ − 1 ,  1 ]   {\displaystyle t\in \,[-1,\;1]}    Hipérbola abierta de arriba abajo:  También       r ( t ) = ( a sinh ⁡ ( t ) ,  ± b cosh ⁡ ( t ) ) ,  t ∈   R    {\displaystyle r(t)=(a\sinh(t),\;\pm b\cosh(t)),\;t\in \,\mathbb {R} }  .    En este caso el vértice en cada rama es     r ( 0 )   {\displaystyle r(0)}   y los puntos     r ( t )   {\displaystyle r(t)}   y     r ( − t )   {\displaystyle r(-t)}   son simétricos respecto al eje focal.  En todas las fórmulas     h   {\displaystyle h}   y     k   {\displaystyle k}   son las abcisa y ordenada, respectivamente, del centro de la hipérbola,     a   {\displaystyle a}   es la longitud del semieje mayor,     b   {\displaystyle b}   es la longitud del semieje menor.  Parámetros focales de la hipérbola y=1/x Para determinar los parámetros focales de una hipérbola equilátera definida según la ecuación:  se puede aplicar una operación matricial que permite modificar las coordenadas de un conjunto de puntos del plano cuando se les aplica un giro     θ   {\displaystyle \theta }  :  Como la hipérbola equilátera     y = 1  /  x   {\displaystyle y=1/x}   está girada con respecto al eje x según un ángulo     θ = 45  °    {\displaystyle \theta =45{\text{°}}}  , la matriz de transformación toma la forma:  Partiendo de la ecuación de la hipérbola equilátera     y = 1  /  x   {\displaystyle y=1/x}  , la transformación pasa a ser:  Operando la matriz, resulta:  Calculando      x  ′  2    −  y  ′  2      {\displaystyle x'^{2}-y'^{2}}  , se tiene que:  de donde se deduce que:  De acuerdo con la notación focal, se tiene que la hipérbola equilátera     y = 1  /  x   {\displaystyle y=1/x}   tiene semiejes de valor     a = b =   2     {\displaystyle a=b={\sqrt {2}}}  , y la distancia de sus focos al origen es:  Dado que los focos se encuentran en la recta de simetría     y = x   {\displaystyle y=x}   (inclinada 45°) que corta la hipérbola equilátera     y = 1  /  x   {\displaystyle y=1/x}  , sus coordenadas proyectadas sobre los ejes son:  Elementos de la hipérbola Eje transversal o transverso Se le denomina al segmento rectilíneo donde se encuentran los focos y los vértices de la hipérbola. Su valor es     2 a   {\displaystyle 2a}   y es perpendicular al eje conjugado.  "
matematicas,"En geometría, un plano es un objeto ideal que solo posee dos dimensiones, y contiene infinitos puntos y rectas; es un concepto  fundamental de la geometría junto con el punto y la recta.   Cuando se habla de un plano de polina, se está hablando del objeto geométrico que no posee volumen, es decir bidimensional, y que contiene un número infinito de rectas y puntos. Sin embargo, cuando el término se utiliza en plural, se está hablando de aquel objeto elaborado como una representación gráfica de superficies en diferentes posiciones. Los planos son especialmente utilizados en ingeniería, arquitectura y diseño, ya que sirven para diagramar en una superficie plana o en otras superficies que son regularmente tridimensionales.  Un plano queda definido por los siguientes elementos geométricos:  Los planos suelen nombrarse con una letra del alfabeto griego.   Suele representarse gráficamente, para su mejor visualización, como una figura delimitada por bordes irregulares (para indicar que el dibujo es una parte de una superficie infinita).  En un sistema de coordenadas cartesianas, un punto del plano queda determinado por un par ordenado, llamados abscisa y ordenada del punto. Mediante ese procedimiento, a todo punto del plano corresponden siempre dos números reales ordenados (abscisa y ordenada), y recíprocamente, a un par ordenado de números corresponde un único punto del plano. Consecuentemente, el sistema cartesiano establece una correspondencia biunívoca entre un concepto geométrico como es el de los puntos del plano y un concepto algebraico como son los pares ordenados de números. En coordenadas polares, por un ángulo y una distancia. Esta correspondencia constituye el fundamento de la geometría analítica.  El área es una medida de extensión de una superficie, o de una figura geométrica plana, expresada en unidades de medida denominadas unidades de superficie. Para superficies planas el concepto es más intuitivo. Cualquier superficie plana de lados rectos, por ejemplo un polígono, puede triangularse y se puede calcular su área como suma de las áreas de dichos triángulos. Ocasionalmente se usa el término ""área"" como sinónimo de superficie, cuando no existe confusión entre el concepto geométrico en sí mismo (superficie) y la magnitud métrica asociada al concepto geométrico (área).  Historia Los libros I a IV y VI de Elementos de Euclides trataban de la geometría bidimensional, desarrollando nociones como la semejanza de formas, el teorema de Pitágoras (Proposición 47), la igualdad de ángulos y área, el paralelismo, la suma de los ángulos de un triángulo y los tres casos en que los triángulos son ""iguales"" (tienen la misma área), entre otros muchos temas.  Posteriormente, el plano se describió en el llamado sistema de coordenadas cartesianas, un sistema de coordenadas que especifica cada punto de forma única en un plano mediante un par de numéricos coordenadas, que son las signos distancias desde el punto a dos  rectas perpendiculares fijas, medidas en la misma unidad de longitud. Cada línea de referencia se llama eje de coordenadas o simplemente eje del sistema, y el punto donde se encuentran es su origen', normalmente en el par ordenado (0, 0). Las coordenadas también pueden definirse como las posiciones de las proyecciones perpendiculares del punto sobre los dos ejes, expresadas como distancias con signo desde el origen.  La idea de este sistema fue desarrollada en 1637 en escritos de Descartes e independientemente por Pierre de Fermat, aunque Fermat también trabajaba en tres dimensiones, y no publicó el descubrimiento. [1] Ambos autores utilizaron un único eje en sus tratamientos[cita requerida] y tienen una longitud variable medida en referencia a este eje. El concepto de utilizar un par de ejes se introdujo más tarde, después de que La Géométrie de Descartes fuera traducida al latín en 1649 por Frans van Schooten y sus alumnos. Estos comentaristas introdujeron varios conceptos al tiempo que intentaban aclarar las ideas contenidas en la obra de Descartes.[2]  Más tarde, se pensó en el plano como un campo, en el que dos puntos cualesquiera podían multiplicarse y, excepto 0, dividirse. Esto se conoció como plano complejo. El plano complejo se denomina a veces plano de Argand porque se utiliza en los diagramas de Argand. Estos reciben su nombre de Jean-Robert Argand (1768-1822), aunque fueron descritos por primera vez por el topógrafo y matemático danés-noruego Caspar Wessel (1745-1818).[3] Los diagramas de Argand se utilizan frecuentemente para trazar las posiciones del polos y del ceroes de una función en el plano complejo.  En geometría Sistemas de coordenadas En matemáticas, la geometría analítica (también llamada geometría cartesiana) describe cada punto del espacio bidimensional mediante dos coordenadas.  Se dan dos  ejes de coordenadas perpendiculares que se cruzan en el origen.  Suelen denominarse x e y. En relación con estos ejes, la posición de cualquier punto en el espacio bidimensional viene dada por un par ordenado de números reales, cada número dando la distancia de ese punto desde el origen medido a lo largo del eje dado, que es igual a la distancia de ese punto desde el otro eje.  Otro sistema de coordenadas ampliamente utilizado es el sistema de coordenadas polares, que especifica un punto en términos de su distancia desde el origen y su ángulo relativo a un rayo de referencia hacia la derecha.  Sistema de coordenadas cartesianas  Sistema de coordenadas polares  Incorporación en el espacio tridimensional En geometría euclidiana, un plano es una superficie plana de dos dimensiones  que se extiende indefinidamente.  Los  planos euclídeos surgen a menudo como subespacios del espacio tridimensional.       R   3     {\displaystyle \mathbb {R} ^{3}}  . Un ejemplo prototípico es una de las paredes de una habitación, infinitamente extendida y que se supone infinitesimal delgada. Mientras que un par de números reales       R   2     {\displaystyle \mathbb {R} ^{2}}   basta para describir puntos en un plano, la relación con puntos fuera del plano requiere una consideración especial para su incrustación en el espacio ambiente       R   3     {\displaystyle \mathbb {R} ^{3}}  .  Politopos En dos dimensiones, existen infinitos politopos: los polígonos. A continuación se muestran los primeros regulares:  El símbolo de Schläfli     { n }   {\displaystyle \{n\}}   representa un n-ágono regular.  El monógono (o henágono) regular {1} y el digon regular {2} {2} pueden considerarse polígonos regulares degenerados y existen de forma no degenerada en espacios no euclidianos como una 2-esfera, un 2-toro, o un cilindro circular recto.  Existen infinitos polígonos regulares no convexos en dos dimensiones, cuyos símbolos de Schläfli consisten en números racionales {n/m}. Se llaman polígonos estrella y comparten la misma disposición de vértices de los polígonos regulares convexos.  En general, para cualquier número natural n, existen estrellas poligonales regulares no convexas de n puntas con símbolos de Schläfli {n/m} para todo m tal que m < n/2 (estrictamente hablando {n/m} = {n/(n - m)}) y m y n son coprimos.  Círculo La hiperesfera en 2 dimensiones es un círculo, a veces llamado 1-esfera (S1) porque es un colector unidimensional. En un plano euclídeo, tiene la longitud 2πr y el área de su interior es  donde     r   {\displaystyle r}   es el radio.  Otras formas Existen infinidad de otras formas curvas en dos dimensiones, entre las que destacan las secciones cónicas: la elipse, la  parábola y la hipérbola.  Propiedades del plano ℝ3 En un espacio euclidiano tridimensional ℝ3, podemos hallar los siguientes hechos (los cuales no son necesariamente válidos para dimensiones mayores):  Ecuación vectorial del plano Un plano queda definido por los siguientes elementos geométricos: un punto y dos vectores:  Punto P = (x1, y1, z1)Vector u = (ux, uy, uz)Vector v = (a2, b2, c2)   donde     m   {\displaystyle m}   y     n   {\displaystyle n}   son escalares.  Esta es la forma vectorial del plano; sin embargo, la forma más utilizada es la reducida, resultado de igualar a cero el determinante formado por los dos vectores y el punto genérico X = (x, y, z) con el punto dado. De esta manera la ecuación del plano es:  Donde (A, B, C) es un vector perpendicular al plano y coincide con el producto vectorial de los vectores u y v. La fórmula para hallar la ecuación cuando no está en el origen es:  P = P0 + mA + nB es la ecuación del plano determinado por un punto fijo y dos vectores A y B no colineales.[4]  Ecuación mediante vector ortogonal a.x = 0, donde a es un vector ortogonal y x un punto del plano.  Posición relativa entre dos planos Si tenemos un plano 1 con un punto A y un vector normal 1, y también tenemos un plano 2 con un punto B y un vector normal 2.  Sus posiciones relativas pueden ser:  Distancia de un punto a un plano Para un plano cualquiera      Π : a x + b y + c z + d = 0    {\displaystyle \Pi :ax+by+cz+d=0\,}   y un punto cualquiera       p   1   = (  x  1   ,  y  1   ,  z  1   )   {\displaystyle \mathbf {p} _{1}=(x_{1},y_{1},z_{1})}   no necesariamente contenido en dicho plano Π, la menor distancia entre P1 y el plano Π es:  De lo anterior se deduce que el punto P1 pertenecerá al plano Π si y solo si D=0.  Si los coeficientes a, b y c de la ecuación canónica de un plano cualquiera están normalizados, esto es cuando        a  2   +  b  2   +  c  2     = 1   {\displaystyle {\sqrt {a^{2}+b^{2}+c^{2}}}=1}  , entonces la fórmula anterior de la distancia D se reduce a:  Semiplano Se llama semiplano, en geometría, cada una de las dos partes en que un plano queda dividido por una recta.   Postulados de la división de un plano En cada pareja de semiplanos que una recta r determina sobre un plano existen infinitos puntos tales que:  En topología En topología, el plano se caracteriza por ser el único espacio contráctil  bidimensional.  Su dimensión se caracteriza por el hecho de que la eliminación de un punto del plano deja un espacio que está conectado, pero no  simplemente conectado.  En teoría de grafos En teoría de grafos, un grafo plano es un grafo que se puede  incrustar en el plano, es decir, que se puede dibujar en el plano de tal manera que sus aristas se crucen sólo en sus puntos extremos.  En otras palabras, se puede dibujar de forma que ninguna arista se cruce con otra.[6] Tal dibujo se llama un grafo plano o incrustación plana del grafo. Un grafo plano puede definirse como un grafo plano con un mapeado desde cada nodo a un punto en un plano, y desde cada arista a una curva plana en ese plano, de tal forma que los puntos extremos de cada curva son los puntos mapeados desde sus nodos extremos, y todas las curvas son disjuntas excepto en sus puntos extremos."